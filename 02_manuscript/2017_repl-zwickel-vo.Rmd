---
title             : "Gaze cueing in naturalistic scenes under top-down modulation - A conceptual replication"
shorttitle        : "Gaze cueing in naturalistic scenes"

author:
  - name          : "Jonas Großekathöfer"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Marcusstraße 9-11, 97070 Würzburg"
    email         : "jonas.grossekathoefer@uni-wuerzburg.de"
  - name          : "Kristina Suchotzki"
    affiliation   : "1"
  - name          : "Matthias Gamer"
    affiliation   : "1"  

affiliation:
  - id            : "1"
    institution   : "Julius-Maximilian University, Würzburg"

author_note: |
  Department of Psychology, Julius Maximilians University of Würzburg, Würzburg, Germany.

  Enter author note here.

abstract: |
  Humans as social beings rely on information provided by conspecifics. One important signal in social communication is eye gaze. The current study (n=90) sought to replicate and extend previous findings of attentional guidance by eye gaze in complex everyday scenes. In line with previous studies, longer, more and earlier fixations for objects referenced by gaze were observed in free viewing conditions. To investigate how robust this prioritization is against top-down modulation, half of the observers receive a memory task that required scanning the whole scene instead of exclusively focusing on *referenced* objects. Interestingly, similar gaze cueing effects occurred in this condition. Moreover, the human beings depicted in the scene received a large amount of attention even though they were irrelevant to the current task. These results indicate that the mere presence of other human beings as well as their gaze orientation have a strong impact on attentional exploration.


keywords          : "keywords"
wordcount         : "?wordcountaddin"

bibliography      : 
  - "r-references.bib"
  - "../2017_repl-zwickel-vo.bib"

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no

lang              : "english"
class             : "man"
output            :
  papaja::apa6_pdf:
    keep_tex: TRUE
                      
---

```{r load_packages, include = FALSE}

library("papaja")
library("tidyverse")
library("car")
library("afex")
library("ggpubr")

```

```{r paths}

rm(list=ls())

pathMEM <- "../01_data/Memory/"
pathET <- "../01_data/prot/"
pathFB <- "../01_data/FB/"

```

```{r Generate prot}

```

```{r Check stimuli}

```

```{r Check baseline quality}

```

```{r Analyze fixations}

```

```{r Reading eye tracking data in, warning = FALSE, message = FALSE}

# ALL EYE TRACKING DATA
vpn <- paste0("vpja",ifelse(c(1:78,81:96)<10,"0",""),c(1:78,81:96))
bed <- rep(c("free","mem"),47)

# ORIGINALLY ACQUIRED DATA
#vpn <- paste0("vpja",ifelse(c(1:78)<10,"0",""),c(1:78))
#bed <- rep(c("free","mem"),39)

bed <- bed[!(vpn %in% "vpja23")]  # missing data
vpn <- vpn[!(vpn %in% "vpja23")]  

# Loop over subjects
erg <- numeric(); nvalid <- numeric(); cleantime <- numeric()
for (vp in vpn) {
  #  print(vp)

  prot <- read.csv2(paste0(pathET,vp,"_Fixations.csv"))

  # Restrict to trials with valid baseline?
  nvalid <- c(nvalid,sum(prot$blok==1))
  prot <- prot[prot$blok==1,]

  cleantime <- c(cleantime,mean(prot$cleantime))

  erg <- rbind(erg,apply(prot[,8:ncol(prot)],2,mean,na.rm=TRUE))
}

df.w.et <- data.frame(code=vpn,group=bed,nvalid,cleantime,erg)


df.l.et <- gather(df.w.et, key, value, fix.face:sac.bnongaze, factor_key=TRUE) %>%
  mutate(
    key = as.character(key),
    fixations =
      as.factor(
        ifelse(startsWith(key, "fix."), "fix",
             ifelse(startsWith(key, "fixn."), "fixn",
                    ifelse(startsWith(key, "fixlat."), "fixlat",
                           ifelse(startsWith(key, "sac."), "sac", NA))))),
    region =
      as.factor(
        ifelse(endsWith(key, ".face"), "face",
               ifelse(endsWith(key, ".body"), "body",
                      ifelse(endsWith(key, ".gaze"), "gaze",
                             ifelse(endsWith(key, ".nongaze"), "nongaze",
                                    ifelse(endsWith(key, ".pgaze"), "pgaze",
                                           ifelse(endsWith(key, ".fgaze"), "fgaze",
                                                  ifelse(endsWith(key, ".bgaze"), "bgaze",
                                                         ifelse(endsWith(key, ".pnongaze"), "pnongaze",
                                                                ifelse(endsWith(key, ".fnongaze"), "fnongaze",
                                                                       ifelse(endsWith(key, ".bnongaze"), "bnongaze",NA)))))))))))) %>%
  arrange(code)

rm(prot, bed, cleantime, nvalid, vp, vpn, erg)

```

```{r Reading memory data in, warning = FALSE, message = FALSE}

# ALL MEMORY DATA
vpn <- paste0("vpja",ifelse(c(1:78,81:96)<10,"0",""),c(1:78,81:96))
bed <- rep(c("free","mem"),47)

# ORIGINALLY ACQUIRED DATA
#vpn <- pasteo("vpja",ifelse(c(1:78)<10,"0",""),c(1:78))
#bed <- rep(c("free","mem"),39)

# Loop over subjects
erg <- numeric()
for (vp in vpn) {
  # print(vp)

  prot <- read.csv2(paste0(pathMEM,vp,".csv"))

  # Item recalled
  gaze <- sum(prot$memgazedat)
  nogaze <- sum(prot$memnongazedat)

  erg <- rbind(erg,c(gaze,nogaze))
}

df.w.mem <- data.frame(code=vpn,bed,erg)               
names(df.w.mem) <- c("code","bed","memgaze","memnogaze")

rm(gaze, nogaze, erg, prot, bed, vp, vpn)

```

```{r Reading questionnaire data in, warning = FALSE, message = FALSE}

df.w.demo <- read_csv2(paste0(pathFB,"Projektarbeit_Dateneingabemaske.csv")) %>%
  transmute(
    vp = as.factor(VP_Nr),
    sex = as.factor(Demo_Sex),
    age = Demo_Alter,
    aq_social = ifelse((5-AQK_1) < 3, 1, 0) + ifelse((5-AQK_7) < 3, 1, 0) + ifelse(AQK_8 < 3, 1, 0) + ifelse((5-AQK_10) < 3, 1, 0) + ifelse((5-AQK_11) < 3, 1, 0) + ifelse(AQK_13 < 3, 1, 0) + ifelse((5-AQK_14) < 3, 1, 0) + ifelse((5-AQK_20) < 3, 1, 0) + ifelse((5-AQK_24) < 3, 1, 0) + ifelse((5-AQK_28) < 3, 1, 0) + ifelse((5-AQK_31) < 3, 1, 0), # 5- 'item' for reversed items, then if 1/2 -> 1, 3/4 -> 0  
    aq_imagination = ifelse((5-AQK_3) < 3, 1, 0) + ifelse((5-AQK_5) < 3, 1, 0) + ifelse((5-AQK_6) < 3, 1, 0) + ifelse((5-AQK_9) < 3, 1, 0) + ifelse((5-AQK_16) < 3, 1, 0) + ifelse((5-AQK_17) < 3, 1, 0) + ifelse((5-AQK_18) < 3, 1, 0) + ifelse((5-AQK_22) < 3, 1, 0) + ifelse((5-AQK_23) < 3, 1, 0) + ifelse((5-AQK_26) < 3, 1, 0) + ifelse((5-AQK_32) < 3, 1, 0) + ifelse((5-AQK_33) < 3, 1, 0),
    aq_communication = ifelse(AQK_2 < 3, 1, 0) + ifelse(AQK_4 < 3, 1, 0) + ifelse(AQK_12 < 3, 1, 0) + ifelse(AQK_15 < 3, 1, 0) + ifelse(AQK_19 < 3, 1, 0) + ifelse(AQK_21 < 3, 1, 0) + ifelse(AQK_25 < 3, 1, 0) + ifelse(AQK_27 < 3, 1, 0) + ifelse(AQK_29 < 3, 1, 0) + ifelse(AQK_30 < 3, 1, 0),
    aq_sumscore = ifelse((5-AQK_1) < 3, 1, 0) + ifelse(AQK_2 < 3, 1, 0) + ifelse((5-AQK_3) < 3, 1, 0) + ifelse(AQK_4 < 3, 1, 0) + ifelse((5-AQK_5) < 3, 1, 0) + ifelse((5-AQK_6) < 3, 1, 0) + ifelse((5-AQK_7) < 3, 1, 0) + ifelse(AQK_8 < 3, 1, 0) + ifelse((5-AQK_9) < 3, 1, 0) + ifelse((5-AQK_10) < 3, 1, 0) + ifelse((5-AQK_11) < 3, 1, 0) + ifelse(AQK_12 < 3, 1, 0) + ifelse(AQK_13 < 3, 1, 0) + ifelse((5-AQK_14) < 3, 1, 0) + ifelse(AQK_15 < 3, 1, 0) + ifelse((5-AQK_16) < 3, 1, 0) + ifelse((5-AQK_17) < 3, 1, 0) + ifelse((5-AQK_18) < 3, 1, 0) + ifelse(AQK_19 < 3, 1, 0) + ifelse((5-AQK_20) < 3, 1, 0) + ifelse(AQK_21 < 3, 1, 0) + ifelse((5-AQK_22) < 3, 1, 0) + ifelse((5-AQK_23) < 3, 1, 0) + ifelse((5-AQK_24) < 3, 1, 0) + ifelse(AQK_25 < 3, 1, 0) + ifelse((5-AQK_26) < 3, 1, 0) + ifelse(AQK_27 < 3, 1, 0) + ifelse((5-AQK_28) < 3, 1, 0) + ifelse(AQK_29 < 3, 1, 0) + ifelse(AQK_30 < 3, 1, 0) + ifelse((5-AQK_31) < 3, 1, 0) + ifelse((5-AQK_32) < 3, 1, 0) + ifelse((5-AQK_33) < 3, 1, 0))

```

# Introduction
Humans in their social environment rely on the information conspecifics provide. This does not only hold for reading explicit signals, as in conversations, but also for implicit signals, as in gazes. Specifically, if an individual looks into a specific direction, this information is often read spontaneously and redirects the observers' attention towards the referred object or location. The process of following someone's gaze is can be referred to as joint attention. These attentional shifts are crucial for joint attention. <!--**Nach der kognitiven Relevanz-Hypothese werden top-down- Prozesse kontrolliert über Ziele und Intentionen gesteuert (goal-driven), wohingegen bottom-up- Prozesse nach der Salienz-Hypothese durch Stimuli automatisch ausgelöst werden (stimuli-driven), wodurch die Aufmerksamkeit auf einen bestimmten Ort gelenkt wird (Borji & Itti, 2014). End & Gamer, 2017**-->

  <!-- gaze cuing to joint attenion & social attention -->
The most used paradigm to investigate these attentional shifts is the so-called gaze cueing paradigm [@Friesen1998; @Driver1999; @Langton2000]. These studies show that perceived gaze cues lead to a reflexive attentional shifts, which can result in processing benefit for specific locations [for a very recent study see @Langton2017]. Even though gaze cues are crucial for joint attention, the standard gaze cueing paradigm is criticized for lacking ecological validity [for an overview see: @Risko2012], because (among others) these studies use isolated heads [@Friesen1998; @Langton2000] or even cartoon heads [@Driver1999; @Ristic2005] as gaze cues. For example, in a recent study @Hayward2017 did not find reliable links between classical gaze cueing tasks and real social engage.  
The relevance of ecological valid stimuli is a core aspect of social attention research, which provides a bigger framework for gaze cueing [@Itier2009]. Social attention research describes attentional consequences of social interactions and focuses often on similarities and differences of different types of social stimuli [@Risko2012], from real human interactions [for example: @Laidlaw2011; @Freeth2013a] to highly controlled laboratory settings with isolated faces as stimuli [for example: @Langton2017].<!--The main finding of this research line is, that some research findings related to social stimuli do not extend to the real world. Researches must be sensible to these differences when generalizing laboratory findings [@Jarick2015; @Hayward2017; for an overview see: @Risko2016].
Additionally, laboratory studies showed, that joint attention cannot only be induced by the gaze [for an detailed overview see: @Frischen2007a], but also from the body or the head itself [@Lawson2016].-->

  <!-- social attention -->
<!--Birmingham2008 und Birmingham2009b als vormacher und grundsätzliche Idee das problem anzugehen, weitere Ansätze-->To account for these issues in gaze cuing paradigms @Zwickel2010 conducted a joint attention study using a so called free viewing paradigm with a full person as a directional cue.
A main aspect of their study was that participants had no explicit task to fulfil. @Zwickel2010 argue, that the lack of a specific task puts gaze following to a stricter test. This is explicitly in contrast to @Castelhano2007 who asked participants to understand the story. Additionally, without a task represents naturalistic viewing behavior and therefore adds more ecological validity to classical gaze cuing paradigms. They presented their observers multiple scenes for several seconds. The scenes contained a reference, either a person or a loudspeaker, and two objects. The authors of the study made sure, that the relevant objects were not placed in prominent locations, so that the object is not highlighted by position itself. In each scene, only one of the objects was referenced either by the oriented person or by the loudspeaker.
They showed that observer of the scene fixated the referenced object remarkably earlier, more often and overall longer than the not referenced object. However, the prioritization of the object occurred only when the person referenced the object. By showing that leaving saccades from the head (but not from the loudspeaker) landed most often onto the referenced object the results give direct support for the relation between cue type and object role.
The attentional focus of the person in the scene guided attentional distribution of the observer. Interestingly, the same was not true for the loudspeaker. The referenced objects were not just focused because they might have been salient by themselves (due to e.g. positioning), but became more salient merely by the persons reference.<!--detailierter?!--> <!--Additionally finding is in line with general predictions from the social attention approach. First, that social stimuli are prioritized was confirmed by the results, the most prioritized region was still the head of the shown person.-->After all, @Zwickel2010 provide evidence that joint attention as consequence of gaze cues happen spontaneously and that it has high relevance even in situations that are more naturalistic.<!--softerer übergang pls-->

  <!-- research motivation -->
Unclear remains, how robust these joint attentional effects are. So, this conceptual replication of Zwickel and Võ's work aims at answering multiple research questions regarding joint and social attention, including the replication of Zwickel and Võ's [-@Zwickel2010] findings and extending this line of research.

  <!-- extended research | Top-Down Modulations-->
To extend this line of research the influence of top-down modulations on joint and social attention in naturalistic scenes is investigated. Earlier research showed that social attention is influenced by multiple factors like social status [@Foulsham2010] or expectations [@Perez-Osorio2015]. These studies have in common that they manipulate viewing behavior of the observer by manipulating the stimuli in one or the other way. For example, @Foulsham2010 build the stimulus set from stimuli that were previously rated for social status and confirmed the predicted shift in attention with eye-tracking measures.

In the present study, however, observers receive a very explicit manipulation. In the *explicit encoding group* the observers received the information that a memory test is following the experimental part. In contrast, in the *free viewing group* observers were instructed right before the memory task. Besides that, manipulation all observer did exactly the same experiment (with balanced and randomized stimuli). By instruction objects became relevant for succeeding in the memory task. Conclusively, attention towards the person would be inefficient. The manipulation is thought to induce a more explicit and systematic encoding of the presented scenes, specifically towards objects.

Additionally, the consequences for latter cognitive processes, here memory effects, are examined for the different roles of the objects. Observers from the *free viewing group* who did not receive any top-down modulation in advance should provide an unaffected viewing behavior, just as @Zwickel2010 demonstrated. Both groups had to recall as many as things as possible from the scenes in a free-recall memory test. With this manipulation it is thought to demonstrate top-down influence on social attention aspects, but also its influence on joint attention. As @Zwickel2010 showed, a person in a scene influences the viewing behavior of observers spontaneously and without further instruction or manipulation.

In the given study, the motivation for the manipulation of the instructions for the memory test was twofold. First, it is thought to test robustness of the social and joint attention effects and second joint attentional effects on memory can be observed. The influence of the manipulation is expected to reduce spontaneous viewing behavior and foster a more systematic processing of the scene. It is important to note, that it is not expected that the influence of the presence of the person vanishes completely for social and joint attentional effects due to top-down modulation. It is supposed to be weaker, because it does not represent the optimal strategy for the observer. Therefore, the voluntary part of the scene processing might be overwritten. However, as social and joint attentional effects are expected to be (partially) independent of volition and occur spontaneously and reflexive the effects should still be visible regardless for the social attention part and for the joint attention part of this study. Specifically, prioritization effects for the head should be smaller but still visible in the *explicit encoding group*. For the joint attention measures a decreased prioritization for the referenced object is expected for the *explicit encoding group* is decreased compared with the not referenced object. An optimal strategy for observer in the *explicit encoding group* would be to ignore completely the person in the scene, resulting in smaller differences in attentional prioritization between the two object roles.
Additionally, memory effects sensitive for the role of the object were examined, to answer the question whether more attentional resources on an object pays off with enhanced memorability. This is mainly interesting for the free viewing group with the unbiased viewing behavior, as it is expected that both objects bind comparable amount of attention due to the more systematic processing in the explicit encoding group.

<!-- @Zwickel2010 had very little power in their study, with n = 16 participants -- zu unserer Power was schreiben?!-->
@Zwickel2010 used computer-generated stimuli. Here, stimuli that are more naturalistic and a bigger sample size is used.
Consequently, real photographs are chosen over computer rendered scenes. As often, by being more naturalistic experimental control is reduced. The consequences are hold to a minimum by producing the stimuli the same way. In particular, each scene was photographed four times, with the individual looking twice to the left and right to each object. Although, as much as possible was controlled for in the photographs, real photographs contain small unavoidable changes. Whereas @Zwickel2010 rotate the figure in the computer rendered scene, they have complete control of all the changes, e.g. angel of the body or facial expression. Here, another photography was taken. As consequence, for example, the body orientation within the four balanced scenes cannot be perfectly controlled and might differ slightly between photographs within scenes.<!-- nochmal sagen, warum das torztdem toll ist? -->
With higher power and stimuli, that are more naturalistic, this study will <!-- still, however, nevertheless, yet, regardless, even though, despite --> underline @Zwickel2010 findings. However, it is presumed that the effect showed by @Zwickel2010 is genuine for social stimuli. Therefore, no non-social condition comparable to the loudspeaker-condition was tested.  

  <!-- Hypothesis -->
<!-- alter baustein: First, for joint attention, it is presumed to replicate the main effects of a persons presence on the observer described by @Zwickel2010. It is therefore hypothesized that the gaze of the person in the scene leads to prioritization for the *referenced*/referenced object. The prioritization can be seen in earlier, more and longer fixations. In line with @Zwickel2010 it is also expected, that saccades leaving from the head are more likely to move to the *referenced* object. -->
        <!-- joint attention -->
The foundation of this study is to examine top-down modulation on gaze cueing and to replicate the findings from [@Zwickel2010].  
*H1*. Consequently, a prioritization for the *referenced* objects is predicted. In line with @Zwickel2010, this  prioritization is measured in multiple ways. First, there should be an early fixation bias towards the *referenced* object. During presentation time, the total time that the *referenced* object is fixated should be prolonged, with more fixations as well. For saccades it is expected, that leaving from the head it is more likely to move to the *referenced* object, in contrast to the not-*referenced* object.<!--Main effect icond, Anova-->
*H2*. New, compared to Zwickel and Võ's [-@Zwickel2010] study, is the instruction condition. For gaze following it is expected that the exclusive prioritization for the *referenced* object will decrease for subjects in explicit encoding group, due to the instruction of the memory test and a more explicit and systematic processing of the scene. This means smaller differences in the fixation measurements are expected. For leaving saccades it is also expected that the under H1 stated effect decreases. <!--1st: (Interaction group:icond)
(?!Hypothese zu Gruppen, dass explicit mehr focus auf Gegensätnde richtet, da anweisung? (Maineffect group) -->
*H3*. For the follow-up memory test it is assumed, that observers with announced memory test (in the *explicit encoding group*) will recall more items, because they process the scenes more thorough. Additionally it is expected, that in the free-recall condition *referenced* objects are better recalled than not-*referenced* objects.
        <!-- social attention -->
*H4*. Additionally to the joint attentional effects stated in H1, the basic effects of social attention are expected. This means, that the head will be prioritized over the body. This prioritization can be measured, in multiple ways. Again, it is expected, that first fixation fall earlier on the head, that it is fixated longer and more often than the body. Additionally, fixations occur earlier on the head, than on the *referenced* object, because the gaze cue needs to be processed in advance.<!--gibt's gar keine Analyse für!!!-->
*H5*. Due to the instruction of the memory test, and the assumed change in processing the scene, the viewing behavior should change for social stimuli. Due to explicit encoding, it is expected that natural viewing behavior that is known to prioritize the head is reduced and therefore the head loses some of its salience. As a consequence, the head should be prioritized stronger in the *free viewing group* compared to the *explicit encoding group*, resulting in longer fixations, faster fixations and more fixations.


# Methods
<!-- We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study. <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) -->

## Participants
In this study `r length(df.w.demo$vp)` observers (`r length(df.w.demo$sex[df.w.demo$sex==1])` female and `r length(df.w.demo$sex[df.w.demo$sex!=1])` male) between `r min(df.w.demo$age)` and `r max(df.w.demo$age)` years (M = `r round(mean(df.w.demo$age),2)`, SD = `r round(sd(df.w.demo$age),2)`) participated voluntarily. All observer had normal or corrected vision and were recruited at the University of Würzburg's online subject pool or by blackboard. For participation observers received study credit or 5€. One observer was excluded due to missing data.

## Stimuli and Apparatus
The experimental stimuli consisted of 26 different scenes with a single individual in the center looking at one of two objects of interests in the right or left half of the photography. The direction of the gaze cue and the place of the objects were balanced, creating 104 unique naturalistic scenes in the end. For each participant, a set was randomly generated from this pool containing each scene only once, resulting in 26 trials with exclusive scenes. Eye movements were tracked with the corneal-reflexion method and were recorded with an EyeLink1000 tower system, sampling at 1000 Hz. The eye tracker and stimuli were controlled by the software Presentation® (Neurobehavioral Systems).

## Design and Procedure
The experimental design was a 2 x 2 within-between observer design. First, as a two-level factor the instruction was manipulated between subjects (*free viewing* vs. *explicit encoding*). Additionally, as a two-level within subjects factor the role of the object was manipulated (*referenced* vs. *not-referenced*).

First, observer were ask to give full informed consent. Then the eye-tracker was calibrated for the observer. According to the manipulation, only half of the observers were told that  there is a follow-up memory test. All observers were then told to look at the following scenes as they would look at photographs. The presentation of the scenes was randomized and the roles of the objects and their location were completely balanced and controlled. In each trial the scene was present for 10 seconds. Inter trial interval was randomized between 2 and 4 seconds. After the last trial participants filled in questionnaires (demographics, autism-questionnaire (short), and Inventar soziale kompetenz). To prevent primacy and recency effects the questionnaire session was also used as a puffer for the following memory task. Afterwards all observers were asked to recall as many items as possible in a free recall memory-test. Observers received credit afterwards.


## Data analysis
For the analysis standard configuration of SR Research’s EyeLink DataViewer software was used to categorize eye movements into saccades and fixations. Saccades were defined as eye movements exceeding a velocity threshold of 30°/sec or an acceleration threshold of 8.000°/sec2. Fixations were defined as time periods between saccades. Regions-of-Interest (ROI) were hand-drawn around the relevant objects and the face and body of the individual in the center. These ROIs were color-coded for cued and uncued objects and for the head and body of the individual on the photography to determine gaze locations. <!-- Outlier detection?!!? -->

`r cite_r("r-references.bib")` is used for all analyses.


# Results
Prioritization of the ROIs in the scene were measured in multiple ways. 

## Joint attention

As a measure of prioritization fixation duration, fixation number, fixation latency, as well as leaving saccades are used. For all measures, higher values indicate prioritization. 

### Fixations
```{r TO-DO: Plots fixations joint, warining = FALSE, message = FALSE}

```

```{r Descriptives fix*, warining = FALSE, message = FALSE}

# Descriptives for fixation chacracteristics for joint attention

dscr.fix.rpl <- df.w.et %>%
  group_by(group) %>%
  summarise_at(vars(fix.face:fixlat.nongaze), funs(mean,sd,se=sd(.)/sqrt(n()))) %>% # mit den funs() die variablen vars(von:bis) berechnen
  gather(key, measure, fix.face_mean:fixlat.nongaze_se) %>% # ins longformat
  mutate(fix = as.factor(map(strsplit(key,"[[:punct:]]"), ~.x[1]) %>% unlist()),
         gazed = map(strsplit(key,"[[:punct:]]"), ~.x[2]) %>% unlist(),
         gazed = as.factor(substring(gazed,1)),
         stat = as.factor(map(strsplit(key,"[[:punct:]]"), ~.x[3]) %>% unlist()),
         key = as.factor("replication")) %>% # 
  select(-measure, everything()) %>% # neusortieren der variablen
  select(key, everything())

dscr.fix.rpl$gazed <- factor(dscr.fix.rpl$gazed, levels=c("face", "body", "gaze", "nongaze"))

dscr.fix.zwckl <- data.frame(group="free",fix=c("fix","fix","fix","fix","fixn","fixn","fixn","fixn","fixlat","fixlat","fixlat","fixlat"), gazed=c("gaze","gaze", "nongaze", "nongaze"), stat=c("mean","se"), measure=c(0.08,.01,.07,.01,5.89,.37,5.24,.34,3588,133,4008,166)) # free = person

```

```{r Anova fixations joint, warning = FALSE, message = FALSE}

# ANOVA Fixations characteristics for joint attention
## 2 (Group) x 2 (Gaze) ANOVA

icond <- gl(2,1,labels=c("cued","uncued")) # within-factor
idata <- data.frame(icond)

for (st in seq(7,16,4)) { # Variables 7:16, every 4th: fix.gaze and fix.nogaze; fixn.gaze and ...
  carmod <- lm(as.matrix(df.w.et[,st:(st+1)]) ~ df.w.et$group)
  #print(colnames(df.w.et[,st:(st+1)]))
  #print(Anova(carmod, idata=idata, idesign=~icond, type="III"))
  assign(paste0("anova.",colnames(df.w.et)[st]), Anova(carmod, idata=idata, idesign=~icond, type="III"))
  assign(paste0("apa.anova.",colnames(df.w.et)[st]), apa_print(Anova(carmod, idata=idata, idesign=~icond, type="III"), correction="GG", mse = FALSE))
  }

rm(carmod, idata, icond, st)

```

#### Fixation duration
<!-- (H1, H2) -->
The duration of all fixations is cumulated for each object. Specifically, the cumulative time (in ms) a fixation rested on an object was divided by the total time spent fixating any other rest of the scene. By that, a relative measures for fixation duration was gained. Observer show a bias towards the *referenced* object, which was fixated longer (see Table/Figure). As predicted the corresponding ANOVA revealed a significant main effect for reference, `r apa.anova.fix.gaze$full_result$icond` and for condition `r apa.anova.fix.gaze$full_result$df_w_et_group`. Against prediction there was no effect of the two-way interaction, `r apa.anova.fix.gaze$full_result$df_w_et_group_icond`. <!-- was it against pred? -->  <!-- The relative cumulative fixation duration is calculated as time duration on an object divided by--> 

#### Fixation latency
<!-- (H1, H2) -->
An additional measurement of prioritization is fixation latency. That is for each object the mean time out of all first fixations per object each trial<!--besser formulieren!!-->. This measure shows, that objects are fixated earlier when they are *referenced*, but also when the observer was in the explicit encoding condition. The statistical significance is confirmed by an ANOVA: for reference `r apa.anova.fixlat.gaze$full_result$icond` and condition, `r apa.anova.fixlat.gaze$full_result$df_w_et_group`). The interaction shows a trend, `r apa.anova.fixlat.gaze$full_result$df_w_et_group_icond`, suggesting a smaller difference for reference for the *explicit encoding group*.

#### Fixation number
<!-- (H1, H2) -->
As a third measurement of prioritization, fixation number, as the count of fixations per object, divided by the total number of fixations during scene presentation, was calculated. It shows that objects were fixated more often when *referenced*, `r apa.anova.fixn.gaze$full_result$icond`. Objects were also more often fixated in the explicit encoding condition`r apa.anova.fixn.gaze$full_result$df_w_mem`, and again against prediction there was no interaction, `r apa.anova.fixn.gaze$full_result$df_w_et_group_icond`.
 

### Saccades

```{r descriptives saccade, include=FALSE}
dscr.sac.rpl <- df.w.et %>%
  group_by(group) %>%
  summarise_at(vars(sac.pgaze:sac.bnongaze), funs(mean,sd,se=sd(.)/sqrt(n()))) %>% # mit den funs() die variablen vars(von:bis) berechnen
  gather(key, measure, sac.pgaze_mean:sac.bnongaze_se) %>% # ins longformat
  mutate(stat = as.factor(map(strsplit(key,"[[:punct:]]"), ~.x[3]) %>% unlist()),
         area = map(strsplit(key,"[[:punct:]]"), ~.x[2]) %>% unlist(),
         area = as.factor(substring(area,1,1)),
         gazed = map(strsplit(key,"[[:punct:]]"), ~.x[2]) %>% unlist(),
         gazed = as.factor(substring(gazed,2)), 
         key = as.factor("replication")) %>% #rausnehmen, zur kontrolle, ob alles passt, was oben läuft.
  select(-measure, everything()) %>% # neusortieren der variablen
  select(key, everything())

dscr.sac.zwckl <- data.frame(key=c("zwickel","zwickel"),group=c("free", "free"),gazed=c("gaze", "gaze", "nongaze", "nongaze"),stat=c("mean", "se"),measure=c(.14,0.01,.09,0.01)) # free = person

```

```{r Matthias plots saccade, include=FALSE, echo = FALSE}
# Plots: Saccade data
'
farben <- c("blue","yellow")
ttxt <- c("Person","Face","Body")

for (i in 1:3) {
  st <- (17:19)[i]

  # Fixation density
  m  <- cbind(apply(df.w.et[df.w.et$group=="free",c(st,st+3)],2,mean),
              apply(df.w.et[df.w.et$group=="mem",c(st,st+3)],2,mean))
  se <- cbind(apply(df.w.et[df.w.et$group=="free",c(st,st+3)],2,sd)/sqrt(sum(df.w.et$group=="free")),
              apply(df.w.et[df.w.et$group=="mem",c(st,st+3)],2,sd)/sqrt(sum(df.w.et$group=="mem")))

  yrng <- c(0,max(m+se))

  x <- barplot(m,beside=TRUE,col=farben,ylim=yrng,xlab="",ylab="% of total fixation time")
  arrows(x,m-se,x,m+se,length=0.03,angle=90,code=3,col="black")
  axis(1,apply(x,2,mean),c("Free viewing","Explicit encoding"),tick=FALSE)

  title(ttxt[i])
}

legend(max(x),max(yrng),c("Cued","Uncued"),fill=farben,xjust=1,yjust=1)

rm(m, se, yrng, x, st, farben, ttxt, i)
'
```

```{r Anova saccades, warning=FALSE, message=FALSE}
# ANOVA Saccades
## 2 (Group) x 2 (Gaze) ANOVA

icond <- gl(2,1,labels=c("cued","uncued")) # within-factor
idata <- data.frame(icond)

for (st in 17:19) {
  carmod <- lm(as.matrix(df.w.et[,c(st,(st+3))]) ~ df.w.et$group)
  #print(colnames(df.w.et[,c(st,(st+3))]))
  #print(Anova(carmod, idata=idata, idesign=~icond, type="III"))
  assign(paste0("anova.",colnames(df.w.et)[st]), Anova(carmod, idata=idata, idesign=~icond, type="III"))
  assign(paste0("postHoc.",colnames(df.w.et)[st]), lsmeans::lsmeans(carmod, specs = c(names(carmod$model[2]), "rep.meas")))
  assign(paste0("apa.anova.",colnames(df.w.et)[st]), apa_print(Anova(carmod, idata=idata, idesign=~icond, type="III"), correction="GG", mse = FALSE))
}


rm(carmod, idata, icond, st)
```

#### Leaving saccades
<!-- (H1, H2) -->
The second hypothesis predicts that there are more saccades leaving from the head to the *referenced* object. Furthermore it is claimed, that an interaction between instruction and reference results in increasing differences in the free viewing group. The first claim finds statistical support from an Anova. There is a significant main effect for reference, `r apa.anova.sac.fgaze$full_result$icond`, and for group,  `r apa.anova.sac.fgaze$full_result$df_w_et_group`. No confirmation was found for the predicted interaction between reference and instruction `r apa.anova.sac.fgaze$full_result$df_w_et_group_icond`.

### Memory
<!-- (H3) -->

```{r plots memory}

```

```{r Anova memory, warning=FALSE, message=FALSE, echo = FALSE}

# Recalled items
# 2 (Condition) x 2 (Gaze) ANOVA (auf erinnerte Details)

imem <- gl(2,1,labels=c("cued","uncued"))
idata <- data.frame(imem)

# Car
carmod <- lm(as.matrix(df.w.mem[,3:4]) ~ df.w.mem$bed)
#print(colnames(df.w.mem[c(3,4)]))
#print(Anova(carmod, idata=idata, idesign=~imem, type="III"))
anova.mem <- Anova(carmod, idata=idata, idesign=~imem, type="III")
apa.anova.mem <- apa_print(Anova(carmod, idata=idata, idesign=~imem, type="III"), correction="GG", mse = FALSE)

dscr.mem <- df.w.mem %>% summarise(mean(memgaze), sd(memgaze), mean(memnogaze), sd(memnogaze))
dscr.mem.group <- df.w.mem %>% group_by(bed) %>% summarise(mean(memgaze), sd(memgaze), mean(memnogaze), sd(memnogaze))

rm(imem, idata, carmod)
```

The free-recall memory test showed, that observers with who received the instruction of the memory test before the free viewing remembered more items than observers from the other group. An Anova showed its statistical significance, `r apa.anova.mem$full_result$df_w_mem_bed`. However, against prediction, referencing an object did not influence memory performance, `r apa.anova.mem$full_result$imem`. There was also no interaction, `r apa.anova.mem$full_result$df_w_mem_bed_imem`, such that referencing an object would increase memorability only for one group, as it is stated in the hypothesis for the free viewing group.

## Social attention
A similar pattern to the objects can be seen when comparing the head with the body region. Again, there were the same measurements to account for prioritization. 

```{r TO-DO: Plots fixations social, warining = FALSE, message = FALSE}

```

```{r Descriptives fixations social, warining = FALSE, message = FALSE}

# Descriptives for fixation chacracteristics for social attention

dscr.fixsoc <- df.w.et %>% summarise(mean(fix.face), sd(fix.face), mean(fix.body), sd(fix.body), mean(fixn.face), sd(fixn.face), mean(fixn.body), sd(fixn.body), mean(fixlat.face), sd(fixlat.face), mean(fixlat.body), sd(fixlat.body))
dscr.fixsoc.group <- df.w.et %>% group_by(group) %>% summarise(mean(fix.face), sd(fix.face), mean(fix.body), sd(fix.body), mean(fixn.face), sd(fixn.face), mean(fixn.body), sd(fixn.body), mean(fixlat.face), sd(fixlat.face), mean(fixlat.body), sd(fixlat.body))
```

```{r Anova fixations social, warning = FALSE, message = FALSE}
# ANOVA Fixations characteristics
## 2 (Group) x 2 (Face/Body) ANOVA

icond <- gl(2,1,labels=c("head","body")) # within-factor
idata <- data.frame(icond)

for (st in seq(5,16,4)) { # Variables 5:16, every 4th: fix.face and fix.noface; fixn.face and ...
    carmod <- lm(as.matrix(df.w.et[,st:(st+1)]) ~ df.w.et$group)
  #print(Anova(carmod, idata=idata, idesign=~icond, type="III"))
  assign(paste0("anova.",colnames(df.w.et)[st]), Anova(carmod, idata=idata, idesign=~icond, type="III"))
  assign(paste0("apa.anova.",colnames(df.w.et)[st]), apa_print(Anova(carmod, idata=idata, idesign=~icond, type="III"), correction="GG", mse = FALSE))
  }


rm(carmod, idata, icond, st)

```
For the social attention part Anovas are conducted for the same dependent variables indicating prioritization, but the head is compared against the body region.

### Face vs body fixation (H4)
The first measurement shows, that head regions were fixated longer than the body  (Table X), `r apa.anova.fix.face$full_result$icond`, but there is no difference between the instruction-groups, `r apa.anova.fix.face$full_result$df_w_et_group`, and no interaction between the two factors, `r apa.anova.fix.face$full_result$df_w_et_group_icond`.

The same pattern can be seen for fixation number, with a strong effect for region, with more fixations on the head, `r apa.anova.fixn.face$full_result$icond` and no difference between the groups, `r apa.anova.fixn.face$full_result$df_w_et_group`, and no interaction `r apa.anova.fixn.face$full_result$df_w_et_group_icond`.

Latencies of fixation differed also remarkably between head and body, but only for region `r apa.anova.fixlat.face$full_result$icond`. There was neither an effect for instruction, `r apa.anova.fixlat.face$full_result$df_w_et_group` nor was there an interaction, `r apa.anova.fixlat.face$full_result$df_w_et_group_icond`.

### free viewing vs explicit encoding for face fixations (H5)
```{r Plots free vs mem face fixations, warning = FALSE, message = FALSE}

# plots

plot.face.free_vs_mem <- df.l.et %>%
  filter(fixations=="fix" & region=="face") %>%
  ggboxplot(x = "fixations", y = "value",
                fill = "group", palette =c("#00AFBB", "#E7B800", "#FC4E07"),
                add = "jitter", shape = "group") #+
  #stat_compare_means()
#plot.face.free_vs_mem

```

```{r T-test face fixations, warning = FALSE, message = FALSE}

# Fixations Face in Free Viewing vs. Explicit Encoding

for (st in seq(5,16,4)) {
  msd <- c(mean(df.w.et[df.w.et$group=="free",st]),sd(df.w.et[df.w.et$group=="free",st]),
           mean(df.w.et[df.w.et$group=="mem",st]),sd(df.w.et[df.w.et$group=="mem",st]))
  teststat <- t.test(df.w.et[,st] ~ df.w.et$group)
  
  assign(paste0("ttest.",colnames(df.w.et)[st]), data.frame(M.free=msd[1],SD.free=msd[2], M.mem=msd[3], SD.mem=msd[4],  df=teststat$parameter, t=teststat$statistic, p=teststat$p.value))
 
  assign(paste0("apa.ttest.",colnames(df.w.et)[st]), apa_print(t.test(df.w.et[,st] ~ df.w.et$group)))
  }


rm(msd, teststat, st) 

```

A post Hoc t-test between head fixations for each group provides information whether heads are differently processed dependent on the instruction observers get. It reveals, that only fixation latency differed significantly between groups, `r apa.ttest.fixlat.face$full_result`, with faster fixations for the free viewing group. For fixation duration there was only a trend for difference between groups, `r apa.ttest.fix.face$full_result`. In addition, fixation number did not differ, `r apa.ttest.fixn.face$full_result`.


# Discussion
<!-- Main motivation -->
The main attempt to replicate @Zwickel2010's findings on attentional prioritization from the mere presence of a person in a scene on gaze behavior was successful.
<!-- replication -->
As in @Zwickel2010 and others studies [for example @Birmingham2008], a strong prioritization can be seen for the head of the person in the scene. Accordingly, the head was fixated first, before any other ROI. Besides that the heads were also fixated more often, meaning that during scene presentation observers kept looking at the head. Consequently, no ROI was fixated longer in total than the head.
Additionally, also in line with @Zwickel2010 *referenced* objects were preferenced over the object not being *referenced* in a given trial. The preference for the *referenced* object is consistent over all measures. First, the object that was *referenced* was fixated remarkably earlier. Furthermore, objects were also fixated longer with more frequent fixations when they were *referenced*. Therefore, the gaze cue by the person does not only leads to more thorough processing during the whole time of scene presentation, but also guides attentional resources very early. All these findings together indicate a strong prioritization through the gaze cue of the person in the scene.
<!-- saccacades -->
Moreover, the consistent prioritization of the head and the *referenced* object indirectly suggest a link between these two regions. To investigate a direct relation leaving saccades from the head are examined as well. It is shown that saccades leaving the head are more likely to end on the *referenced* object. This underlines a direct link between the head and the *referenced* object.
So far, the given results replicate all aspects @Zwickel2010 demonstrated for the presence of a person in their study.
However, this replication included a top-down modulation, used photographic scenes and had considerably more power than the original study. Although it is unclear how the photographs changed the results compared to the computer rendered scenes, the higher power lead to more precise estimation of the effect sizes. Note, however, that this was not a direct but a conceptual replication, showing that @Zwickel2010 findings can be generalized from computer rendered to photographic scenes.
<!-- top-down -->
  Besides replicating, this study aims also at extending the line of research, generalizing it to new stimuli and at testing the pattern in gaze behavior as it was shown here against top-down modulation, specifically an instructed task prior free viewing. Unsurprisingly the task scores show, that observer, who knew about the free-recall task in advance perform better in recalling items. More interesting, viewing behavior was different depending on instruction group. Specifically, observer pay more attention to objects when they received the instruction prior to the viewing part, and at the same time, the head loses some of its natural salience. Both findings are consistent over all measures. These findings suggest that observers with instruction had a more systematic gaze pattern, where they preferred task relevant objects. **However, no interaction between top-down modulation and reference of an object can be found.** Consistent for social attention as well as joint attention, prioritization in all measurements remain stable against the memory task instruction. The prioritization shown for the head and for the *referenced* object was unaffected by the given top-down modulation<!--, although descriptive differences became smaller (note, that this trend was not significant)-->. The attentional guidance of social and joint attentional processes can be seen, even when observers investigate the scene more systematic. This evidence provides support for the automaticity and reflexivness of joint and social attentional effects, even when the observer voluntarily aims at scanning the scene systematically. Nevertheless, observer with instructed free-recall task performed better. However, the contribution of the attentional reallocation remains unclear.
In particular, **whether an object was cued or not by the person, did not influence its probability being recalled**. This is in contrast to the increased attention referenced objects receive. That is partially inconsistent with the hypothesis, which stated correct that scene processing changes, but the actual link is missing that the change in processing for solving the task can be explained from the attentional reallocation of attention. Even as it is assumed, these findings do not indicate a link between spent attentional resources and performance for recall.<!--This missing link can mean WHAT!?--> <!-- korrelation zeit angeschaut und erinnert? -->
Overall, the results provide additional support to previous findings that attention is shifted reflexively where other persons are looking at [e.g. @Ristic2005,  @Hayward2017]. This evidence, which was previously extended to free viewing of static naturalistic scenes by @Zwickel2010, is shown to be robust against top-down modulation. Even when attentional allocation changes due to a more systematic viewing pattern, social and joint attentional shifts are still affected by the mere presence of a person, comparable as if the observers perform unbiased free viewing.  <!-- Einordnen mit privious findings && unclear was bei anderen top down modulationen! -->
<!-- Conclusio -->
All in all, our results indicate that the mere presence of other human beings as well as their gaze orientation have a strong impact on attentional exploration. The observed attentional guidance of the gaze was so robust to resist even top-down modulation. It is concluded that attentional guidance by social attentional and joint attentional trigger is very robust.

<!--#Schrott
Zwickel struktur: 1: general (social atteniton/head), 2: gaze cueing, 3(2a): diskutiert, 4(2b): diskutiert, 5: Leaving saccades, 6: head body, 7: surprising data, 8: vergleich mit unpassender studie, 9: conclusio
Meine struktur: 1: general (social attention/head) finding, 2: gaze cueing (object), 3: saccades (head -> object), 4: (kurzes oder detailliertes?!) fazit replication, 5: top down-modulation, 6: ?! DENK NACH! ?!, 7: conclusio



Unlike @Zwickel2010, naturalistic photographic are used, in contrast to computer rendered scenes.-->


<!-- was will ich erzählen?! Replikation -> Kopf-prio, gazed-prio, sac-prio! --> 
<!-- was ist relevant?! --> 
<!-- was überrascht nicht?! topdown-modulatioon verändert blickverhalten--> 
<!-- was überrascht? Obwohl (!) top-down modulation, blickverhalten ändert, bleiben Effekte bestehen! --> 



<!-- alt: 

For genuine social stimuli like the person, and more specifically the head and the body, a similar pattern can be observed. The head had the lowest latency until first fixation, so it is even shorter than the latency till first fixation for the *referenced* object. Under the light of joint attention this makes sense, since it is assumed, that the head in the first place leads the gaze of the observer to the *referenced* object. Additionally, the head was also fixated longer and more often than the body.

The extension of this line of research, namely whether the gaze behavior can be influenced by top-down modulation. Consequently, this research is extended by the given study by an explicit task concerning the given scene. this modulation has an impact on gaze behavior.

-->

<!-- List bibtex errors:
  Risko2012
  Langton2000
  Langton2017
  Laidlaw2011 -->

\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
