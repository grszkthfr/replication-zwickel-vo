---
title             : "Gaze cueing in naturalistic scenes under top-down modulation - A conceptual replication"
shorttitle        : "Gaze cueing in naturalistic scenes"

author:
  - name          : "Jonas Großekathöfer"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Marcusstraße 9-11, 97070 Würzburg"
    email         : "jonas.grossekathoefer@uni-wuerzburg.de"
  - name          : "Kristina Suchotzki"
    affiliation   : "1"
  - name          : "Matthias Gamer"
    affiliation   : "1"  

affiliation:
  - id            : "1"
    institution   : "Julius-Maximilian University, Würzburg"

author_note: |
  Department of Psychology, Julius Maximilians University of Würzburg, Würzburg, Germany.

  Enter author note here.

abstract: |
  Humans as social beings rely on information provided by conspecifics. One important signal in social communication is eye gaze. The current study (n=93) sought to replicate and extend previous findings of attentional guidance by eye gaze in complex everyday scenes. In line with previous studies, longer, more and earlier fixations for objects cued by gaze were observed in free viewing conditions. To investigate how robust this prioritization is against top-down modulation, half of the participants receive a memory task that required scanning the whole scene instead of exclusively focusing on cued objects. Interestingly, similar gaze cueing effects occurred in this group. Moreover, the human beings depicted in the scene received a large amount of attention even though they were irrelevant to the current task. These results indicate that the mere presence of other human beings as well as their gaze orientation have a strong impact on attentional exploration.


keywords          : "keywords"
wordcount         : "?wordcountaddin"

bibliography      : 
  - "r-references.bib"
  - "../2017_repl-zwickel-vo.bib"

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no

lang              : "english"
class             : "man"
output            :
  papaja::apa6_pdf:
    keep_tex: TRUE
                      
---

```{r load_packages, include = FALSE}

library("tidyverse")
#library("ggsignif")
library("car")
library("knitr")
library("papaja")
library("kableExtra")

```

```{r Paths}

rm(list=ls())

pathMEM <- "../01_data/Memory/"
pathET <- "../01_data/prot/"
pathFB <- "../01_data/FB/"

```

```{r Generate prot}

```

```{r Check stimuli}

```

```{r Check baseline quality}

```

```{r Analyze fixations}

```

```{r Reading eye tracking data in, warning = FALSE, message = FALSE}

# ALL EYE TRACKING DATA
vpn <- paste0("vpja",ifelse(c(1:78,81:96)<10,"0",""),c(1:78,81:96))
bed <- rep(c("free","mem"),47)

# ORIGINALLY ACQUIRED DATA
#vpn <- paste0("vpja",ifelse(c(1:78)<10,"0",""),c(1:78))
#bed <- rep(c("free","mem"),39)

bed <- bed[!(vpn %in% "vpja23")]  # missing data
vpn <- vpn[!(vpn %in% "vpja23")]  

# Loop over subjects
erg <- numeric(); nvalid <- numeric(); cleantime <- numeric()
for (vp in vpn) {
  #  print(vp)

  prot <- read.csv2(paste0(pathET,vp,"_Fixations.csv"))

  # Restrict to trials with valid baseline?
  nvalid <- c(nvalid,sum(prot$blok==1))
  prot <- prot[prot$blok==1,]

  cleantime <- c(cleantime,mean(prot$cleantime))

  erg <- rbind(erg,apply(prot[,8:ncol(prot)],2,mean,na.rm=TRUE))
}

df.w.et <- data.frame(code=vpn,group=bed,nvalid,cleantime,erg) %>% 
  mutate(
    code = as.factor(unlist(map(strsplit(as.character(code),"ja"), ~.x[2])))) %>% 
  rename(vp = code)
    


df.l.et <- gather(df.w.et, key, value, fix.face:sac.bnongaze, factor_key=TRUE) %>%
  mutate(
    key = as.character(key),
    fixations =
      as.factor(
        ifelse(startsWith(key, "fix."), "fix",
             ifelse(startsWith(key, "fixn."), "fixn",
                    ifelse(startsWith(key, "fixlat."), "fixlat",
                           ifelse(startsWith(key, "sac."), "sac", NA))))),
    region =
      as.factor(
        ifelse(endsWith(key, ".face"), "face",
               ifelse(endsWith(key, ".body"), "body",
                      ifelse(endsWith(key, ".gaze"), "gaze",
                             ifelse(endsWith(key, ".nongaze"), "nongaze",
                                    ifelse(endsWith(key, ".pgaze"), "pgaze",
                                           ifelse(endsWith(key, ".fgaze"), "fgaze",
                                                  ifelse(endsWith(key, ".bgaze"), "bgaze",
                                                         ifelse(endsWith(key, ".pnongaze"), "pnongaze",
                                                                ifelse(endsWith(key, ".fnongaze"), "fnongaze",
                                                                       ifelse(endsWith(key, ".bnongaze"), "bnongaze",NA)))))))))))) %>%
  arrange(vp)

rm(prot, bed, cleantime, nvalid, vp, vpn, erg)

```

```{r Reading memory data in, warning = FALSE, message = FALSE}

# ALL MEMORY DATA
vpn <- paste0("vpja",ifelse(c(1:78,81:96)<10,"0",""),c(1:78,81:96))
bed <- rep(c("free","mem"),47)

# ORIGINALLY ACQUIRED DATA
#vpn <- pasteo("vpja",ifelse(c(1:78)<10,"0",""),c(1:78))
#bed <- rep(c("free","mem"),39)

# Loop over subjects
erg <- numeric()
for (vp in vpn) {
  # print(vp)

  prot <- read.csv2(paste0(pathMEM,vp,".csv"))

  # Item recalled
  gaze <- sum(prot$memgazedat)
  nogaze <- sum(prot$memnongazedat)

  erg <- rbind(erg,c(gaze,nogaze))
}

df.w.mem <- data.frame(code=vpn,bed,erg) %>% 
  mutate(
    code = as.factor(unlist(map(strsplit(as.character(code),"ja"), ~.x[2])))) %>% 
  rename(vp = code)

names(df.w.mem) <- c("code","bed","memgaze","memnogaze")


rm(gaze, nogaze, erg, prot, bed, vp, vpn)

```

```{r Reading questionnaire data in, warning = FALSE, message = FALSE}

df.w.demo <- read_csv2(paste0(pathFB,"Projektarbeit_Dateneingabemaske.csv")) %>%
mutate_at(
    vars(AQK_1, AQK_3, AQK_5, AQK_6, AQK_7, AQK_9, AQK_10, AQK_11, AQK_14, AQK_16, AQK_17, AQK_18, AQK_20, AQK_22, AQK_23, AQK_24, AQK_26, AQK_28, AQK_31, AQK_32, AQK_33),
    funs(5 - .)) %>% # reverse variables
  mutate_at(
    vars(AQK_1:AQK_33),
    funs(ifelse(. <= 2, 1, ifelse(. <= 4, 0, NA)))) %>% # recode variable
   mutate_at(
     vars(AQK_1:AQK_33),
     funs(ifelse(is.na(.), round(mean(., na.rm = T),0), .))) %>% # replace NAs with mean
  transmute(
    vp = as.factor(VP_Nr),
    sex = as.factor(Demo_Sex),
    age = Demo_Alter,
    aqk_social = (AQK_1 + AQK_7 + AQK_8 + AQK_10 + AQK_11 + AQK_13 + AQK_14 + AQK_20 + AQK_24 + AQK_28 + AQK_31),
    aqk_imagination = (AQK_3 + AQK_5 + AQK_6 + AQK_9 + AQK_16 + AQK_17 + AQK_18 + AQK_22 + AQK_23 + AQK_26 + AQK_32 + AQK_33),
    aqk_communication = (AQK_2 + AQK_4 + AQK_12 + AQK_15 + AQK_19 + AQK_21 + AQK_25 + AQK_27 + AQK_29 + AQK_30),
    aqk_sumscore = (aqk_social + aqk_imagination + aqk_communication))
```

# Introduction
Humans in their social environment rely on the information conspecifics provide. This does not only hold for reading explicit signals, as in conversations, but also for implicit signals, as in gazes. Specifically, if an individual looks into a certain direction, this information is often read spontaneously by an observer who redirects his or her attention towards the referred object or location. Such guidance of someone else's attention is  called gaze following. As a consequence, joint attention is established.

  <!-- gaze cuing to joint attenion & social attention -->
The most frequently used paradigm to investigate such attentional shifts is the so-called gaze cueing paradigm [@Friesen1998; @Driver1999; @Langton2000; for a review see: @Frischen2007]. Studies using this paradigm show that perceived gaze cues (e.g. faces looking at a certain location) lead to reflexive attentional shifts, which can result in a processing benefit for the gazed-at locations. Even though gaze cues are crucial for joint attention, the standard gaze cueing paradigm is criticized for lacking ecological validity, because (among others) these studies use isolated heads [@Friesen1998; @Langton2000] or even cartoon heads [@Driver1999; @Ristic2005] as gaze cues [for an overview see: @Risko2012]. For example, in a recent study @Hayward2017 compared attentional measures of gaze following from laboratory (classical gaze cueing) and real world (real social engage) and did not find reliable links between those measures.  

  <!-- social attention -->
<!--Birmingham2008 und Birmingham2009b als vormacher und grundsätzliche Idee das problem anzugehen, weitere Ansätze-->To account for these issues in gaze cuing paradigms @Zwickel2010 conducted a gaze following study <!--using a so called free viewing paradigm-->with a full person (instead of isolated heads or faces) as a directional cue.
In this study, the authors used a free viewing instruction, meaning that participants had no explicit task to fulfill. @Zwickel2010 argue, that the lack of a specific task puts gaze following to a stricter test. In earlier studies, participants' task was to follow the gaze cue. In classical gaze cueing paradigms the task contained reacting to the only change in the display [e.g. @Langton2017] and in more recent and more naturalistic studies participants were asked to understand a scene [@Castelhano2007]. Consequently, it remains unclear how large the proportion of solely gaze is for gaze cueing paradigms, especially in absence of a specific task. @Zwickel2010 presented participants multiple complex scenes for several seconds with either a person or a loudspeaker and two objects. The absence of a specific task represents a more naturalistic viewing behavior and the complex scenes add ecological validity to gaze cuing paradigms.
The results of the study showed that participants fixated the cued object remarkably earlier, more often and longer than the uncued object. However, the prioritization of the object occurred only when the person cued the object. By showing that leaving saccades from the head landed most often onto the cued object the results give evidence for the relation between cue type and attentional guidance <!-- and highlight therefore the innewohnende relevance of gaze cues-->
The attentional focus of the person in the scene guided attentional distribution of the participant. Crucially, the same was not true for the loudspeaker. The cued objects were not just focused because they might have been salient by themselves (due to e.g. positioning), or because they were cued by another object, but became more salient merely by the persons reference.<!--detailierter?!--> <!--Additionally finding is in line with general predictions from the social attention approach. First, that social stimuli are prioritized was confirmed by the results, the most prioritized region was still the head of the shown person.--> @Zwickel2010 provide evidence that joint attention is a direct consequence of gaze cues and gaze following, happens spontaneously and has high relevance even in situations that are more naturalistic (with complex scenes and absence of tasks).<!--softerer übergang pls-->

  <!-- Hypothesis -->
The given conceptual replication study aims at replicating and extending Zwickel and Võ's findings [-@Zwickel2010].
<!-- extended research | Top-Down Modulations-->
To extend this line of research the influence of top-down modulations on gaze following in naturalistic scenes is investigated. Earlier research showed that social attention is influenced by multiple factors like social status [@Foulsham2010] or action-related expectations [@Perez-Osorio2015]. These studies have in common [together with @Zwickel2010] that they manipulate viewing behavior of the participant by manipulating the stimuli. <!--For example, @Foulsham2010 build the stimulus set from stimuli that were previously rated for social status and confirmed the predicted shift towards hi in attention with eye-tracking measures.--> @Perez-Osorio2015, for example, showed stronger gaze following for action-congruent stimuli. @Zwickel2010 manipulated the source of the reference towards the objects, and showed that objects were prioritized only when cued by a person but not when cued by loudspeaker.

In the present study, however, viewing behavior should not be manipulated via the stimuli, but via top-down modulation. The motivation for the manipulation was twofold. First, it is thought to test robustness of the gaze following against top-down processes. Second, gaze following effects on memory can be observed. To investigate this question all participants performed a memory task after completing a free viewing task of static scenes. In the *explicit encoding group*, participants received the information that the viewing task is followed by a memory test before seeing the scenes. In contrast, the *free viewing group* did not receive this instruction in advance. Participants from the free viewing group should provide an unaffected viewing behavior as @Zwickel2010 demonstrated in their person condition.

*H1*. To replicate the findings from @Zwickel2010, a prioritization for cued objects is predicted for participants in the free viewing group. Identical to @Zwickel2010, this  prioritization is measured in multiple ways. First, there should be an early fixation bias towards cued objects. During presentation time, the total time that cued objects are fixated should be prolonged, with more fixations compared to uncued objects. For saccades it is expected, that saccades leaving from the head are more likely to land on cued objects, in contrast to uncued objects.<!--Main effect icond, ANOVA-->
  
The instruction in the explicit encoding group is thought to induce a more explicit and systematic encoding of the presented scenes resulting in less prioritization for the referenced object. For the free viewing group prioritization for the referenced object are expected (as stated under H1). Note, that it is not expected that the influence of the presence of the person vanish completely due to top-down modulation. However, the preference for the cued object is supposed to be weaker. For the participants in the explicit encoding group all objects should be equally important for succeeding the task. Prioritizing one object over the other is not beneficial for the memory task.
  
*H2*. For gaze following it is expected that the prioritization for the cued object will decrease for participants in the explicit encoding group compared to participants in the free viewing group. This should result in smaller differences between cued and uncued objects in the fixation measurements for the explicit encoding group. For leaving saccades it is also expected that the under H1 expected difference between cued and uncued objects decreases for participant in the explicit encoding group.<!--1st: (Interaction group:icond) (?!Hypothese zu Gruppen, dass explicit mehr focus auf Gegensätnde richtet, da anweisung? (Maineffect group) -->
  
Additionally, the consequences for later cognitive processes, here memory effects, are examined for the different objects and groups. The study aims at answering the question whether more attentional resources on an object results in enhanced memorability. <!--... whether a memory test is sensitive to the attentional resources spent on an object-->This effect is mainly expected for participants in the free viewing group. Those participants are expected to spend remarkably more attentional resources on cued objects but not on uncued objects.
  
*H3*. For the follow-up memory test it is assumed, that participants with announced memory test (in the explicit encoding group) will recall more items than participants without announced memory test (free viewing group), because they process the scenes more thorough. Additionally it is expected, that in the explicit encoding group cued objects are better recalled than uncued objects.
  
*H4*. As in @Zwickel2010 and other studies [e.g. @Freeth2013] a strong prioritization for the head over the body is expected for fixation duration, number of fixation and fixation latency. Additionally it is expected, that the prioritization for the head decreases for participants in the explicit encoding group compared to participants in the free viewing group due to the systematic scene processing.


# Methods
<!-- We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study. <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) -->

## Participants
In this study `r length(df.w.demo$vp)` subjects (`r length(df.w.demo$sex[df.w.demo$sex==1])` female and `r length(df.w.demo$sex[df.w.demo$sex!=1])` male) between `r min(df.w.demo$age)` and `r max(df.w.demo$age)` years (M = `r round(mean(df.w.demo$age),2)`, SD = `r round(sd(df.w.demo$age),2)`) participated voluntarily. All participant had normal or corrected vision and were recruited at the University of Würzburg's online subject pool or by blackboard. Participants received study credit or 5€. All participants gave informed consent. One participant was excluded due to missing data (resulting in n = `r length(df.w.et$vp)` for the eye tracking analysis).

## Stimuli and Apparatus
The experimental stimuli consisted of 4 pictures taken by camera of 26 different scenes. In each scene a single individual was looking at one of two objects. The direction of the gaze and the place of the objects were balanced, creating 104 unique naturalistic pictures in the end. For each participant, a set was randomly generated from this pool containing one version of each scene, resulting in 26 trials. Eye movements were tracked with the corneal-reflexion method and were recorded with an EyeLink1000 tower system, sampling at 1000 Hz. The eye tracker and stimuli were controlled by the software Presentation® (Neurobehavioral Systems).

## Design and Procedure
The experimental design was a 2 x 2 mixed design. First, as a two-level factor the instruction group was manipulated between participants, they were randomly assigned to a free viewing or explicit encoding group. Additionally, as a two-level within subject factor object role was manipulated, objects were either cued or uncued.

First, participants were asked to give full informed consent. Then the eye-tracker was calibrated for each participant. According to the manipulation, only half of the participants were told that  there is a follow-up memory test. All participants were then told to look at the following scenes as they would look at pictures at home. The presentation of the scenes was randomized. In each trial, the scene was presented for 10 seconds. The inter trial interval was randomized between 2 and 4 seconds. After the last trial participants filled in questionnaires [demographics, Autism-Spectrum Quotient  (AQ-k, German version, @Freitag2007]). To prevent primacy and recency effects the questionnaire session was placed as a buffer between the viewing task and memory task. After filling in the questionnaires, participants were asked to recall as many items as possible in a free recall memory task. Participants received credit or payment afterwards.


## Data analysis
For data processing and statistical analysis the the open-source statistical programming language *R* [@R-base][^Rpkg] is used with the packages *tidyverse* [@R-tidyverse] and for reproducible reporting *knitr* [@R-knitr] and *papaja* [@R-papaja] are used.
For the analysis of the eye tracking data standard configuration of SR Research’s EyeLink DataViewer software was used to categorize eye movements into saccades and fixations. Saccades were defined as eye movements exceeding a velocity threshold of 30°/sec or an acceleration threshold of 8.000°/sec². Fixations were defined as time periods between saccades. Regions of interest (ROI), the cued object, the uncued object, the head and the body, were color-coded by hand using GIMP (GNU Image Manipulation Program) to determine gaze locations.
ANOVAs were conducted for several measures of prioritization. Separate repeated measures ANOVAs (rmANOVA) with the between-subject factor instruction group and the within-subject factor object role were conducted for the dependent variables fixation latency of the objects, fixation duration of the objects, number of fixation of the objects and recalled items. Additionally, rmANOVAs are also conducted for the head and body region with the factor group with the dependent variables fixation latency of the region, fixation duration of the region, number of fixations of the region to investigate prioritization for the presented person.
For all analysis the a priori significance level of $\alpha$ = 0.05 was used. As effect sizes $\eta$² for ANOVAS are reported.
Questionnaire data will be pooled across several studies and are therefore not part of this manuscript.

[^Rpkg]: Specifically, `r cite_r("r-references.bib")` is used for all analyses.

## Variables
The duration and the number of all fixations is cumulated for each ROI in each trial. Specifically, for fixation duration the cumulative time a fixation rested on a ROI was divided by the total time spent fixating any other rest of the scene. For fixation number, fixations were count per ROI and divided by the total number of fixations.
An additional measurement of prioritization, particularly for early attentional allocation, is fixation latency. It describes in each trial the time for each ROI until first fixation.
These measures allow for effective comparisons of prioritization between the two relevant objects and between the head and the body.
To reveal direct relations between the head and the relevant objects saccades leaving the head are count for cued and uncued objects and divided by all saccades leaving the head.


# Results
```{r Descriptives fix*, echo = FALSE}

# Descriptives for fixation chacracteristics for joint attention

dscr.fix.rpl <- df.w.et %>%
  group_by(group) %>%
  summarise_at(vars(fix.face:fixlat.nongaze), funs(mean,sd,se=sd(.)/sqrt(n()))) %>% # mit den funs() die variablen vars(von:bis) berechnen
  gather(key, measure, fix.face_mean:fixlat.nongaze_se) %>% # ins longformat
  mutate(fix = as.factor(map(strsplit(key,"[[:punct:]]"), ~.x[1]) %>% unlist()),
         gazed = map(strsplit(key,"[[:punct:]]"), ~.x[2]) %>% unlist(),
         gazed = as.factor(substring(gazed,1)),
         stat = as.factor(map(strsplit(key,"[[:punct:]]"), ~.x[3]) %>% unlist()),
         key = as.factor("replication"),
         measure = ifelse(as.character(fix) != "fixlat", round(measure, 3), round(measure, 0))) %>%
  select(-measure, everything()) %>% # neusortieren der variablen
  select(key, everything())

dscr.fix.rpl$gazed <- factor(dscr.fix.rpl$gazed, levels=c("face", "body", "gaze", "nongaze"))

dscr.fix.zwckl <- data.frame(group="free",fix=c("fix","fix","fix","fix","fixn","fixn","fixn","fixn","fixlat","fixlat","fixlat","fixlat"), gazed=c("gaze","gaze", "nongaze", "nongaze"), stat=c("mean","se"), measure=c(0.08,.01,.07,.01,5.89,.37,5.24,.34,3588,133,4008,166)) # free = person

```

```{r Descriptives saccade, echo = FALSE}
dscr.sac.rpl <- df.w.et %>%
  group_by(group) %>%
  summarise_at(vars(sac.pgaze:sac.bnongaze), funs(mean,sd,se=sd(.)/sqrt(n()))) %>% # mit den funs() die variablen vars(von:bis) berechnen
  gather(key, measure, sac.pgaze_mean:sac.bnongaze_se) %>% # ins longformat
  mutate(stat = as.factor(map(strsplit(key,"[[:punct:]]"), ~.x[3]) %>% unlist()),
         area = map(strsplit(key,"[[:punct:]]"), ~.x[2]) %>% unlist(),
         area = as.factor(substring(area,1,1)),
         gazed = map(strsplit(key,"[[:punct:]]"), ~.x[2]) %>% unlist(),
         gazed = as.factor(substring(gazed,2)), 
         key = as.factor("replication")) %>% #rausnehmen, zur kontrolle, ob alles passt, was oben läuft.
  select(-measure, everything()) %>% # neusortieren der variablen
  select(key, everything())

dscr.sac.zwckl <- data.frame(key=c("zwickel","zwickel"),group=c("free", "free"),gazed=c("gaze", "gaze", "nongaze", "nongaze"),stat=c("mean", "se"),measure=c(.14,0.01,.09,0.01)) # free = person

```

```{r Descriptives memory, echo = FALSE}

dscr.mem <- df.w.mem %>% rename( group = bed) %>% 
  group_by(group) %>%
  summarise_at(vars(memgaze:memnogaze), funs(mean,sd,se=sd(.)/sqrt(n()))) %>%
  gather(key, value, memgaze_mean: memnogaze_se, factor_key = T) %>%
  separate(key, c("key", "stat"), sep = "\\_", extra = "merge") %>% 
  spread(stat, value) %>%
  mutate(lowerSE = mean - se,
         upperSE = mean + se)

```

```{r Table objects, echo = FALSE}

t1 <- dscr.fix.rpl %>%
  filter(group == "free", gazed == "gaze") %>% 
  select(fix, stat, measure) %>% 
  mutate(measure = ifelse(fix != "fixlat", measure*100, measure),
         Measurements = ifelse(fix == "fixlat", "Fixation latency (in ms)",
                      ifelse(fix == "fix", "Fixation duration (in %)",
                      ifelse( fix == "fixn", "Fixation number (in %)",NA)))) %>% 
  spread(key = "stat", value = "measure") %>% 
  select(Measurements:se)

t2 <- dscr.fix.rpl %>%
  filter(group == "free", gazed == "nongaze") %>% 
  select(fix, stat, measure) %>%
  mutate(measure = ifelse(fix != "fixlat", measure*100, measure)) %>% 
  spread(key = "stat", value = "measure")

t3 <- dscr.fix.rpl %>%
  filter(group == "mem", gazed == "gaze") %>% 
  select(fix, stat, measure) %>%
  mutate(measure = ifelse(fix != "fixlat", measure*100, measure)) %>% 
  spread(key = "stat", value = "measure")

t4 <- dscr.fix.rpl %>%
  filter(group == "mem", gazed == "nongaze") %>% 
  select(fix, stat, measure) %>%
  mutate(measure = ifelse(fix != "fixlat", measure*100, measure)) %>% 
  spread(key = "stat", value = "measure")
           

# apa_table(t5,
#           align = c("l", rep("c", 12)),
#           caption = "Mean Fixation Latency (in Milliseconds), relative fixation number (FF), and fixation duration as a function of Group (explicit encoding, free viewing) and object (cued, not gazed at",
#           note = "here is space for a note.",
#           #added_stub_head = "Variables",
#           #col_spanners = list(`free viewing`= c(2,7), `explicit encoding`= c(8,13)),
#           landscape = TRUE
#           )

```

```{r Table head, echo = FALSE}

t6 <- dscr.fix.rpl %>%
  filter(group == "free", gazed == "face") %>% 
  select(fix, stat, measure) %>% 
  mutate(measure = ifelse(fix != "fixlat", measure*100, measure),
         Measurements = ifelse(fix == "fixlat", "Fixation latency (in ms)",
                      ifelse(fix == "fix", "Fixation duration (in %)",
                      ifelse( fix == "fixn", "Fixation number (in %)",NA)))) %>% 
  spread(key = "stat", value = "measure") %>% 
  select(Measurements:se)

t7 <- dscr.fix.rpl %>%
  filter(group == "free", gazed == "body") %>% 
  select(fix, stat, measure) %>%
  mutate(measure = ifelse(fix != "fixlat", measure*100, measure)) %>% 
  spread(key = "stat", value = "measure")

t8 <- dscr.fix.rpl %>%
  filter(group == "mem", gazed == "face") %>% 
  select(fix, stat, measure) %>%
  mutate(measure = ifelse(fix != "fixlat", measure*100, measure)) %>% 
  spread(key = "stat", value = "measure")

t9 <- dscr.fix.rpl %>%
  filter(group == "mem", gazed == "body") %>% 
  select(fix, stat, measure) %>%
  mutate(measure = ifelse(fix != "fixlat", measure*100, measure)) %>% 
  spread(key = "stat", value = "measure")
           

# apa_table(t5,
#           align = c("l", rep("c", 12)),
#           caption = "Mean Fixation Latency (in Milliseconds), relative fixation number (FF), and fixation duration as a function of Group (explicit encoding, free viewing) and object (cued, not gazed at",
#           note = "here is space for a note.",
#           #added_stub_head = "Variables",
#           #col_spanners = list(`free viewing`= c(2,7), `explicit encoding`= c(8,13)),
#           landscape = TRUE
#           )

```

```{r tableObjects, results = 'asis', echo = FALSE}

t5 <- cbind(t1,t2[-1],t3[-1],t4[-1]) # only objects

kable(t5, format = "latex", longtable = F, booktabs = T, digits = 0, align =  c('l', rep("c",12)), caption = "Mean Fixation Latency (in Milliseconds), relative fixation number (FF), and fixation duration as a function of Group (explicit encoding, free viewing) and object (cued, uncued)") %>%
   add_header_above(c(" ", "cued" = 3, "uncued" = 3, "cued" = 3, "uncued" = 3)) %>%
   add_header_above(c(" ", "free viewing" = 6, "explicit encoding" = 6)) %>%
   footnote(general = "Here is a footnote.") #%>% 
   #landscape()
 
 rm(t1,t2,t3,t4,t5)

```

```{r tableHead, results = 'asis', echo = FALSE}

t10 <- cbind(t6, t7[-1],t8[-1], t9[-1])

kable(t10, format = "latex", booktabs = T, digits = 0, align =  c('l', rep("c",12)), caption = "Mean Fixation Latency (in Milliseconds), relative fixation number, and fixation duration as a function of Group (explicit encoding, free viewing) and person (head and body)") %>%
  add_header_above(c(" ", "head" = 3, "body" = 3, "head" = 3, "body" = 3)) %>%
   add_header_above(c(" ", "free viewing" = 6, "explicit encoding" = 6)) %>%
  footnote(general = "Here is a footnote.") %>%
  landscape()

rm(t6,t7,t8,t9,t10)

```

```{r Layout plots, echo = FALSE}
cTheme = theme(
    title = element_text(
      size = 8,
      #family = "Times New Roman", #excluded due to warnings
      color = "black"),
    text=element_text(
      size=12,
      #family = "Times New Roman", #excluded due to warnings
      color = "black"),
    axis.title=element_text(
      size=12,
      #family = "Times New Roman", #excluded due to warnings
      color = "black"),
    axis.line = element_line(
      colour = "black"),
    panel.grid = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.major.y = element_line(
      size = .01,
      colour = "grey"),
    panel.background = element_blank(),
    legend.text = element_text(
      size=12,
      color = "black"),
    legend.position = "none")
cColors = c("darkgrey", "lightgrey")
cLabels = c("free", "explicit")



```

```{r Plots objects, echo = FALSE}

cLimitsObjects = c(0,0.17)

p1.fixdur.rpl <- ggplot(subset(dscr.fix.rpl, stat == "mean" & fix == "fix" & gazed %in% c("gaze", "nongaze")), aes(x= group, y= measure, fill = gazed)) +
  geom_col(position = "dodge") +
  geom_errorbar(aes(
    ymin=subset(dscr.fix.rpl, stat == "mean" & fix == "fix" & gazed %in% c("gaze", "nongaze"))$measure - subset(dscr.fix.rpl, stat == "se" & fix == "fix" & gazed %in% c("gaze", "nongaze"))$measure,
    ymax=subset(dscr.fix.rpl, stat == "mean" & fix == "fix" & gazed %in% c("gaze", "nongaze"))$measure + subset(dscr.fix.rpl, stat == "se" & fix == "fix" & gazed %in% c("gaze", "nongaze"))$measure),
    width=.2,
    position=position_dodge(.9)) +
  #geom_signif(annotation="***", y_position=.19, xmin=1, xmax=2, tip_length = 0.01, vjust = 0.5) + # free - mem: p < .001
  #geom_signif(annotation="***", y_position=.42, xmin=1.188, xmax=2.188, tip_length = 0.01, vjust = 0.5) + # not-gazed free - not-gazed mem: p < .001, 
  #geom_signif(annotation="**", y_position=.40, xmin=0.81, xmax=1.81, tip_length = 0.01, vjust = 0.5) + # gazed free - gazed mem: p < .003
  #geom_signif(annotation="*", y_position=.17, xmin=0.81, xmax=1.188, tip_length = 0.01, vjust = 0.5) + # gazed free - not gazed free: p = .01
  #geom_signif(annotation="n.s.", y_position=.17, xmin=1.81, xmax=2.188, tip_length = 0.01, vjust = 0) + # gazed mem - not gazed mem: p = .679
  scale_y_continuous(name = element_blank(), labels = scales::percent_format(), limits = cLimitsObjects) +
  scale_x_discrete(name = element_blank(), labels=element_blank()) +
  scale_fill_manual(name = "Objects", labels = c("cued", "uncued"), values=cColors) +
  labs(title = "Objects", subtitle = "Fixation duration") 

cLegendObjects <- ggpubr::get_legend(p1.fixdur.rpl)

p1.fixdur.rpl <- p1.fixdur.rpl +
  cTheme

p2.fixnum.rpl <- ggplot(subset(dscr.fix.rpl, stat == "mean" & fix == "fixn" & gazed %in% c("gaze", "nongaze")), aes(x= group, y= measure, fill = gazed)) +
  geom_col(position = "dodge") +
  geom_errorbar(aes(
    ymin=subset(dscr.fix.rpl, stat == "mean" & fix == "fixn" & gazed %in% c("gaze", "nongaze"))$measure - subset(dscr.fix.rpl, stat == "se" & fix == "fixn" & gazed %in% c("gaze", "nongaze"))$measure,
    ymax=subset(dscr.fix.rpl, stat == "mean" & fix == "fixn" & gazed %in% c("gaze", "nongaze"))$measure + subset(dscr.fix.rpl, stat == "se" & fix == "fixn" & gazed %in% c("gaze", "nongaze"))$measure),
    width=.2,
    position=position_dodge(.9)) +
  ## Significanes from duration!!!
  #geom_signif(annotation="***", y_position=.19, xmin=1, xmax=2, tip_length = 0.01, vjust = 0.5) + # free - mem: p < .001
  #geom_signif(annotation="***", y_position=.42, xmin=1.188, xmax=2.188, tip_length = 0.01, vjust = 0.5) + # not-gazed free - not-gazed mem: p < .001, 
  #geom_signif(annotation="**", y_position=.40, xmin=0.81, xmax=1.81, tip_length = 0.01, vjust = 0.5) + # gazed free - gazed mem: p < .003
  #geom_signif(annotation="*", y_position=.17, xmin=0.81, xmax=1.188, tip_length = 0.01, vjust = 0.5) + # gazed free - not gazed free: p = .01
  #geom_signif(annotation="n.s.", y_position=.17, xmin=1.81, xmax=2.188, tip_length = 0.01, vjust = 0) + # gazed mem - not gazed mem: p = .679
  scale_y_continuous(name = element_blank(), labels = scales::percent_format(), limits = cLimitsObjects) +
  scale_x_discrete(name = element_blank(), labels=element_blank()) +
  scale_fill_manual(name = element_blank(), values=cColors) +
  labs(title = " ", subtitle = "Fixation number") +
  cTheme

p3.fixlat.rpl <- ggplot(subset(dscr.fix.rpl, stat == "mean" & fix == "fixlat" & gazed %in% c("gaze", "nongaze")), aes(x= group, y= measure, fill = gazed)) +
  geom_col(position = "dodge") +
  geom_errorbar(aes(
    ymin=subset(dscr.fix.rpl, stat == "mean" & fix == "fixlat" &  gazed %in% c("gaze", "nongaze"))$measure - subset(dscr.fix.rpl, stat == "se" & fix == "fixlat" & gazed %in% c("gaze", "nongaze"))$measure,
    ymax=subset(dscr.fix.rpl, stat == "mean" & fix == "fixlat" &  gazed %in% c("gaze", "nongaze"))$measure + subset(dscr.fix.rpl, stat == "se" & fix == "fixlat" & gazed %in% c("gaze", "nongaze"))$measure),
    width=.2,
    position=position_dodge(.9)) +
  #geom_signif(annotation="***", y_position=3300, xmin=1, xmax=2, tip_length = 0.01, vjust = 0.5) + # free - mem: p < .001
  #geom_signif(annotation="***", y_position=.42, xmin=1.188, xmax=2.188, tip_length = 0.01, vjust = 0.5) + # not-gazed free - not-gazed mem: p < .001, 
  #geom_signif(annotation="***", y_position=.40, xmin=0.81, xmax=1.81, tip_length = 0.01, vjust = 0.5) + # gazed free - gazed mem: p < .001
  #geom_signif(annotation="***", y_position=3100, xmin=0.81, xmax=1.188, tip_length = 0.01, vjust = 0.5) + # gazed free - not gazed free: p < .001
  #geom_signif(annotation="***", y_position=3100, xmin=1.81, xmax=2.188, tip_length = 0.01, vjust = 0.5) + # gazed mem - not gazed mem: p < .001
  scale_y_continuous(name = element_blank(), limits = c(0,3400)) +
  scale_x_discrete(name = element_blank(), labels=cLabels) +
  scale_fill_manual(name = element_blank(), values=cColors) +
  labs(title = " ", subtitle = "Fixation latency") +
  cTheme

p4.sac.rpl <- ggplot(subset(dscr.sac.rpl, stat == "mean" & area == "f"), aes(x= group, y= measure, fill = gazed)) +
  geom_col(position = "dodge") +
  geom_errorbar(aes(
    ymin=subset(dscr.sac.rpl, stat == "mean" & area == "f")$measure - subset(dscr.sac.rpl, stat == "se" & area == "f" )$measure,
    ymax=subset(dscr.sac.rpl, stat == "mean" & area == "f")$measure + subset(dscr.sac.rpl, stat == "se" & area == "f")$measure),
    width=.2,
    position=position_dodge(.9)) +
  #geom_signif(annotation="***", y_position=.15, xmin=1, xmax=2, tip_length = 0.01, vjust = 0.5) + # free - mem: p < .001
  #geom_signif(annotation="***", y_position=.42, xmin=1.188, xmax=2.188, tip_length = 0.01, vjust = 0.5) + # not-gazed free - not-gazed mem: p = .002 
  #geom_signif(annotation="**", y_position=.40, xmin=0.81, xmax=1.81, tip_length = 0.01, vjust = 0.5) + # gazed free - gazed mem: p < .001
  #geom_signif(annotation="***", y_position=.13, xmin=0.81, xmax=1.188, tip_length = 0.01, vjust = 0.5) + # gazed free - not gazed free: p = .001
  #geom_signif(annotation="***", y_position=.13, xmin=1.81, xmax=2.188, tip_length = 0.01, vjust = 0.5) + # gazed mem - not gazed mem: p = .001
  scale_y_continuous(name = element_blank(), labels = scales::percent_format(), limits = cLimitsObjects) +
  scale_x_discrete(name = element_blank(), labels=cLabels) +
  scale_fill_manual(name = element_blank(), values = cColors) +
  labs(title = " ", subtitle = "Saccades leaving head") +
  cTheme

p5.mem.rpl <- dscr.mem %>% 
  ggplot(aes(x = group, y=mean, fill = key)) +
  geom_col(position = "dodge") +
  geom_errorbar(aes(
    ymin=lowerSE,
    ymax=upperSE),
    width=.2,
    position=position_dodge(.9)) +
  #geom_signif(annotation="***", y_position=10.5, xmin=1, xmax=2, tip_length = 0.01, vjust = 0.5) + # free - mem: p < .001
  #geom_signif(annotation="***", y_position=.42, xmin=1.188, xmax=2.188, tip_length = 0.01, vjust = 0.5) + # not-gazed free - not-gazed mem: p < .001, 
  #geom_signif(annotation="**", y_position=.40, xmin=0.81, xmax=1.81, tip_length = 0.01, vjust = 0.5) + # gazed free - gazed mem: p < .003
  #geom_signif(annotation="n.s.", y_position=11, xmin=0.81, xmax=1.188, tip_length = 0.01, vjust = 0) + # gazed free - not gazed free: p = .01
  #geom_signif(annotation="n.s.", y_position=11, xmin=1.81, xmax=2.188, tip_length = 0.01, vjust = 0) + # gazed mem - not gazed mem: p = .679
  scale_y_continuous(name = element_blank(), breaks = seq(0,10, by = 5), limits = c(0,11)) +
  scale_x_discrete(name = element_blank(), labels=cLabels) +
  scale_fill_manual(name = element_blank(), values=cColors) +
  labs(title = " ", subtitle = "Memory performance") +
  cTheme

```

```{r barplotsObjects, fig.cap = "Bar plot of the different prioritization  measures for the fixation of the cued and uncued objects. Error bars represent standard error.", echo = FALSE, dev.args = list(bg = 'white'), fig.height=7}

gridExtra::grid.arrange(p1.fixdur.rpl, p2.fixnum.rpl, p3.fixlat.rpl, p4.sac.rpl, p5.mem.rpl, cLegendObjects, layout_matrix = rbind(c(1,2,6), c(3,4,5)))

rm(p1.fixdur.rpl, p2.fixnum.rpl, p3.fixlat.rpl, p4.sac.rpl, p5.mem.rpl, cLegendObjects, cLimitsObjects)

```

```{r Plots head, echo = FALSE}

cLimitsHeads = c(0,0.28)

p6.fixdur.rpl <- ggplot(subset(dscr.fix.rpl, stat == "mean" & fix == "fix" & gazed %in% c("face", "body")), aes(x= group, y= measure, fill = gazed)) +
  geom_col(position = "dodge") +
  geom_errorbar(aes(
    ymin=subset(dscr.fix.rpl, stat == "mean" & fix == "fix" & gazed %in% c("face", "body"))$measure - subset(dscr.fix.rpl, stat == "se" & fix == "fix" & gazed %in% c("face", "body"))$measure,
    ymax=subset(dscr.fix.rpl, stat == "mean" & fix == "fix" & gazed %in% c("face", "body"))$measure + subset(dscr.fix.rpl, stat == "se" & fix == "fix" & gazed %in% c("face", "body"))$measure),
    width=.2,
    position=position_dodge(.9)) +
  #geom_signif(annotation="***", y_position=.19, xmin=1, xmax=2, tip_length = 0.01, vjust = 0.5) + # free - mem: p < .001
  #geom_signif(annotation="***", y_position=.42, xmin=1.188, xmax=2.188, tip_length = 0.01, vjust = 0.5) + # not-gazed free - not-gazed mem: p < .001, 
  #geom_signif(annotation="**", y_position=.40, xmin=0.81, xmax=1.81, tip_length = 0.01, vjust = 0.5) + # gazed free - gazed mem: p < .003
  #geom_signif(annotation="*", y_position=.17, xmin=0.81, xmax=1.188, tip_length = 0.01, vjust = 0.5) + # gazed free - not gazed free: p = .01
  #geom_signif(annotation="n.s.", y_position=.17, xmin=1.81, xmax=2.188, tip_length = 0.01, vjust = 0) + # gazed mem - not gazed mem: p = .679
  scale_y_continuous(name = element_blank(), labels = scales::percent_format(), limits = cLimitsHeads) +
  scale_x_discrete(name = element_blank(), labels=cLabels) +
  scale_fill_manual(name = "Persons", labels = c("Head", "Body"), values=cColors) +
  labs(title = "Head and Body", subtitle = "Fixation duration") 

cLegendHead <- ggpubr::get_legend(p6.fixdur.rpl)

p6.fixdur.rpl <- p6.fixdur.rpl +
  cTheme


p7.fixnum.rpl <- ggplot(subset(dscr.fix.rpl, stat == "mean" & fix == "fixn" & gazed %in% c("face", "body")), aes(x= group, y= measure, fill = gazed)) +
  geom_col(position = "dodge") +
  geom_errorbar(aes(
    ymin=subset(dscr.fix.rpl, stat == "mean" & fix == "fixn" & gazed %in% c("face", "body"))$measure - subset(dscr.fix.rpl, stat == "se" & fix == "fixn" & gazed %in% c("face", "body"))$measure,
    ymax=subset(dscr.fix.rpl, stat == "mean" & fix == "fixn" & gazed %in% c("face", "body"))$measure + subset(dscr.fix.rpl, stat == "se" & fix == "fixn" & gazed %in% c("face", "body"))$measure),
    width=.2,
    position=position_dodge(.9)) +
  ## Significanes from duration!!!
  #geom_signif(annotation="***", y_position=.19, xmin=1, xmax=2, tip_length = 0.01, vjust = 0.5) + # free - mem: p < .001
  #geom_signif(annotation="***", y_position=.42, xmin=1.188, xmax=2.188, tip_length = 0.01, vjust = 0.5) + # not-gazed free - not-gazed mem: p < .001, 
  #geom_signif(annotation="**", y_position=.40, xmin=0.81, xmax=1.81, tip_length = 0.01, vjust = 0.5) + # gazed free - gazed mem: p < .003
  #geom_signif(annotation="*", y_position=.17, xmin=0.81, xmax=1.188, tip_length = 0.01, vjust = 0.5) + # gazed free - not gazed free: p = .01
  #geom_signif(annotation="n.s.", y_position=.17, xmin=1.81, xmax=2.188, tip_length = 0.01, vjust = 0) + # gazed mem - not gazed mem: p = .679
  scale_y_continuous(name = element_blank(), labels = scales::percent_format(), limits = cLimitsHeads) +
  scale_x_discrete(name = element_blank(), labels=cLabels) +
  scale_fill_manual(name = element_blank(), values=cColors) +
  labs(title = " ", subtitle = "Fixation number") +
  cTheme

p8.fixlat.rpl <- ggplot(subset(dscr.fix.rpl, stat == "mean" & fix == "fixlat" & gazed %in% c("face", "body")), aes(x= group, y= measure, fill = gazed)) +
  geom_col(position = "dodge") +
  geom_errorbar(aes(
    ymin=subset(dscr.fix.rpl, stat == "mean" & fix == "fixlat" &  gazed %in% c("face", "body"))$measure - subset(dscr.fix.rpl, stat == "se" & fix == "fixlat" & gazed %in% c("face", "body"))$measure,
    ymax=subset(dscr.fix.rpl, stat == "mean" & fix == "fixlat" &  gazed %in% c("face", "body"))$measure + subset(dscr.fix.rpl, stat == "se" & fix == "fixlat" & gazed %in% c("face", "body"))$measure),
    width=.2,
    position=position_dodge(.9)) +
  #geom_signif(annotation="***", y_position=3300, xmin=1, xmax=2, tip_length = 0.01, vjust = 0.5) + # free - mem: p < .001
  #geom_signif(annotation="***", y_position=.42, xmin=1.188, xmax=2.188, tip_length = 0.01, vjust = 0.5) + # not-gazed free - not-gazed mem: p < .001, 
  #geom_signif(annotation="***", y_position=.40, xmin=0.81, xmax=1.81, tip_length = 0.01, vjust = 0.5) + # gazed free - gazed mem: p < .001
  #geom_signif(annotation="***", y_position=3100, xmin=0.81, xmax=1.188, tip_length = 0.01, vjust = 0.5) + # gazed free - not gazed free: p < .001
  #geom_signif(annotation="***", y_position=3100, xmin=1.81, xmax=2.188, tip_length = 0.01, vjust = 0.5) + # gazed mem - not gazed mem: p < .001
  scale_y_continuous(name = element_blank(), limits = c(0,2800)) +
  scale_x_discrete(name = element_blank(), labels=cLabels) +
  scale_fill_manual(name = element_blank(), values=cColors) +
  labs(title = " ", subtitle = "Fixation latency") +
  cTheme

```

```{r barplotsHead, fig.cap = "Bar plot of the different prioritization  measures for the person in the scene for the head and body. Error bars represent standard error.", echo = FALSE, dev.args = list(bg = 'white'), fig.height = 3.5}

gridExtra::grid.arrange(p6.fixdur.rpl, p7.fixnum.rpl, p8.fixlat.rpl, cLegendHead, layout_matrix = rbind(c(1,2,3,4)))

rm(p6.fixdur.rpl, p7.fixnum.rpl, p8.fixlat.rpl, cLegendHead, cLimitsHeads, cTheme, cColors, cLabels)

```

```{r ANOVA fixations joint, echo = FALSE}

# ANOVA Fixations characteristics for joint attention
## 2 (Group) x 2 (Gaze) ANOVA

icond <- gl(2,1,labels=c("cued","uncued")) # within-factor
idata <- data.frame(icond)

for (st in seq(7,16,4)) { # Variables 7:16, every 4th: fix.gaze and fix.nogaze; fixn.gaze and ...
  carmod <- lm(as.matrix(df.w.et[,st:(st+1)]) ~ df.w.et$group)
  #print(colnames(df.w.et[,st:(st+1)]))
  #print(Anova(carmod, idata=idata, idesign=~icond, type="III"))
  assign(paste0("anova.",colnames(df.w.et)[st]), Anova(carmod, idata=idata, idesign=~icond, type="III"))
  assign(paste0("apa.anova.",colnames(df.w.et)[st]), apa_print(Anova(carmod, idata=idata, idesign=~icond, type="III"), correction="GG", mse = FALSE))
  }

rm(carmod, idata, icond, st)

```
## Gaze following
#### Fixation duration
<!-- (H1, H2) -->
In fixation duration the predicted difference, that participants fixated the cued object longer (see Figure\ \@ref(fig:barplotsObjects)) finds supported in the 2 x 2 ANOVA for fixation duration, indicating a significant main effect for reference, `r apa.anova.fix.gaze$full_result$icond`. Against prediction there was no effect of the two-way interaction, `r apa.anova.fix.gaze$full_result$df_w_et_group_icond` but a significant main effect for group `r apa.anova.fix.gaze$full_result$df_w_et_group`. Participants in the explicit group fixate objects longer than participants in the free viewing group. <!-- was it against pred? -->  <!-- The relative cumulative fixation duration is calculated as time duration on an object divided by--> 

#### Fixation latency
<!-- (H1, H2) -->
Faster fixations for the cued object compared to uncued objects (see Figure\ \@ref(fig:barplotsObjects)) are confirmed to be statistical significant by the 2 x 2 ANOVA for fixation latency `r apa.anova.fixlat.gaze$full_result$icond`. Instruction group is also significant, `r apa.anova.fixlat.gaze$full_result$df_w_et_group`, with faster fixations in the explicit encoding group compared to the free viewing group. The interaction shows a trend, `r apa.anova.fixlat.gaze$full_result$df_w_et_group_icond`, suggesting a smaller differences between objects in the *explicit encoding group*.

#### Fixation number
<!-- (H1, H2) -->
That cued objects were fixated more often (See Figure\ \@ref(fig:barplotsObjects)) is confirmed in the 2 x 2 ANOVA on fixation number, `r apa.anova.fixn.gaze$full_result$icond`. Objects were also more often fixated in the explicit encoding group`r apa.anova.fixn.gaze$full_result$df_w_mem`. Against prediction there was no interaction, `r apa.anova.fixn.gaze$full_result$df_w_et_group_icond`, indicating no difference between the objects in relation to instruction group.

### Saccades

```{r ANOVA saccades, warning = TRUE, message = TRUE}
# ANOVA Saccades
## 2 (Group) x 2 (Gaze) ANOVA

icond <- gl(2,1,labels=c("cued","uncued")) # within-factor
idata <- data.frame(icond)

for (st in 17:19) {
  carmod <- lm(as.matrix(df.w.et[,c(st,(st+3))]) ~ df.w.et$group)
  #print(colnames(df.w.et[,c(st,(st+3))]))
  #print(Anova(carmod, idata=idata, idesign=~icond, type="III"))
  assign(paste0("anova.",colnames(df.w.et)[st]), Anova(carmod, idata=idata, idesign=~icond, type="III"))
  assign(paste0("postHoc.",colnames(df.w.et)[st]), lsmeans::lsmeans(carmod, specs = c(names(carmod$model[2]), "rep.meas")))
  assign(paste0("apa.anova.",colnames(df.w.et)[st]), apa_print(Anova(carmod, idata=idata, idesign=~icond, type="III"), correction="GG", mse = FALSE))
}


rm(carmod, idata, icond, st)
```

<!-- (H1, H2) -->
That saccades leaving the head are more likely to land on the cued object (see Figure\ \@ref(fig:barplotsObjects)) is confirmed by a significant main effect in the 2 x 2 ANOVA for leaving saccades, `r apa.anova.sac.fgaze$full_result$icond`. The main effect for group,  `r apa.anova.sac.fgaze$full_result$df_w_et_group`, shows that saccades of participants in the explicit encoding group leave the head more often to both objects. No confirmation was found for the predicted interaction `r apa.anova.sac.fgaze$full_result$df_w_et_group_icond`.

### Memory
<!-- (H3) -->

```{r ANOVA memory, warning = TRUE, message = TRUE, echo = FALSE}

# Recalled items
# 2 (Condition) x 2 (Gaze) ANOVA (auf erinnerte Details)

imem <- gl(2,1,labels=c("cued","uncued"))
idata <- data.frame(imem)

# Car
carmod <- lm(as.matrix(df.w.mem[,3:4]) ~ df.w.mem$bed)
#print(colnames(df.w.mem[c(3,4)]))
#print(Anova(carmod, idata=idata, idesign=~imem, type="III"))
anova.mem <- Anova(carmod, idata=idata, idesign=~imem, type="III")
apa.anova.mem <- apa_print(Anova(carmod, idata=idata, idesign=~imem, type="III"), correction="GG", mse = FALSE)


rm(imem, idata, carmod)
```

The free-recall memory test showed, that participants in the explicit encoding group remembered more items than participants from the other group (see Figure\ \@ref(fig:barplotsObjects)). The corresponding 2 x 2 ANOVA indicates its statistical significance, `r apa.anova.mem$full_result$df_w_mem_bed`. The main effect for objects was not significant, `r apa.anova.mem$full_result$imem`. There was also no interaction, `r apa.anova.mem$full_result$df_w_mem_bed_imem`, such that referencing an object would increase memorability only for the free viewing group, as it is stated in the hypothesis.

## Social prioritization
A similar pattern to the objects can be seen when comparing the head with the body region.
```{r Descriptives fixations social, warining = FALSE, message = FALSE}

# Descriptives for fixation chacracteristics for social attention

dscr.fixsoc <- df.w.et %>% summarise(mean(fix.face), sd(fix.face), mean(fix.body), sd(fix.body), mean(fixn.face), sd(fixn.face), mean(fixn.body), sd(fixn.body), mean(fixlat.face), sd(fixlat.face), mean(fixlat.body), sd(fixlat.body))
dscr.fixsoc.group <- df.w.et %>% group_by(group) %>% summarise(mean(fix.face), sd(fix.face), mean(fix.body), sd(fix.body), mean(fixn.face), sd(fixn.face), mean(fixn.body), sd(fixn.body), mean(fixlat.face), sd(fixlat.face), mean(fixlat.body), sd(fixlat.body))
```

```{r ANOVA fixations social, warning = FALSE, message = FALSE}
# ANOVA Fixations characteristics
## 2 (Group) x 2 (Face/Body) ANOVA

icond <- gl(2,1,labels=c("head","body")) # within-factor
idata <- data.frame(icond)

for (st in seq(5,16,4)) { # Variables 5:16, every 4th: fix.face and fix.noface; fixn.face and ...
    carmod <- lm(as.matrix(df.w.et[,st:(st+1)]) ~ df.w.et$group)
  #print(Anova(carmod, idata=idata, idesign=~icond, type="III"))
  assign(paste0("anova.",colnames(df.w.et)[st]), Anova(carmod, idata=idata, idesign=~icond, type="III"))
  assign(paste0("apa.anova.",colnames(df.w.et)[st]), apa_print(Anova(carmod, idata=idata, idesign=~icond, type="III"), correction="GG", mse = FALSE))
  }


rm(carmod, idata, icond, st)

```

#### Fixation duration
That head regions were fixated longer than the body (see Figure\ \@ref(fig:barplotsHead)) finds support in the corresponding 2 x 2 ANOVA on fixation duration for the main effect region, `r apa.anova.fix.face$full_result$icond`. No difference are found for the main effect instruction groups, `r apa.anova.fix.face$full_result$df_w_et_group`, and for the interaction, `r apa.anova.fix.face$full_result$df_w_et_group_icond`.

#### Fixation number
The same pattern can be seen for fixation number (see Figure\ \@ref(fig:barplotsHead)). The 2 x 2 ANOVA on fixation number shows a significant main effect for the region, `r apa.anova.fixn.face$full_result$icond` and no difference between the groups, `r apa.anova.fixn.face$full_result$df_w_et_group`, and no interaction `r apa.anova.fixn.face$full_result$df_w_et_group_icond`.

#### Fixation latency
Latencies of fixation differed also remarkably between head and body (see Figure\ \@ref(fig:barplotsHead)). The ANOVA showed a significant effect for region `r apa.anova.fixlat.face$full_result$icond` with earlier fixations of the head compared to the body. There was neither an effect for instruction, `r apa.anova.fixlat.face$full_result$df_w_et_group` nor was there an interaction, `r apa.anova.fixlat.face$full_result$df_w_et_group_icond`.

### free viewing vs explicit encoding for face fixations (H5)
```{r Plots free vs mem face fixations, warning = FALSE, message = FALSE}

# # plots
# 
# plot.face.free_vs_mem <- df.l.et %>%
#   filter(fixations=="fix" & region=="face") %>%
#   ggpubr::ggboxplot(x = "fixations", y = "value",
#                 fill = "group", palette =c("#00AFBB", "#E7B800", "#FC4E07"),
#                 add = "jitter", shape = "group") #+
#   #stat_compare_means()
# #plot.face.free_vs_mem

```

```{r T-test face fixations, warning = FALSE, message = FALSE}

# Fixations Face in Free Viewing vs. Explicit Encoding

for (st in seq(5,16,4)) {
  msd <- c(mean(df.w.et[df.w.et$group=="free",st]),sd(df.w.et[df.w.et$group=="free",st]),
           mean(df.w.et[df.w.et$group=="mem",st]),sd(df.w.et[df.w.et$group=="mem",st]))
  teststat <- t.test(df.w.et[,st] ~ df.w.et$group)
  
  assign(paste0("ttest.",colnames(df.w.et)[st]), data.frame(M.free=msd[1],SD.free=msd[2], M.mem=msd[3], SD.mem=msd[4],  df=teststat$parameter, t=teststat$statistic, p=teststat$p.value))
 
  assign(paste0("apa.ttest.",colnames(df.w.et)[st]), apa_print(t.test(df.w.et[,st] ~ df.w.et$group)))
  }


rm(msd, teststat, st) 

```

A post Hoc t-test between head prioritization measures for each instruction group provides information whether heads are differently processed dependent on the instruction participants get. It reveals, that only fixation latency differed significantly between groups, `r apa.ttest.fixlat.face$full_result`, with faster fixations for the free viewing group compared to the explicit encoding group (see Table\ \@ref(tab:tableHead) and\ \@ref(tab:tableHead)). For fixation duration there was only a trend for difference between groups, `r apa.ttest.fix.face$full_result` with longer fixations for participants in the free viewing group. In addition, fixation number did not differ, `r apa.ttest.fixn.face$full_result`.


# Discussion
<!-- Main motivation -->
The main attempt to replicate @Zwickel2010's findings on attentional prioritization from the mere presence of a person in a scene on gaze behavior was successful.

<!-- replication -->
As in @Zwickel2010 and others studies [for example @Birmingham2008], a strong prioritization can be seen for the head of the person in the scene. Accordingly, the head was fixated first, before any object of interest. Besides that, the heads were also fixated most often, meaning that during scene presentation participants kept looking at the head. Consequently, no object was fixated longer in total than the head.
Additionally, also in line with @Zwickel2010 cued objects were preferred over the object not being cued in a given trial. The preference for the cued object is consistent for all measures. First, the object that was cued was fixated remarkably earlier. Furthermore, cued objects were also fixated longer with more frequent fixations. Therefore, the gaze following does not only leads to more thorough processing during the whole time of scene presentation, but also guides attentional resources very early. All these findings together indicate a strong prioritization through the gaze cue of the person in the scene.

<!-- saccacades -->
Moreover, the prioritization of the head and the preference for the cued object indirectly suggest a link between these two regions. To investigate a direct relation leaving saccades from the head were examined. As shown in Figure\ \@ref(fig:barplotsObjects), saccades leaving the head are more likely to end on the cued object indicating a direct link between the head and the cued object.
So far, the given study replicates with pictures that are more naturalistic all aspects @Zwickel2010 demonstrated for the presence of a person.
<!--Although it remains unclear how the pictures changed the results compared to the computer rendered scenes, the higher power lead to more precise estimation of the effect sizes.--> As often, by being more naturalistic experimental control is reduced. The consequences, however, are hold to a minimum by producing the stimuli the same way. In particular, each scene was photographed four times (with the individual looking twice to the left and right to each object). Therefore, the pictures contain small unavoidable changes. @Zwickel2010 rotate the person in the computer rendered scene, they have complete control of all the changes, e.g. angel of the body or facial expression. Here, another picture was taken. As consequence, for example, the body orientation even within the four balanced scenes cannot be perfectly controlled and might differ slightly between pictures, even though it was controlled as much as possible.
Nevertheless, with higher power and stimuli, that are more naturalistic, this study underlines @Zwickel2010 findings.

<!-- top-down -->
Besides replicating, this study aims also at extending the line of research by testing the robustness of gaze following against top-down modulation, specifically an instructed task prior the viewing of the pictures. Unsurprisingly the memory task scores show that participants, who knew about the free recall task in advance perform better in recalling items. More interestingly, viewing behavior was different depending on instruction group. Specifically, participants pay more attention to objects when they received the instruction prior to the viewing part, and at the same time, the head loses some of its natural salience. Both findings are consistent over all measures. These findings suggest that participants with instruction had the intended systematic gaze pattern, where they preferred task relevant objects.
**However, against prediction no interaction between top-down modulation and reference of an object can be found.** Consistent for the person in the scene as well as the objects, prioritization in all measurements remains stable in the memory task instruction group. The prioritization shown for the head and for the cued object was unaffected by the given top-down modulation. The attentional guidance from gaze and therefore from social attentional processes can be seen, even when participants investigate the scene more systematic. This evidence provides support for the automaticity and reflexivness of joint and social attentional processes, even when the participant voluntarily aims at scanning the scene systematically. These findings are in line with earlier studies for gaze cueing in highly controlled studies [e.g. @Ristic2005, @Hayward2017], less controlled studies [e.g. @Castelhano2007, @Zwickel2010] and real life studies [e.g. @Foulsham2010, @Freeth2013, @Hayward2017].
An interesting pattern is that all (descriptive) differences of prioritization consistent for all measurements becomes smaller for participants in the explicit encoding group (but only fixation latency shows a trend towards significance). This could be interpreted that the uncued object was still salient (e.g. due to positioning). This would be critical especially for participants in the free viewing group, where objects should differ in salience as a function of gaze cue. Gaining salience by position or size would lead to two objects being somewhat equally salient. The change in viewing behavior solely by the instruction would then be smaller then expected and presumably too small to be relevant. A solution would be to have the person referencing smaller objects, which are less salient by themselves or position. The instruction would then be more effective to manipulate the salience of the objects, presumably resulting in more variance for the prioritization measures.

As expected participants with instructed memory task performed better in the subsequent memory task. However, the contribution of the attentional reallocation remains unclear. In particular, **whether an object was cued or not by the person, did not influence its probability being recalled**. Toghether with the increased attention spent on the cued object, this is in contrast to studies on eye movements and memory performance showing that increased attention results in better memory performance [@Hollingworth2002]. From that perspective, the cued object should be recalled better than the uncued object, which is not represented by the given results. That is partially inconsistent with the hypothesis, which stated correct that scene processing changes, but the actual link is missing that the change in processing for solving the task can be explained from the attentional reallocation of attention. This missing link supports also the idea that even the uncued object (especially for participants in the free viewing group) was somewhat relevant to the participants. With the before mentioned adaptations, it is expected, that the memory task will become sensitive to attentional resources spent on objects.

<!-- Conclusio -->
Overall, the results provide additional support to previous findings that attention is shifted reflexively where other persons are looking at [e.g. @Ristic2005,  @Hayward2017]. This evidence, which was previously extended to free viewing of static naturalistic scenes by @Zwickel2010, is shown to be robust against top-down modulation. Even when attentional allocation changes due to a more systematic viewing pattern, social and joint attentional shifts are still affected by the mere presence of a person, comparable as if the participants perform unbiased free viewing. The results indicate that the mere presence of other human beings as well as their gaze orientation have a strong impact on attentional exploration. The observed attentional guidance of the gaze was so robust to resist even top-down modulation. It is concluded that attentional guidance by gaze following is very robust.

<!--# Reste Rampe-->

<!--**Nach der kognitiven Relevanz-Hypothese werden top-down- Prozesse kontrolliert über Ziele und Intentionen gesteuert (goal-driven), wohingegen bottom-up- Prozesse nach der Salienz-Hypothese durch Stimuli automatisch ausgelöst werden (stimuli-driven), wodurch die Aufmerksamkeit auf einen bestimmten Ort gelenkt wird (Borji & Itti, 2014). End & Gamer, 2017**-->

<!--The relevance of ecologically valid stimuli is a core aspect of social attention research. Social attention research describes attentional consequences of social interactions and focuses often on similarities and differences of different types of social stimuli [@Risko2012], from real human interactions [for example: @Laidlaw2011; @Freeth2013a] to highly controlled laboratory settings with isolated faces as stimuli [for example: @Langton2017].--> <!--The main finding of this research line is, that some research findings related to social stimuli do not extend to the real world. Researches must be sensible to these differences when generalizing laboratory findings [@Jarick2015; @Hayward2017; for an overview see: @Risko2016].
Additionally, laboratory studies showed, that joint attention cannot only be induced by the gaze [for an detailed overview see: @Frischen2007a], but also from the body or the head itself [@Lawson2016].-->

<!--In the given study, the motivation for the manipulation of the instructions for the memory test was twofold. First, it is thought to test robustness of the social and joint attention effects and second joint attentional effects on memory can be observed. The influence of the manipulation is expected to reduce spontaneous viewing behavior and foster a more systematic processing of the scene. It is important to note, that it is not expected that the influence of the presence of the person vanishes completely for social and joint attentional effects due to top-down modulation. It is supposed to be weaker, because it does not represent the optimal strategy for the participant. Therefore, the voluntary part of the scene processing might be overwritten. However, as social and joint attentional effects are expected to be (partially) independent of volition and occur spontaneously and reflexive the effects should still be visible regardless for the social attention part and for the joint attention part of this study. Specifically, prioritization effects for the head should be smaller but still visible in the *explicit encoding group*. For the joint attention measures a decreased prioritization for the referenced object is expected for the *explicit encoding group* is decreased compared with the uncued object. An optimal strategy for participant in the *explicit encoding group* would be to ignore completely the person in the scene, resulting in smaller differences in attentional prioritization between the two object roles.
Additionally, memory effects sensitive for the role of the object were examined, to answer the question whether more attentional resources on an object pays off with enhanced memorability. This is mainly interesting for the free viewing group with the unbiased viewing behavior, as it is expected that both objects bind comparable amount of attention due to the more systematic processing in the explicit encoding group.-->
<!-- participants from the free viewing group who did not receive any instruction in advance should provide an unaffected viewing behavior. Both groups had to recall as many as things as possible from the scenes in a free-recall memory test. With this manipulation it is thought to demonstrate top-down influence on social attention aspects, but also its influence on joint attention. As @Zwickel2010 showed, a person in a scene influences the viewing behavior of participants spontaneously and without further instruction or manipulation.-->
<!--*H4*. Additionally to the joint attentional effects stated in H1, the basic effects of social attention are expected. This means, that the head will be prioritized over the body. This prioritization can be measured, in multiple ways. Again, it is expected, that first fixation fall earlier on the head, that it is fixated longer and more often than the body. Additionally, fixations occur earlier on the head, than on the cued object, because the gaze cue needs to be processed in advance.--><!--gibt's gar keine Analyse für!!!-->
<!--*H5*. Due to the instruction of the memory test, and the assumed change in processing the scene, the viewing behavior should change for social stimuli. Due to explicit encoding, it is expected that natural viewing behavior that is known to prioritize the head is reduced and therefore the head loses some of its salience. As a consequence, the head should be prioritized stronger in the *free viewing group* compared to the *explicit encoding group*, resulting in longer fixations, faster fixations and more fixations.-->

<!--However, this replication included a top-down modulation, used photographic scenes and had considerably more power than the original study.-->

<!--Besides that, manipulation all participant did exactly the same experiment (with balanced and randomized stimuli). By instruction objects became relevant for succeeding in the memory task. Conclusively, attention towards the person would be inefficient.-->

<!--Unlike @Zwickel2010, naturalistic photographic are used, in contrast to computer rendered scenes.-->

<!--Note, however, that this was not a direct but a conceptual replication, showing that @Zwickel2010 findings can be generalized from computer rendered to naturalistic scenes.-->

<!--However, it is presumed that the effect showed by @Zwickel2010 is genuine for social stimuli. Therefore, no non-social condition comparable to the loudspeaker-condition was tested. -->

<!--#Schrott
Zwickel struktur: 1: general (social atteniton/head), 2: gaze cueing, 3(2a): diskutiert, 4(2b): diskutiert, 5: Leaving saccades, 6: head body, 7: surprising data, 8: vergleich mit unpassender studie, 9: conclusio
Meine struktur: 1: general (social attention/head) finding, 2: gaze cueing (object), 3: saccades (head -> object), 4: (kurzes oder detailliertes?!) fazit replication, 5: top down-modulation, 6: ?! DENK NACH! ?!, 7: conclusio
<!-- was will ich erzählen?! Replikation -> Kopf-prio, gazed-prio, sac-prio! --> 
<!-- was ist relevant?! --> 
<!-- was überrascht nicht?! topdown-modulatioon verändert blickverhalten--> 
<!-- was überrascht? Obwohl (!) top-down modulation, blickverhalten ändert, bleiben Effekte bestehen! --> 



<!-- alt: 

For genuine social stimuli like the person, and more specifically the head and the body, a similar pattern can be observed. The head had the lowest latency until first fixation, so it is even shorter than the latency till first fixation for the cued object. Under the light of joint attention this makes sense, since it is assumed, that the head in the first place leads the gaze of the participant to the cued object. Additionally, the head was also fixated longer and more often than the body.

The extension of this line of research, namely whether the gaze behavior can be influenced by top-down modulation. Consequently, this research is extended by the given study by an explicit task concerning the given scene. this modulation has an impact on gaze behavior.

-->

<!-- List bibtex errors:
  Freitag2007 -->

\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
