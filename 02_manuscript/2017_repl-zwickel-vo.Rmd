---
title             : "Gaze cueing in naturalistic scenes under top-down modulation - A conceptual replication"
shorttitle        : "Gaze cueing in naturalistic scenes"

author:
  - name          : "Jonas D. Großekathöfer"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Marcusstraße 9-11, 97070 Würzburg"
    email         : "jonas.grossekathoefer@uni-wuerzburg.de"
  - name          : "Kristina Suchotzki"
    affiliation   : "1"
  - name          : "Matthias Gamer"
    affiliation   : "1"  

affiliation:
  - id            : "1"
    institution   : "Julius-Maximilian University, Würzburg"

author_note: |
  Department of Psychology, Julius Maximilians University of Würzburg, Würzburg, Germany.

  Enter author note here.

abstract: |
  Humans as social beings rely on information provided by conspecifics. One important signal in social communication is eye gaze. The current study (n=93) sought to replicate and extend previous findings of attentional guidance by eye gaze in complex everyday scenes. In line with previous studies, longer, more and earlier fixations for objects cued by gaze were observed in free viewing conditions. To investigate how robust this prioritization is against top-down modulation, half of the participants receive a memory task that required scanning the whole scene instead of exclusively focusing on cued objects. Interestingly, similar gaze cueing effects occurred in this group. Moreover, the human beings depicted in the scene received a large amount of attention even though they were irrelevant to the current task. These results indicate that the mere presence of other human beings as well as their gaze orientation have a strong impact on attentional exploration.


keywords          : "keywords"
wordcount         : "?wordcountaddin"

bibliography      : 
  - "r-references.bib"
  - "../2017_repl-zwickel-vo.bib"

figsintext        : yes
figurelist        : no
tableintext       : no
tablelist         : no
footnotelist      : no
lineno            : no

documentclass     : "apa6"
classoption       : "man"
output            : 
  papaja::apa6_pdf:
    keep_tex: TRUE
---

```{r load_packages, include = FALSE}

library(tidyverse)
#library(ggsignif)
library(car)
library(knitr)
library(kableExtra)
library(scales)
library(ggpubr)
library(gridExtra)
library(lsmeans)
library(ggplot2)
library(papaja)

```

```{r Paths}

rm(list=ls())

pathMEM <- "../01_data/Memory/"
pathET <- "../01_data/prot/"
pathFB <- "../01_data/FB/"

```

```{r Generate prot}

```

```{r Check stimuli}

```

```{r Check baseline quality}

```

```{r Analyze fixations}

```

```{r Reading eye tracking data in, warning = FALSE, message = FALSE}

# ALL EYE TRACKING DATA
vpn <- paste0("vpja",ifelse(c(1:78,81:96)<10,"0",""),c(1:78,81:96))
bed <- rep(c("free","mem"),47)

# ORIGINALLY ACQUIRED DATA
#vpn <- paste0("vpja",ifelse(c(1:78)<10,"0",""),c(1:78))
#bed <- rep(c("free","mem"),39)

bed <- bed[!(vpn %in% "vpja23")]  # missing data
vpn <- vpn[!(vpn %in% "vpja23")]  

# Loop over subjects
erg <- numeric(); nvalid <- numeric(); cleantime <- numeric()
for (vp in vpn) {
  #  print(vp)

  prot <- read.csv2(paste0(pathET,vp,"_Fixations.csv"))

  # Restrict to trials with valid baseline?
  nvalid <- c(nvalid,sum(prot$blok==1))
  prot <- prot[prot$blok==1,]

  cleantime <- c(cleantime,mean(prot$cleantime))

  erg <- rbind(erg,apply(prot[,8:ncol(prot)],2,mean,na.rm=TRUE))
}

df.w.et <- data.frame(code=vpn,group=bed,nvalid,cleantime,erg) %>% 
  mutate(
    code = as.factor(unlist(map(strsplit(as.character(code),"ja"), ~.x[2])))) %>% 
  rename(vp = code)
    


df.l.et <- gather(df.w.et, key, value, fix.face:sac.bnongaze, factor_key=TRUE) %>%
  mutate(
    key = as.character(key),
    fixations =
      as.factor(
        ifelse(startsWith(key, "fix."), "fix",
             ifelse(startsWith(key, "fixn."), "fixn",
                    ifelse(startsWith(key, "fixlat."), "fixlat",
                           ifelse(startsWith(key, "sac."), "sac", NA))))),
    region =
      as.factor(
        ifelse(endsWith(key, ".face"), "face",
               ifelse(endsWith(key, ".body"), "body",
                      ifelse(endsWith(key, ".gaze"), "gaze",
                             ifelse(endsWith(key, ".nongaze"), "nongaze",
                                    ifelse(endsWith(key, ".pgaze"), "pgaze",
                                           ifelse(endsWith(key, ".fgaze"), "fgaze",
                                                  ifelse(endsWith(key, ".bgaze"), "bgaze",
                                                         ifelse(endsWith(key, ".pnongaze"), "pnongaze",
                                                                ifelse(endsWith(key, ".fnongaze"), "fnongaze",
                                                                       ifelse(endsWith(key, ".bnongaze"), "bnongaze",NA)))))))))))) %>%
  arrange(vp)

rm(prot, bed, cleantime, nvalid, vp, vpn, erg)

```

```{r Reading memory data in, warning = FALSE, message = FALSE}

# ALL MEMORY DATA
vpn <- paste0("vpja",ifelse(c(1:78,81:96)<10,"0",""),c(1:78,81:96))
bed <- rep(c("free","mem"),47)

# ORIGINALLY ACQUIRED DATA
#vpn <- pasteo("vpja",ifelse(c(1:78)<10,"0",""),c(1:78))
#bed <- rep(c("free","mem"),39)

# Loop over subjects
erg <- numeric()
for (vp in vpn) {
  # print(vp)

  prot <- read.csv2(paste0(pathMEM,vp,".csv"))

  # Item recalled
  gaze <- sum(prot$memgazedat)
  nogaze <- sum(prot$memnongazedat)

  erg <- rbind(erg,c(gaze,nogaze))
}

df.w.mem <- data.frame(code=vpn,bed,erg) %>% 
  mutate(
    code = as.factor(unlist(map(strsplit(
      as.character(code),"ja"), ~.x[2])))) %>%
  rename(vp = code)

names(df.w.mem) <- c("code","bed","memgaze","memnogaze")


rm(gaze, nogaze, erg, prot, bed, vp, vpn)

```

```{r Reading questionnaire data in, warning = FALSE, message = FALSE}

df.w.demo <- read_csv2(paste0(pathFB,"Projektarbeit_Dateneingabemaske.csv")) %>%
mutate_at(
    vars(AQK_1, AQK_3, AQK_5, AQK_6, AQK_7, AQK_9, AQK_10, AQK_11, AQK_14,
         AQK_16, AQK_17, AQK_18, AQK_20, AQK_22, AQK_23, AQK_24, AQK_26, AQK_28,
         AQK_31, AQK_32, AQK_33),
    funs(5 - .)) %>% # reverse variables
  mutate_at(
    vars(AQK_1:AQK_33),
    funs(ifelse(. <= 2, 1, ifelse(. <= 4, 0, NA)))) %>% # recode variable
   mutate_at(
     vars(AQK_1:AQK_33),
     funs(ifelse(is.na(.), round(mean(., na.rm = T),0), .))) %>% # replace NAs with mean
  transmute(
    vp = as.factor(VP_Nr),
    sex = as.factor(Demo_Sex),
    age = Demo_Alter,
    aqk_social = (AQK_1 + AQK_7 + AQK_8 + AQK_10 + AQK_11 + AQK_13 + AQK_14
                  + AQK_20 + AQK_24 + AQK_28 + AQK_31),
    aqk_imagination = (AQK_3 + AQK_5 + AQK_6 + AQK_9 + AQK_16 + AQK_17 + AQK_18 
                       + AQK_22 + AQK_23 + AQK_26 + AQK_32 + AQK_33),
    aqk_communication = (AQK_2 + AQK_4 + AQK_12 + AQK_15 + AQK_19 + AQK_21
                         + AQK_25 + AQK_27 + AQK_29 + AQK_30),
    aqk_sumscore = (aqk_social + aqk_imagination + aqk_communication))
```

# Introduction
Humans in their social environment rely on the information conspecifics provide. This does not only hold for reading explicit signals, such as verbal communication in conversations, but also for implicit signals, such as eye gaze or nonverbal cues. Specifically, if an individual looks into a certain direction, this information is often read spontaneously by an observer who redirects his or her attention towards the referred object or location. Such guidance of someone else’s attention is called gaze following. As a consequence, joint attention is established.

  <!-- gaze cuing to joint attenion & social attention -->
The most frequently used paradigm to investigate such attentional shifts is the so-called gaze cueing paradigm [@Friesen1998; @Driver1999; @Langton2000; for a review see: @Frischen2007a]. This paradigm has been inspired by classical studies on spatial attention by @Posner1980 and consists of a centrally presented face with varying gaze direction followed by a subsequently presented target at either the cued location (i.e., the location that the face is looking at) or an uncued location (i.e., a location that is not being looked at by the face). Studies using this gaze cueing paradigm have demonstrated that gaze cues facilitate target processing as evident in smaller reaction times to targets at cued as compared to uncued locations [@Frischen2007a]. However, even though gaze cues are crucial for joint attention, this standard gaze cueing paradigm can be criticized for lacking ecological validity, because these studies typically used isolated heads [@Friesen1998; @Langton2000] or even cartoon heads [@Driver1999; @Ristic2005] as gaze cues [for an overview see: @Risko2012], whereas in field situations, gaze signals occur within a rich context of competing visual information. For example, in a recent study @Hayward2017 compared attentional measures of gaze following from laboratory (classical gaze cueing) and real world (real social engagement) and did not find reliable links between those measures.

  <!-- social attention -->
<!--Birmingham2008 und Birmingham2009b als vormacher und grundsätzliche Idee das problem anzugehen, weitere Ansätze-->As a compromise between rich but also less controllable field conditions and standardized by impoverished laboratory gaze cuing studies, @Zwickel2010 used pictures of a full person (instead of isolated heads or faces) as a directional cue within a naturalistic scene. In this study, the authors used a free viewing instruction, meaning that participants had no explicit task to fulfill. @Zwickel2010 argued, that the lack of a specific task puts gaze following to a stricter test since previous studies frequently used target detection tasks [e.g. @Langton2017] or comprised specific instructions such as asking participants to understand a scene [@Castelhano2007]. Consequently, it remains unclear to what degree gaze following occurred spontaneously or was caused by the specific task at hand. In detail, @Zwickel2010 presented participants multiple 3D rendered outdoor and indoor scenes for several seconds that always included two clearly visible objects as well as either a person or a loudspeaker that was directed towards one of these objects. The loudspeaker, that also represents an object with a clear spatial orientation, served as a control condition to ensure that gaze cueing effects are due to the social meaning (i.e., the direction of the depicted person’s gaze) as compared to a mere gaze following triggered by any directional cue.
The results of the study showed that participants fixated the cued object remarkably earlier, more often and longer than the uncued object. However, this prioritization of occurred only when the person looked at the object and was not evident for images that included the loudspeaker. By showing that leaving saccades from the head most often landed onto the cued object, the results gave further evidence for the direct influence of eye gaze on attentional guidance <!-- and highlight therefore the innewohnende relevance of gaze cues-->
Crucially, similar effects were not obtained for the loudspeaker as a non social directional cue. The cued objects were not just focused because they might have been salient by themselves (e.g. due to positioning), or because they were cued by another object, but became more salient merely by the person's reference.<!--detailierter?!--> <!--Additionally finding is in line with general predictions from the social attention approach. First, that social stimuli are prioritized was confirmed by the results, the most prioritized region was still the head of the shown person.--> To sum up, @Zwickel2010 provide convincing evidence that joint attention is a direct consequence of gaze cues and gaze following, it happens spontaneously and has high relevance even in situations that are more naturalistic (i.e., involve complex scenes and the absence of explicit tasks) than classical gaze cueing studies based on variations of the Posner paradigm.<!--softerer übergang pls-->

  <!-- Hypothesis -->
First, we were interested in whether the previously reported effects hold when using a different set of stimuli. Due to their resolution and reduced richness of details, the originally used 3D rendered scenes did not allow for an assessment of the depicted person’s gaze direction. As a consequence, the observed cueing effects were rather due to directional information inferred from the body and head of the person that were congruently aligned towards one of the objects. To replicate these findings, we developed a new set of photographic stimuli that had sufficient resolution to also allow for perceiving gaze direction. These photos always included a human being who directed his/her gaze towards one of two objects that were placed within reaching distance. In order to be consistent with the study of @Zwickel2010, the depicted person’s head and body were congruently aligned with his/her eye gaze.
Second, in order to extend this line of research, we manipulated top-down instructions to explore the susceptibility of gaze following effects in naturalistic scenes. Earlier research showed that social attention can be influenced by multiple factors like social status [@Foulsham2010] or action-related expectations [@Perez-Osorio2015]. These studies have in common [together with @Zwickel2010] that they manipulate viewing behavior of the participant by manipulating the stimuli. <!--For example, @Foulsham2010 build the stimulus set from stimuli that were previously rated for social status and confirmed the predicted shift towards hi in attention with eye-tracking measures. @Perez-Osorio2015, for example, showed stronger gaze following for action-congruent stimuli. @Zwickel2010 manipulated the source of the reference towards the objects, and showed that objects were prioritized only when cued by a person but not when cued by loudspeaker. -->
In the present study, however, we tried to modulate viewing behavior via top-down instructions. Specifically, half of the participants received an instruction beforehand, that they should try to remember as many objects from the scenes as possible (explicit encoding group). For the other half of the participants (free viewing group), the memory test that was accomplished after the experiment was unannounced and therefore reflected spontaneous encoding of the respective scene details. The motivation for this manipulation was twofold. First, it was thought to test the robustness of gaze following against top-down processes. Second, it allowed for examining gaze following effects on memory.
We expected to replicate the findings of @Zwickel2010 in the free viewing group. Specifically, we anticipated to observe an early fixation bias towards cued objects, an enhanced exploration of these details (i.e., more fixations and longer dwell times) and more saccades leaving the head towards the cued as compared to the uncued object.<!--Main effect icond, ANOVA-->
The instruction in the explicit encoding group was thought to induce a more systematic exploration of the presented scenes resulting in less prioritization of the cued object. With respect to the memory test that was accomplished after viewing the scenes, we expected better memory for cued as compared to uncued objects due to the increased attention on these details. Furthermore, we anticipated a generally enhanced recall performance in the explicit encoding group but a reduced prioritization of cued objects.
Finally, as previous studies showed a strong preference of fixating the head over body and background regions in static images [@End2017; @Freeth2013; @Zwickel2010], we expected to see a similar bias in the current study regarding dwell times, number of fixations and fixation latency. Additionally, we hypothesized that the prioritization for the head decreases when participants follow specific exploration goals such as in the explicit encoding group of the current study [cf., @Flechsenhar2017].

# Methods
<!-- We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study. <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) -->

## Participants

The cueing effects in fixations and saccades that were obtained by @Zwickel2010 can be considered large (Cohen's d~z~ > 0.70). However, since effects of the top-down modulation implemented in the current study might be smaller, we used a medium effect size for estimating the current sample size. When assuming an effect size of Cohen's $f$ = 0.25 at an $\alpha$ level of .05 and a moderate correlation of .40 between factor levels of the within-subjects manipulation object role (cued vs. uncued), a sample size of 66 participants is needed to reveal main effects of the object role or interaction effects between group an object role at a power of .95. Under such conditions, the power for detecting main effects of group is smaller (1-$\beta$ = .67). As a compromise, we aimed at examining 90 participants (plus eventual drop outs) to achieve a power of .80 for the main effect of group and > .95 for main and interaction effects involving the within subjects manipulation object role.  
Finally, `r length(df.w.demo$vp)` subjects participated voluntarily. All participant had normal or corrected vision and were recruited at the University of Würzburg's online subject pool or by blackboard. Participants received course credit or a financial compensation of 5€. All participants gave informed consent. One participant was excluded due to missing data,resulting in n = `r length(df.w.et$vp)` for the eye tracking analysis with (`r df.w.demo %>% filter(vp != 23) %>% summarise(n_female = sum(sex=="1")) %>% .$n_female` female and `r df.w.demo %>% filter(vp != 23) %>% summarise(n_male = sum(sex=="0"))%>% .$n_male` male participants between `r df.w.demo %>% filter(vp != 23) %>% summarise(min_age = min(age)) %>% .$min_age` and `r df.w.demo %>% filter(vp != 23) %>% summarise(max_age = max(age)) %>% .$max_age` years (M = `r df.w.demo %>% filter(vp != 23) %>% summarise(mean_age = round(mean(age),2)) %>% .$mean_age` years, SD = `r df.w.demo %>% filter(vp != 23) %>% summarise(sd_age = round(sd(df.w.demo$age),2)) %>% .$sd_age` years).

## Stimuli and Apparatus

The experimental stimuli consisted of 26 different indoor and outdoor scenes. In each scene a single individual was looking at one of two objects that were placed within reaching distance. The direction of the gaze (left/right) and the placement of the objects (object A and B left/right) were balanced by taking 4 photographs of each scene. This created 104 unique naturalistic pictures in total. For each participant, a set was randomly taken from this pool containing one version of each scene, resulting in 26 trials. Eye movements were tracked with the corneal reflection method and were recorded with an EyeLink1000plus tower system at, a sampling rate of 1000 Hz. The stimulation was controlled via Presentation® (Neurobehavioral Systems). All stimuli had a resolution of 1280 x 960 pixels and were displayed on a 24" LG 24MB65PY-B screen (resolution: 1920 x 1200 pixels, display size: 516.9 x 323.1 mm) with a refresh rate of 60 Hz. The viewing distance amounted to 50 cm thus resulting in a visual angle of 38.03° x 28.99° for the photographs.

## Design and Procedure

The experimental design was a 2 x 2 mixed design. First, as a two-level factor the instruction group was manipulated between participants. Each participant was either assigned to the free viewing or the explicit encoding group. Additionally, as a two-level within subject factor object role was manipulated, with objects being cued or uncued by the depicted individual in the scene.  
After arriving at the laboratory individually, participants were asked to give full informed consent. Then the eye-tracker was calibrated for each participant using a 9-point grid. According to the manipulation, only half of the participants were told that  there is a follow-up memory test for objects that are part of the depicted scenes. All participants were then told to look at the following scenes freely without specifying further exploration goals or mentioning the content of the scenes. The presentation of the pictures was randomized. Each trial started with the presentation of a fixation cross for one second, followed by the scene for 10 seconds. The inter-trial interval varied randomly between 1 and 3 seconds. After the last trial, participants filled in demographic questionnaires and completed the Autism-Spectrum Quotient scale [AQ-k, German version, @Freitag2007]. The questionnaires were mainly introduced to reduce recency effects in the memory task that was accomplished afterwards. Participants were asked to recall as many objects from the scene as possible and write them down on a blank sheet of paper. No time limit was given but after 10 minutes, the experimenter asked participants to come to an end. In fact, most participants stopped earlier and indicated that they do not recall further objects.  
Finally, participants received course credit or payment and were debriefed.

## Data analysis

For data processing and statistical analysis the the open-source statistical programming language *R* [@R-base][^Rpkg] was used with the packages *tidyverse* [@R-tidyverse] and *knitr* [@R-knitr] and *papaja* [@R-papaja]  for reproducible reporting.
For the analysis of the eye-tracking data, EyeLink's standard configuration was used to parse eye movements into saccades and fixations. Saccades were defined as eye movements exceeding a velocity threshold of 30°/sec or an acceleration threshold of 8.000°/sec². Fixations were defined as time periods between saccades. We determined the following regions of interest (ROI) by color coding respective images regions by hand using GIMP (GNU Image Manipulation Program): the cued object, the uncued object, the head and the body of the depicted person. Gaze variables of interest were calculated in a largely similar fashion as in @Zwickel2010. Specifically, we determined the cumulative duration and number of fixations on each ROI per trial. These values were divided by the total time or number of fixations, respectively, to yield proportions. As an additional measure of prioritization, particularly for early attentional allocation, we determined the latency of the first fixation that was directed towards each ROI. These measures allow for effective comparisons of prioritization between the two relevant objects and between the head and the body. To reveal direct relations between the head and the relevant objects, we calculated the proportion of saccades that left the head region of the depicted individual and landed on the cued and uncued objects, respectively.
The memory test was scored manually by comparing the list of recalled objects to the objects that appeared in the scenes. We separately scored whether cued or uncued objects were recalled and ignored any other reported details. Afterwards, we calculated the sum of recalled objects separately for cued and uncued details.
In order to analyze the influence of the experimental manipulations on eye-tracking and memory data, we carried out separate analyses of variance (ANOVAs) with the between-subject factor instruction group and the within-subject factor object role were conducted on the dependent variables fixation latency, fixation duration, number of fixation, proportion of saccades from the head towards the object and recalled items. To examine general effects of social attention, separate ANOVAs using the between-subject factor instruction group and the within-subject factor ROI (head vs. body region) were conducted conducted on the dependent variables fixation latency, fixation duration and number of fixations.
For all analysis the apriori significance level of $\alpha$ = 0.05 was used. As effect sizes generalized eta-square ($\eta^2_G$) for ANOVAS are reported, where guidelines suggest .26 as a large, .13 as a medium and .02 as a small effect [@Bakeman2005].

[^Rpkg]: Specifically, `r cite_r("r-references.bib")` is used for all analyses.

# Results

```{r Layout plots, echo = FALSE}

cTheme = theme(
    title = element_text(
      size = 8,
      #family = "Times New Roman", #excluded due to warnings
      color = "black"),
    text=element_text(
      size=12,
      #family = "Times New Roman", #excluded due to warnings
      color = "black"),
    axis.title=element_text(
      size=12,
      #family = "Times New Roman", #excluded due to warnings
      color = "black"),
    axis.line = element_line(
      colour = "black"),
    panel.grid = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.major.y = element_line(
      size = .01,
      colour = "grey"),
    panel.background = element_blank(),
    legend.text = element_text(
      size=12,
      color = "black"),
    legend.position = "none")
cColors = c("darkgrey", "lightgrey")
cLabels = c("free", "explicit")

```

```{r Descriptives fix*, echo = FALSE}

# Descriptives for fixation chacracteristics for joint attention

dscr.fix.rpl <- df.w.et %>%
  group_by(group) %>%
  summarise_at(vars(fix.face:fixlat.nongaze),
               funs(mean,sd,se=sd(.)/sqrt(n()))) %>% # mit den funs() die variablen vars(von:bis) berechnen
  gather(key, measure, fix.face_mean:fixlat.nongaze_se) %>% # ins longformat
  mutate(fix = as.factor(map(strsplit(key,"[[:punct:]]"), ~.x[1]) %>%
                           unlist()),
         gazed = map(strsplit(key,"[[:punct:]]"), ~.x[2]) %>%
           unlist(),
         gazed = as.factor(substring(gazed,1)),
         stat = as.factor(map(strsplit(key,"[[:punct:]]"), ~.x[3]) %>%
                            unlist()),
         key = as.factor("replication"),
         measure = ifelse(as.character(fix) != "fixlat", round(measure, 3),
                          round(measure, 0))) %>%
  select(-measure, everything()) %>% # neusortieren der variablen
  select(key, everything())

dscr.fix.rpl$gazed <- factor(dscr.fix.rpl$gazed, levels=c("face", "body",
                                                          "gaze", "nongaze"))

dscr.fix.zwckl <- data.frame(group="free",fix=c("fix","fix","fix","fix","fixn",
                                                "fixn","fixn","fixn","fixlat",
                                                "fixlat","fixlat","fixlat"),
                             gazed=c("gaze","gaze", "nongaze", "nongaze"),
                             stat=c("mean","se"),
                             measure=c(0.08,.01,.07,.01,5.89,.37,5.24,.34,3588,
                                       133,4008,166)) # free = person

```

```{r Descriptives saccade, echo = FALSE}
dscr.sac.rpl <- df.w.et %>%
  group_by(group) %>%
  summarise_at(vars(sac.pgaze:sac.bnongaze), funs(mean,sd,se=sd(.)/sqrt(n()))) %>% # mit den funs() die variablen vars(von:bis) berechnen
  gather(key, measure, sac.pgaze_mean:sac.bnongaze_se) %>% # ins longformat
  mutate(stat = as.factor(map(strsplit(key,"[[:punct:]]"), ~.x[3]) %>% unlist()),
         area = map(strsplit(key,"[[:punct:]]"), ~.x[2]) %>% unlist(),
         area = as.factor(substring(area,1,1)),
         gazed = map(strsplit(key,"[[:punct:]]"), ~.x[2]) %>% unlist(),
         gazed = as.factor(substring(gazed,2)), 
         key = as.factor("replication")) %>% #rausnehmen, zur kontrolle, ob alles passt, was oben läuft.
  select(-measure, everything()) %>% # neusortieren der variablen
  select(key, everything())

dscr.sac.zwckl <- data.frame(key=c("zwickel","zwickel"),group=c("free", "free"),
                             gazed=c("gaze", "gaze", "nongaze", "nongaze"),
                             stat=c("mean", "se"),measure=c(.14,0.01,.09,0.01)) # free = person

```

```{r Descriptives memory, echo = FALSE}

dscr.mem <- df.w.mem %>% rename( group = bed) %>% 
  group_by(group) %>%
  summarise_at(vars(memgaze:memnogaze), funs(mean,sd,se=sd(.)/sqrt(n()))) %>%
  gather(key, value, memgaze_mean: memnogaze_se, factor_key = T) %>%
  separate(key, c("key", "stat"), sep = "\\_", extra = "merge") %>% 
  spread(stat, value) %>%
  mutate(lowerSE = mean - se,
         upperSE = mean + se)

```

```{r Descriptives fixations social, warining = FALSE, message = FALSE}

# Descriptives for fixation chacracteristics for social attention

dscr.fixsoc <- df.w.et %>% summarise(mean(fix.face), sd(fix.face),
                                     mean(fix.body), sd(fix.body),
                                     mean(fixn.face), sd(fixn.face),
                                     mean(fixn.body), sd(fixn.body),
                                     mean(fixlat.face), sd(fixlat.face),
                                     mean(fixlat.body), sd(fixlat.body))
dscr.fixsoc.group <- df.w.et %>%
  group_by(group) %>%
  summarise(mean(fix.face), sd(fix.face),
            mean(fix.body), sd(fix.body),
            mean(fixn.face), sd(fixn.face),
            mean(fixn.body), sd(fixn.body),
            mean(fixlat.face), sd(fixlat.face),
            mean(fixlat.body), sd(fixlat.body))
```

## Gaze following
<!-- #### Fixation latency -->
<!-- (H1, H2) -->

```{r Prerequisits Table Objects, echo = FALSE}

t1 <- dscr.fix.rpl %>%
  filter(group == "free", gazed == "gaze") %>%
  select(fix, stat, measure) %>%
  mutate(measure = ifelse(fix != "fixlat", measure*100, measure),
         Measurements = ifelse(fix == "fixlat", "Fixation latency (in ms)",
                      ifelse(fix == "fix", "Fixation duration (in %)",
                      ifelse( fix == "fixn", "Fixation number (in %)",NA)))) %>%
  spread(key = "stat", value = "measure") %>%
  select(Measurements:se)

t2 <- dscr.fix.rpl %>%
  filter(group == "free", gazed == "nongaze") %>%
  select(fix, stat, measure) %>%
  mutate(measure = ifelse(fix != "fixlat", measure*100, measure)) %>%
  spread(key = "stat", value = "measure")

t3 <- dscr.fix.rpl %>%
  filter(group == "mem", gazed == "gaze") %>%
  select(fix, stat, measure) %>%
  mutate(measure = ifelse(fix != "fixlat", measure*100, measure)) %>%
  spread(key = "stat", value = "measure")

t4 <- dscr.fix.rpl %>%
  filter(group == "mem", gazed == "nongaze") %>%
  select(fix, stat, measure) %>%
  mutate(measure = ifelse(fix != "fixlat", measure*100, measure)) %>%
  spread(key = "stat", value = "measure")

# apa_table(t5,
#           align = c("l", rep("c", 12)),
#           caption = "Mean Fixation Latency (in Milliseconds), relative fixation number (FF), and fixation duration as a function of Group (explicit encoding, free viewing) and object (cued, not gazed at",
#           note = "here is space for a note.",
#           #added_stub_head = "Variables",
#           #col_spanners = list(`free viewing`= c(2,7), `explicit encoding`= c(8,13)),
#           landscape = TRUE
#           )

```

```{r Table Objects, results = 'asis', echo = FALSE}

t5 <- cbind(t1,t2[-1],t3[-1],t4[-1]) # only objects

t.fix.obj <- kable(t5, format = "latex", longtable = F, booktabs = T,
                           digits = 0, align =  c('l', rep("c",12)), escape = T,
                           caption = "Mean Fixation Latency (in Milliseconds), relative fixation number (FF), and fixation duration as a function of Group (explicit encoding, free viewing) and object (cued, uncued)") %>%
   add_header_above(c(" ", "cued" = 3, "uncued" = 3, "cued" = 3, "uncued" = 3)) %>%
   add_header_above(c(" ", "free viewing" = 6, "explicit encoding" = 6)) %>%
   footnote(general = "Here is a footnote.") #%>%
   #landscape()

rm(t1,t2,t3,t4,t5)

```

```{r Plots for Barplot Objects, echo = FALSE}

cLimitsObjects = c(0,0.17)

pl.fixdur.obj <- ggplot(subset(dscr.fix.rpl, stat == "mean" & fix == "fix"
                               & gazed %in% c("gaze", "nongaze")),
                        aes(x= group, y= measure, fill = gazed)) +
  geom_col(position = "dodge") +
  geom_errorbar(aes(
    ymin=subset(dscr.fix.rpl, stat == "mean" & fix == "fix"
                & gazed %in% c("gaze", "nongaze"))$measure - subset(
                  dscr.fix.rpl, stat == "se" & fix == "fix"
                  & gazed %in% c("gaze", "nongaze"))$measure,
    ymax=subset(dscr.fix.rpl, stat == "mean" & fix == "fix"
                & gazed %in% c("gaze", "nongaze"))$measure + subset(
                  dscr.fix.rpl, stat == "se" & fix == "fix"
                  & gazed %in% c("gaze", "nongaze"))$measure),
    width=.2,
    position=position_dodge(.9)) +
  #geom_signif(annotation="***", y_position=.19, xmin=1, xmax=2, tip_length = 0.01, vjust = 0.5) + # free - mem: p < .001
  #geom_signif(annotation="***", y_position=.42, xmin=1.188, xmax=2.188, tip_length = 0.01, vjust = 0.5) + # not-gazed free - not-gazed mem: p < .001,
  #geom_signif(annotation="**", y_position=.40, xmin=0.81, xmax=1.81, tip_length = 0.01, vjust = 0.5) + # gazed free - gazed mem: p < .003
  #geom_signif(annotation="*", y_position=.17, xmin=0.81, xmax=1.188, tip_length = 0.01, vjust = 0.5) + # gazed free - not gazed free: p = .01
  #geom_signif(annotation="n.s.", y_position=.17, xmin=1.81, xmax=2.188, tip_length = 0.01, vjust = 0) + # gazed mem - not gazed mem: p = .679
  scale_y_continuous(name = element_blank(), labels = percent_format(),
                     limits = cLimitsObjects, expand = c(0,0),
                     breaks = c(.05, .10, .15)) +
  scale_x_discrete(name = element_blank(), labels=cLabels) +
  scale_fill_manual(name = "Objects", labels = c("cued", "uncued"),
                    values=cColors) +
  labs(title = "Object fixations", subtitle = "Fixation duration")

cLegendObjects <- get_legend(pl.fixdur.obj)

pl.fixdur.obj <- pl.fixdur.obj +
  labs(title = "B", subtitle = "Fixation duration") +
  cTheme

pl.fixnum.obj <- ggplot(subset(dscr.fix.rpl, stat == "mean" & fix == "fixn"
                               & gazed %in% c("gaze", "nongaze")),
                        aes(x= group, y= measure, fill = gazed)) +
  geom_col(position = "dodge") +
  geom_errorbar(aes(
    ymin=subset(dscr.fix.rpl, stat == "mean" & fix == "fixn"
                & gazed %in% c("gaze", "nongaze"))$measure - subset(
                  dscr.fix.rpl, stat == "se" & fix == "fixn"
                  & gazed %in% c("gaze", "nongaze"))$measure,
    ymax=subset(dscr.fix.rpl, stat == "mean" & fix == "fixn"
                & gazed %in% c("gaze", "nongaze"))$measure + subset(
                  dscr.fix.rpl, stat == "se" & fix == "fixn"
                  & gazed %in% c("gaze", "nongaze"))$measure),
    width=.2,
    position=position_dodge(.9)) +
  ## Significanes from duration!!!
  #geom_signif(annotation="***", y_position=.19, xmin=1, xmax=2, tip_length = 0.01, vjust = 0.5) + # free - mem: p < .001
  #geom_signif(annotation="***", y_position=.42, xmin=1.188, xmax=2.188, tip_length = 0.01, vjust = 0.5) + # not-gazed free - not-gazed mem: p < .001,
  #geom_signif(annotation="**", y_position=.40, xmin=0.81, xmax=1.81, tip_length = 0.01, vjust = 0.5) + # gazed free - gazed mem: p < .003
  #geom_signif(annotation="*", y_position=.17, xmin=0.81, xmax=1.188, tip_length = 0.01, vjust = 0.5) + # gazed free - not gazed free: p = .01
  #geom_signif(annotation="n.s.", y_position=.17, xmin=1.81, xmax=2.188, tip_length = 0.01, vjust = 0) + # gazed mem - not gazed mem: p = .679
  scale_y_continuous(name = element_blank(), labels = percent_format(),
                     limits = cLimitsObjects, expand = c(0,0),
                     breaks = c(.05, .10, .15)) +
  scale_x_discrete(name = element_blank(), labels=cLabels) +
  scale_fill_manual(name = element_blank(), values=cColors) +
  labs(title = "C", subtitle = "Fixation number") +
  cTheme

pl.fixlat.obj <- ggplot(subset(dscr.fix.rpl, stat == "mean" & fix == "fixlat"
                               & gazed %in% c("gaze", "nongaze")),
                        aes(x= group, y= measure, fill = gazed)) +
  geom_col(position = "dodge") +
  geom_errorbar(aes(
    ymin=subset(dscr.fix.rpl, stat == "mean" & fix == "fixlat"
                &  gazed %in% c("gaze", "nongaze"))$measure - subset(
                  dscr.fix.rpl, stat == "se" & fix == "fixlat"
                  & gazed %in% c("gaze", "nongaze"))$measure,
    ymax=subset(dscr.fix.rpl, stat == "mean" & fix == "fixlat"
                &  gazed %in% c("gaze", "nongaze"))$measure + subset(
                  dscr.fix.rpl, stat == "se" & fix == "fixlat"
                  & gazed %in% c("gaze", "nongaze"))$measure),
    width=.2,
    position=position_dodge(.9)) +
  #geom_signif(annotation="***", y_position=3300, xmin=1, xmax=2, tip_length = 0.01, vjust = 0.5) + # free - mem: p < .001
  #geom_signif(annotation="***", y_position=.42, xmin=1.188, xmax=2.188, tip_length = 0.01, vjust = 0.5) + # not-gazed free - not-gazed mem: p < .001,
  #geom_signif(annotation="***", y_position=.40, xmin=0.81, xmax=1.81, tip_length = 0.01, vjust = 0.5) + # gazed free - gazed mem: p < .001
  #geom_signif(annotation="***", y_position=3100, xmin=0.81, xmax=1.188, tip_length = 0.01, vjust = 0.5) + # gazed free - not gazed free: p < .001
  #geom_signif(annotation="***", y_position=3100, xmin=1.81, xmax=2.188, tip_length = 0.01, vjust = 0.5) + # gazed mem - not gazed mem: p < .001
  scale_y_continuous(name = element_blank(), limits = c(0,3400),
                     expand = c(0,0)) +
  scale_x_discrete(name = element_blank(), labels=cLabels) +
  scale_fill_manual(name = element_blank(), values=cColors) +
  labs(title = "A", subtitle = "Fixation latency") +
  cTheme

pl.sac.obj <- ggplot(subset(dscr.sac.rpl, stat == "mean" & area == "f"),
                     aes(x= group, y= measure, fill = gazed)) +
  geom_col(position = "dodge") +
  geom_errorbar(aes(
    ymin=subset(dscr.sac.rpl, stat == "mean" & area == "f")$measure - subset(
      dscr.sac.rpl, stat == "se" & area == "f" )$measure,
    ymax=subset(dscr.sac.rpl, stat == "mean" & area == "f")$measure + subset(
      dscr.sac.rpl, stat == "se" & area == "f")$measure),
    width=.2,
    position=position_dodge(.9)) +
  #geom_signif(annotation="***", y_position=.15, xmin=1, xmax=2, tip_length = 0.01, vjust = 0.5) + # free - mem: p < .001
  #geom_signif(annotation="***", y_position=.42, xmin=1.188, xmax=2.188, tip_length = 0.01, vjust = 0.5) + # not-gazed free - not-gazed mem: p = .002
  #geom_signif(annotation="**", y_position=.40, xmin=0.81, xmax=1.81, tip_length = 0.01, vjust = 0.5) + # gazed free - gazed mem: p < .001
  #geom_signif(annotation="***", y_position=.13, xmin=0.81, xmax=1.188, tip_length = 0.01, vjust = 0.5) + # gazed free - not gazed free: p = .001
  #geom_signif(annotation="***", y_position=.13, xmin=1.81, xmax=2.188, tip_length = 0.01, vjust = 0.5) + # gazed mem - not gazed mem: p = .001
  scale_y_continuous(name = element_blank(), labels = percent_format(),
                     limits = cLimitsObjects, expand = c(0,0),
                     breaks = c(.05, .10, .15)) +
  scale_x_discrete(name = element_blank(), labels=cLabels) +
  scale_fill_manual(name = element_blank(), values = cColors) +
  labs(title = "D", subtitle = "Saccades leaving head") +
  cTheme

```

```{r ANOVA fixations joint, echo = FALSE}

# ANOVA Fixations characteristics for joint attention
## 2 (Group) x 2 (Gaze) ANOVA

icond <- gl(2,1,labels=c("cued","uncued")) # within-factor
idata <- data.frame(icond)

for (st in seq(7,16,4)) { # Variables 7:16, every 4th: fix.gaze and fix.nogaze; fixn.gaze and ...
  carmod <- lm(as.matrix(df.w.et[,st:(st+1)]) ~ df.w.et$group)
  #print(colnames(df.w.et[,st:(st+1)]))
  #print(Anova(carmod, idata=idata, idesign=~icond, type="III"))
  assign(paste0("anova.",colnames(df.w.et)[st]),
         Anova(carmod, idata=idata, idesign=~icond, type="III"))
  assign(paste0("apa.anova.",colnames(df.w.et)[st]),
         apa_print(Anova(carmod, idata=idata, idesign=~icond, type="III"),
                   correction="GG", mse = FALSE))
  }

rm(carmod, idata, icond, st)

```

A significant main effect of object role in the analysis of fixation latencies indicates earlier fixations on cued compared to uncued objects (see Figure \ \@ref(fig:FigureObjects) A), `r apa.anova.fixlat.gaze$full_result$icond`. The main effect of instruction group was also significant, `r apa.anova.fixlat.gaze$full_result$df_w_et_group`, with earlier fixations on both objects in the explicit encoding compared to the free viewing group. The interaction effect failed to reach statistical significance, `r apa.anova.fixlat.gaze$full_result$df_w_et_group_icond`.

```{r FigureObjects, fig.cap = "Bar plot of the different prioritization  measures for the fixation of the cued and uncued objects. Error bars represent standard error.", echo = FALSE, dev.args = list(bg = 'white'), , fig.height=3, fig.width= 7.5}

grid.pl.obj <- grid.arrange(pl.fixlat.obj, pl.fixdur.obj, pl.fixnum.obj,
                            pl.sac.obj, cLegendObjects, 
                            layout_matrix = rbind(c(1, 2, 3, 4, 5)))

#rm(pl.fixdur.obj, pl.fixnum.obj, pl.fixlat.obj, pl.sac.obj, pl.mem, cLegendObjects, cLimitsObjects)

```

<!--#### Fixation duration & number-->
<!-- (H1, H2) -->
Largely similar effects were obtained in the analyses of fixation duration and numbers . Significant main effects of object role indicate that participants fixated the cued object longer, `r apa.anova.fix.gaze$full_result$icond`, and more often then the uncued object, `r apa.anova.fixn.gaze$full_result$icond`. Explicit instructions also led to longer, `r apa.anova.fix.gaze$full_result$df_w_et_group`, and more fixations on the objects as compared to the free viewing condition, `r apa.anova.fixn.gaze$full_result$df_w_et_group`. The interaction effects of instruction group and object role were not statistically significant, neither for the duration, `r apa.anova.fix.gaze$full_result$df_w_et_group_icond`, nor for the number of fixations, `r apa.anova.fixn.gaze$full_result$df_w_et_group_icond` (see Figure\ \@ref(fig:FigureObjects) B & C).

<!--### Saccades-->

```{r ANOVA saccades, warning = TRUE, message = TRUE}
# ANOVA Saccades
## 2 (Group) x 2 (Gaze) ANOVA

icond <- gl(2,1,labels=c("cued","uncued")) # within-factor
idata <- data.frame(icond)

for (st in 17:19) {
  carmod <- lm(as.matrix(df.w.et[,c(st,(st+3))]) ~ df.w.et$group)
  #print(colnames(df.w.et[,c(st,(st+3))]))
  #print(Anova(carmod, idata=idata, idesign=~icond, type="III"))
  assign(paste0("anova.",colnames(df.w.et)[st]),
         Anova(carmod, idata=idata, idesign=~icond, type="III"))
  assign(paste0("postHoc.",colnames(df.w.et)[st]),
         lsmeans(carmod, specs = c(names(carmod$model[2]), "rep.meas")))
  assign(paste0("apa.anova.",colnames(df.w.et)[st]),
         apa_print(Anova(carmod, idata=idata, idesign=~icond, type="III"),
                   correction="GG", mse = FALSE))
}

rm(carmod, idata, icond, st)
```

<!-- (H1, H2) -->
Saccades leaving the head were more likely to land on the cued compared to the uncued object as confirmed by a significant main effect of object role, `r apa.anova.sac.fgaze$full_result$icond`. The main effect for group,  `r apa.anova.sac.fgaze$full_result$df_w_et_group`, shows that saccades of participants in the explicit encoding group were more often directed towards any of the objects as compared to the free viewing group. Again, the interaction effect of instruction group and object role fails to reach statistical significance `r apa.anova.sac.fgaze$full_result$df_w_et_group_icond` (see Figure\ \@ref(fig:FigureObjects) D).

## Memory for objects
<!-- (H3) -->

```{r Plot memory}

pl.mem <- dscr.mem %>%
  ggplot(aes(x = group, y=mean, fill = key)) +
  geom_col(position = "dodge") +
  geom_errorbar(aes(
    ymin=lowerSE,
    ymax=upperSE),
    width=.2,
    position=position_dodge(.9)) +
  #geom_signif(annotation="***", y_position=10.5, xmin=1, xmax=2, tip_length = 0.01, vjust = 0.5) + # free - mem: p < .001
  #geom_signif(annotation="***", y_position=.42, xmin=1.188, xmax=2.188, tip_length = 0.01, vjust = 0.5) + # not-gazed free - not-gazed mem: p < .001,
  #geom_signif(annotation="**", y_position=.40, xmin=0.81, xmax=1.81, tip_length = 0.01, vjust = 0.5) + # gazed free - gazed mem: p < .003
  #geom_signif(annotation="n.s.", y_position=11, xmin=0.81, xmax=1.188, tip_length = 0.01, vjust = 0) + # gazed free - not gazed free: p = .01
  #geom_signif(annotation="n.s.", y_position=11, xmin=1.81, xmax=2.188, tip_length = 0.01, vjust = 0) + # gazed mem - not gazed mem: p = .679
  scale_y_continuous(name = element_blank(), breaks = seq(0,10, by = 5),
                     limits = c(0,11), expand = c(0,0)) +
  scale_x_discrete(name = element_blank(), labels=cLabels) +
  scale_fill_manual(name = element_blank(), values=cColors) +
  labs(title = " ", subtitle = "Memory performance") +
  cTheme

```

```{r ANOVA memory, warning = TRUE, message = TRUE, echo = FALSE}

# Recalled items
# 2 (Condition) x 2 (Gaze) ANOVA (auf erinnerte Details)

imem <- gl(2,1,labels=c("cued","uncued"))
idata <- data.frame(imem)

# Car
carmod <- lm(as.matrix(df.w.mem[,3:4]) ~ df.w.mem$bed)
#print(colnames(df.w.mem[c(3,4)]))
#print(Anova(carmod, idata=idata, idesign=~imem, type="III"))
anova.mem <- Anova(carmod, idata=idata, idesign=~imem, type="III")
apa.anova.mem <- apa_print(Anova(carmod, idata=idata,
                                 idesign=~imem, type="III"),
                           correction="GG", mse = FALSE)


rm(imem, idata, carmod)
```

An analysis of the recall data showed, that participants in the explicit encoding group remembered more items than participants from the other group (see Figure\ \@ref(fig:barplotsObjects)), `r apa.anova.mem$full_result$df_w_mem_bed`. Neither the main effect of object role, `r apa.anova.mem$full_result$imem` nor the interaction effect were statistically significant, `r apa.anova.mem$full_result$df_w_mem_bed_imem` (see Figure\ \@ref(fig:FigureMemory)).

```{r FigureMemory, fig.cap = "Bar plot of the memory performance for the cued and uncued objects. Error bars represent standard error.", echo = FALSE, dev.args = list(bg = 'white'), fig.height=3, fig.width= 3}

grid.pl.mem <- grid.arrange(pl.mem, cLegendObjects,
                            layout_matrix = rbind(c(1,2)))

```

## Social prioritization

```{r Prerequisits Table Head, echo = FALSE}

t6 <- dscr.fix.rpl %>%
  filter(group == "free", gazed == "face") %>%
  select(fix, stat, measure) %>%
  mutate(measure = ifelse(fix != "fixlat", measure*100, measure),
         Measurements = ifelse(fix == "fixlat", "Fixation latency (in ms)",
                      ifelse(fix == "fix", "Fixation duration (in %)",
                      ifelse( fix == "fixn", "Fixation number (in %)",NA)))) %>%
  spread(key = "stat", value = "measure") %>%
  select(Measurements:se)

t7 <- dscr.fix.rpl %>%
  filter(group == "free", gazed == "body") %>%
  select(fix, stat, measure) %>%
  mutate(measure = ifelse(fix != "fixlat", measure*100, measure)) %>%
  spread(key = "stat", value = "measure")

t8 <- dscr.fix.rpl %>%
  filter(group == "mem", gazed == "face") %>%
  select(fix, stat, measure) %>%
  mutate(measure = ifelse(fix != "fixlat", measure*100, measure)) %>%
  spread(key = "stat", value = "measure")

t9 <- dscr.fix.rpl %>%
  filter(group == "mem", gazed == "body") %>%
  select(fix, stat, measure) %>%
  mutate(measure = ifelse(fix != "fixlat", measure*100, measure)) %>%
  spread(key = "stat", value = "measure")

# apa_table(t5,
#           align = c("l", rep("c", 12)),
#           caption = "Mean Fixation Latency (in Milliseconds), relative fixation number (FF), and fixation duration as a function of Group (explicit encoding, free viewing) and object (cued, not gazed at",
#           note = "here is space for a note.",
#           #added_stub_head = "Variables",
#           #col_spanners = list(`free viewing`= c(2,7), `explicit encoding`= c(8,13)),
#           landscape = TRUE
#           )

```

```{r Table Head, results = 'asis', echo = FALSE}

t10 <- cbind(t6, t7[-1],t8[-1], t9[-1])

t.fix.head <- kable(t10, format = "latex", longtable = F, booktabs = T,
                        digits = 0, align =  c('l', rep("c",12)), escape = T,
                        caption = "Mean Fixation Latency (in Milliseconds), relative fixation number, and fixation duration as a function of Group (explicit encoding, free viewing) and person (head and body)") %>%
    add_header_above(c(" ", "head" = 3, "body" = 3, "head" = 3, "body" = 3)) %>%
    add_header_above(c(" ", "free viewing" = 6, "explicit encoding" = 6)) %>%
    footnote(general = "Here is a footnote.") %>%
    landscape()

rm(t6,t7,t8,t9,t10)

```

```{r Plots for Barplot Heads, echo = FALSE}

cLimitsHeads = c(0,0.28)

pl.fixdur.head <- ggplot(subset(dscr.fix.rpl, stat == "mean" & fix == "fix"
                                & gazed %in% c("face", "body")),
                         aes(x= group, y= measure, fill = gazed)) +
  geom_col(position = "dodge") +
  geom_errorbar(aes(
    ymin=subset(dscr.fix.rpl, stat == "mean" & fix == "fix"
                & gazed %in% c("face", "body"))$measure - subset(
                  dscr.fix.rpl, stat == "se" & fix == "fix"
                  & gazed %in% c("face", "body"))$measure,
    ymax=subset(dscr.fix.rpl, stat == "mean" & fix == "fix"
                & gazed %in% c("face", "body"))$measure + subset(
                  dscr.fix.rpl, stat == "se" & fix == "fix"
                  & gazed %in% c("face", "body"))$measure),
    width=.2,
    position=position_dodge(.9)) +
  #geom_signif(annotation="***", y_position=.19, xmin=1, xmax=2, tip_length = 0.01, vjust = 0.5) + # free - mem: p < .001
  #geom_signif(annotation="***", y_position=.42, xmin=1.188, xmax=2.188, tip_length = 0.01, vjust = 0.5) + # not-gazed free - not-gazed mem: p < .001,
  #geom_signif(annotation="**", y_position=.40, xmin=0.81, xmax=1.81, tip_length = 0.01, vjust = 0.5) + # gazed free - gazed mem: p < .003
  #geom_signif(annotation="*", y_position=.17, xmin=0.81, xmax=1.188, tip_length = 0.01, vjust = 0.5) + # gazed free - not gazed free: p = .01
  #geom_signif(annotation="n.s.", y_position=.17, xmin=1.81, xmax=2.188, tip_length = 0.01, vjust = 0) + # gazed mem - not gazed mem: p = .679
  scale_y_continuous(name = element_blank(), labels = percent_format(),
                     limits = cLimitsHeads, expand = c(0,0),
                     breaks = c(.05, .10, .15, .20, .25)) +
  scale_x_discrete(name = element_blank(), labels=cLabels) +
  scale_fill_manual(name = "Persons", labels = c("Head", "Body"),
                    values=cColors) +
  labs(title = "B", subtitle = "Fixation duration")

cLegendHead <- get_legend(pl.fixdur.head)

pl.fixdur.head <- pl.fixdur.head +
  cTheme


pl.fixnum.head <- ggplot(subset(dscr.fix.rpl, stat == "mean" & fix == "fixn"
                                & gazed %in% c("face", "body")),
                         aes(x= group, y= measure, fill = gazed)) +
  geom_col(position = "dodge") +
  geom_errorbar(aes(
    ymin=subset(dscr.fix.rpl, stat == "mean" & fix == "fixn"
                & gazed %in% c("face", "body"))$measure - subset(
                  dscr.fix.rpl, stat == "se" & fix == "fixn"
                  & gazed %in% c("face", "body"))$measure,
    ymax=subset(dscr.fix.rpl, stat == "mean" & fix == "fixn"
                & gazed %in% c("face", "body"))$measure + subset(
                  dscr.fix.rpl, stat == "se" & fix == "fixn"
                  & gazed %in% c("face", "body"))$measure),
    width=.2,
    position=position_dodge(.9)) +
  ## Significanes from duration!!!
  #geom_signif(annotation="***", y_position=.19, xmin=1, xmax=2, tip_length = 0.01, vjust = 0.5) + # free - mem: p < .001
  #geom_signif(annotation="***", y_position=.42, xmin=1.188, xmax=2.188, tip_length = 0.01, vjust = 0.5) + # not-gazed free - not-gazed mem: p < .001,
  #geom_signif(annotation="**", y_position=.40, xmin=0.81, xmax=1.81, tip_length = 0.01, vjust = 0.5) + # gazed free - gazed mem: p < .003
  #geom_signif(annotation="*", y_position=.17, xmin=0.81, xmax=1.188, tip_length = 0.01, vjust = 0.5) + # gazed free - not gazed free: p = .01
  #geom_signif(annotation="n.s.", y_position=.17, xmin=1.81, xmax=2.188, tip_length = 0.01, vjust = 0) + # gazed mem - not gazed mem: p = .679
  scale_y_continuous(name = element_blank(), labels = percent_format(),
                     limits = cLimitsHeads, expand = c(0,0),
                     breaks = c(.05, .10, .15, .20, .25)) +
  scale_x_discrete(name = element_blank(), labels=cLabels) +
  scale_fill_manual(name = element_blank(), values=cColors) +
  labs(title = "C", subtitle = "Fixation number") +
  cTheme

pl.fixlat.head <- ggplot(subset(dscr.fix.rpl, stat == "mean" & fix == "fixlat"
                                & gazed %in% c("face", "body")),
                         aes(x= group, y= measure, fill = gazed)) +
  geom_col(position = "dodge") +
  geom_errorbar(aes(
    ymin=subset(dscr.fix.rpl, stat == "mean" & fix == "fixlat"
                &  gazed %in% c("face", "body"))$measure - subset(
                  dscr.fix.rpl, stat == "se" & fix == "fixlat"
                  & gazed %in% c("face", "body"))$measure,
    ymax=subset(dscr.fix.rpl, stat == "mean" & fix == "fixlat"
                &  gazed %in% c("face", "body"))$measure + subset(
                  dscr.fix.rpl, stat == "se" & fix == "fixlat"
                  & gazed %in% c("face", "body"))$measure),
    width=.2,
    position=position_dodge(.9)) +
  #geom_signif(annotation="***", y_position=3300, xmin=1, xmax=2, tip_length = 0.01, vjust = 0.5) + # free - mem: p < .001
  #geom_signif(annotation="***", y_position=.42, xmin=1.188, xmax=2.188, tip_length = 0.01, vjust = 0.5) + # not-gazed free - not-gazed mem: p < .001,
  #geom_signif(annotation="***", y_position=.40, xmin=0.81, xmax=1.81, tip_length = 0.01, vjust = 0.5) + # gazed free - gazed mem: p < .001
  #geom_signif(annotation="***", y_position=3100, xmin=0.81, xmax=1.188, tip_length = 0.01, vjust = 0.5) + # gazed free - not gazed free: p < .001
  #geom_signif(annotation="***", y_position=3100, xmin=1.81, xmax=2.188, tip_length = 0.01, vjust = 0.5) + # gazed mem - not gazed mem: p < .001
  scale_y_continuous(name = element_blank(), limits = c(0,2800),
                     expand = c(0,0)) +
  scale_x_discrete(name = element_blank(), labels=cLabels) +
  scale_fill_manual(name = element_blank(), values=cColors) +
  labs(title = "A", subtitle = "Fixation latency") +
  cTheme

```

```{r ANOVA fixations social, warning = FALSE, message = FALSE}
# ANOVA Fixations characteristics
## 2 (Group) x 2 (Face/Body) ANOVA

icond <- gl(2,1,labels=c("head","body")) # within-factor
idata <- data.frame(icond)

for (st in seq(5,16,4)) { # Variables 5:16, every 4th: fix.face and fix.noface; fixn.face and ...
    carmod <- lm(as.matrix(df.w.et[,st:(st+1)]) ~ df.w.et$group)
  #print(Anova(carmod, idata=idata, idesign=~icond, type="III"))
  assign(paste0("anova.",colnames(df.w.et)[st]),
         Anova(carmod, idata=idata, idesign=~icond, type="III"))
  assign(paste0("apa.anova.",colnames(df.w.et)[st]),
         apa_print(Anova(carmod, idata=idata, idesign=~icond, type="III"),
                   correction="GG", mse = FALSE))
  }

rm(carmod, idata, icond, st)

```

<!--#### Fixation latency-->
Fixation latencies differed remarkably between the head and the body (see Figure\ \@ref(fig:barplotsHead)). Consequently, the ANOVA yielded a significant main effect of ROI, `r apa.anova.fixlat.face$full_result$icond` with earlier fixations of the head compared to the body. There was neither a statistically significant main effect of instruction group, `r apa.anova.fixlat.face$full_result$df_w_et_group` nor an interaction between both factors, `r apa.anova.fixlat.face$full_result$df_w_et_group_icond` (see Figure\ \@ref(fig:FigureHead) A).

```{r FigureHead, fig.cap = "Bar plot of the different prioritization measures for the person in the scene for the head and body. Error bars represent standard error.", echo = FALSE, dev.args = list(bg = 'white'), , fig.height = 3, fig.width= 6}

grid.pl.head <- grid.arrange(pl.fixlat.head, pl.fixdur.head,
                             pl.fixnum.head, cLegendHead,
                             layout_matrix = rbind(c(1,2,3,4)))

rm(pl.fixdur.head, pl.fixnum.head, pl.fixlat.head, cLegendHead, cLimitsHeads,
   cTheme, cColors, cLabels)

```

<!--#### Fixation duration & number-->
Fixation duration and numbers showed a very similar pattern with longer, `r apa.anova.fix.face$full_result$icond`, as well as more fixations on the head than the body ROI, `r apa.anova.fixn.face$full_result$icond` (see Figure\ \@ref(fig:barplotsHead)). Remarkably, the instruction group did not exhibit a statistically significant effect, neither for the fixation duration, `r apa.anova.fix.face$full_result$df_w_et_group`, nor for the number of fixation, `r apa.anova.fixn.face$full_result$df_w_et_group`. Furthermore, the interaction effects of instruction group and ROI failed to reach statistical significance for fixation duration, `r apa.anova.fix.face$full_result$df_w_et_group_icond` and fixation numbers, `r apa.anova.fixn.face$full_result$df_w_et_group_icond`. These findings indicate that the prioritization of social ROIs was unaffected by the explicit instruction to attend to objects depicted in the scene (see Figure\ \@ref(fig:FigureHead) B & C).

<!--### free viewing vs explicit encoding for face fixations (H5)-->
```{r Plots free vs mem face fixations, warning = FALSE, message = FALSE}

# # plots
# 
# plot.face.free_vs_mem <- df.l.et %>%
#   filter(fixations=="fix" & region=="face") %>%
#   ggpubr::ggboxplot(x = "fixations", y = "value",
#                 fill = "group", palette =c("#00AFBB", "#E7B800", "#FC4E07"),
#                 add = "jitter", shape = "group") #+
#   #stat_compare_means()
# #plot.face.free_vs_mem

```

```{r T-test face fixations, warning = FALSE, message = FALSE}

# Fixations Face in Free Viewing vs. Explicit Encoding

for (st in seq(5,16,4)) {
  msd <- c(mean(df.w.et[df.w.et$group=="free",st]),
           sd(df.w.et[df.w.et$group=="free",st]),
           mean(df.w.et[df.w.et$group=="mem",st]),
           sd(df.w.et[df.w.et$group=="mem",st]))
  teststat <- t.test(df.w.et[,st] ~ df.w.et$group)
  
  assign(paste0("ttest.",colnames(df.w.et)[st]),
         data.frame(M.free=msd[1],SD.free=msd[2], M.mem=msd[3], SD.mem=msd[4],
                    df=teststat$parameter, t=teststat$statistic,
                    p=teststat$p.value))
 
  assign(paste0("apa.ttest.",colnames(df.w.et)[st]),
         apa_print(t.test(df.w.et[,st] ~ df.w.et$group)))
  }

rm(msd, teststat, st) 

```

<!--A post Hoc t-test between head prioritization measures for each instruction group provides information whether heads are differently processed dependent on the instruction participants get. It reveals, that only fixation latency differed significantly between groups, `r apa.ttest.fixlat.face$full_result`, with faster fixations for the free viewing group compared to the explicit encoding group (see Table\ \@ref(tab:tableHead) and\ \@ref(tab:tableHead)). For fixation duration there was only a trend for difference between groups, `r apa.ttest.fix.face$full_result` with longer fixations for participants in the free viewing group. In addition, fixation number did not differ, `r apa.ttest.fixn.face$full_result`.-->

# Discussion
<!-- Main motivation -->
By using naturalistic scenes with rich detail, this study aimed at conceptually replicating previous findings of a general prioritization of social cues [i.e. heads and bodies, @Birmingham2008, @End2017, @Flechsenhar2017]  as well as previously reported gaze cueing effects elicited by a person being directed towards a specific object in the scene [@Zwickel2010]. Both effects could be replicated. In detail, heads of persons in the scene were fixated earlier and explored more extensively as compared to body regions but also cued and uncued objects.
Additionally, in line with @Zwickel2010 cued objects were preferred over uncued objects. They were fixated remarkably earlier, longer and more often. Thus, gaze following effects did not only occur with respect to a more thorough processing during the whole time of scene presentation, but were also evident in an early allocation of attentional resources after stimulus onset. All these findings together indicate a strong and automatic bias of using gaze cues for attentional guidance.

<!-- saccacades -->
Moreover, the prioritization of the head and the preference for the cued object indirectly suggest a link between these two regions. To investigate this relationship in more detail, we examined saccades leaving the head towards the cued and uncued object, respectively. As shown in Figure\ \@ref(fig:barplotsObjects), saccades leaving the head were significantly more likely to end on the cued than on the uncued object indicating a direct link between head and cued object fixations.
So far, current results fully replicate the findings of  @Zwickel2010 with a more naturalistic set of stimuli.
<!--Although it remains unclear how the pictures changed the results compared to the computer rendered scenes, the higher power lead to more precise estimation of the effect sizes.--> As often, by using more naturalistic material experimental control is reduced. We tried top minimize unsystematic effects by producing the stimuli in the same way as @Zwickel2010 but now using real as compared to 3D rendered scenes. In particular, each scene was photographed four times with gaze direction an object placement being fully counterbalanced. Since four individual photographs of each scene were taken in the current study, we could not fully control for all stimulus aspects. However, the full replication of the effects that were previously obtained with a different set of virtual scenes indicate that these effects generalize to naturalistic conditions and are stable against small variations in scene layout and presentation.

<!-- top-down -->
Besides replicating previous findings, this study also aimed at extending the line of research by testing the robustness of gaze following against top-down modulations. This was achieved by instructing half of the participants to memorize as many details of the presented scenes as possible. Since the depicted human beings are not relevant to this task, we expected a generally reduced attention towards head and body regions as well as a more systematic exploration pattern, potentially reducing gaze cueing effects in fixations on and saccades towards cued objects. Unsurprisingly the memory task that was accomplished after the eye tracking experiment showed that participants, who knew about the free recall task in advance performed better in recalling items. More interestingly, however, social attention as well as cueing effects in viewing behavior were largely unaffected by the explicit instruction viewing behavior were largely unaffected by the explicit instruction to remember as many objects from the scenes as possible. Specifically, whereas participants in the explicit recall group paid more attention towards depicted objects in general they did not show reduced attention towards head and body regions as compared to the free viewing group. Moreover, gaze cueing effects on fixation latencies and densities as well as the direction for saccades leaving the head region did not differ significantly between both experimental groups. These findings indicate that the prioritization of social information in general as well as of the cued objects in particular are largely unaffected by top-down instructions. The attentional guidance of gaze was effective, even when participants investigated the scenes with an explicit (non-social) task goal. This evidence provides support for the automaticity and reflexivity of social attentional processes and is in line with previous studies on gaze cueing within highly controlled studies [e.g., @Ristic2005, @Hayward2017], more naturalistic laboratory studies [e.g., @Castelhano2007, @Zwickel2010] and even real life social situations [e.g., @Foulsham2010, @Freeth2013, @Hayward2017]. Moreover, the current results are consistent with recent findings of an early attention bias towards social information [@End2017, @Roesler2017] that seems to be relatively resistant against specific top down instructions [@Flechsenhar2017].

As expected participants with specific recall instructions performed better in the subsequent memory task. However, the contribution of the automatic attentional processes to memory encoding remains unclear. In particular, although cued objects were prioritized in the attentional exploration, this did not increase their probability of being recalled. This is in contrast to studies on eye movements and memory performance showing that increased attention results in better memory performance [e.g., @Hollingworth2002]. From that perspective, the cued object should be recalled better than the uncued object. However, another study showed that if certain scene details have a special meaning (e.g., by being central to the content of a picture story), attention does no longer predict memory for these details [@Kim2013]. With respect to the current study, these findings may indicate that both objects that were placed within reaching distance of the depicted person conveyed such meaning and were therefore remembered with equal probability. Since we only tested for early memory effects, it would be very interesting to delay the memory test by at least 24h to examine whether memory consolidation differs between cued and uncued objects [@Squire].

<!-- Limitations -->
Although the current study has several strengths including the systematic generation of novel stimulus material and the large sample size, it also has some limitations that need to be mentioned.
First, although this study shows that humans follow other persons gaze implicitly in unconstrained situations, we show evidence for situations without real interactions between humans. Research shows, that fixation pattern differ remarkably when a real interaction between persons is possible [e.g., @Laidlaw2011, @Hayward2017, for an overview see: @Risko2016]. However, our findings add evidence to the classic highly controlled laboratory approaches to gaze cueing, approximating the need for more ecological research [@Risko2012].  
An additional limitations is that we haven't controlled for head or body orientation of the person in the scene. Consequently, the observed effects can't be attributed purely to gaze-cueing effects but to the directional orientation of the person overall. This is supported by studies, showing that head and body orientation is perceived as a social cue comparable to gaze direction [@Hietanen1999, @Lawson2016]. 

<!-- Conclusio -->
Overall, the current results provide additional support to previous findings that attention is shifted reflexively to locations where other persons are looking at [e.g. @Ristic2005,  @Hayward2017]. This evidence, which was previously extended to free viewing of more complex static scenes by @Zwickel2010, was shown to be robust against top-down modulation. Even when explicitly directing attention away from depicted individuals by making objects task-relevant, social and joint attentional shifts were still affected by the mere presence of a person, comparable to the unbiased free viewing condition. These results indicate that the mere presence of other human beings as well as their gaze orientation have a strong impact on attentional exploration.

<!--# Reste Rampe-->

<!--**Nach der kognitiven Relevanz-Hypothese werden top-down- Prozesse kontrolliert über Ziele und Intentionen gesteuert (goal-driven), wohingegen bottom-up- Prozesse nach der Salienz-Hypothese durch Stimuli automatisch ausgelöst werden (stimuli-driven), wodurch die Aufmerksamkeit auf einen bestimmten Ort gelenkt wird (Borji & Itti, 2014). End & Gamer, 2017**-->

<!--The relevance of ecologically valid stimuli is a core aspect of social attention research. Social attention research describes attentional consequences of social interactions and focuses often on similarities and differences of different types of social stimuli [@Risko2012], from real human interactions [for example: @Laidlaw2011; @Freeth2013a] to highly controlled laboratory settings with isolated faces as stimuli [for example: @Langton2017].The main finding of this research line is, that some research findings related to social stimuli do not extend to the real world. Researches must be sensible to these differences when generalizing laboratory findings [@Jarick2015; @Hayward2017; for an overview see: @Risko2016].
Additionally, laboratory studies showed, that joint attention cannot only be induced by the gaze [for an detailed overview see: @Frischen2007a], but also from the body or the head itself [@Lawson2016].-->

<!--In the given study, the motivation for the manipulation of the instructions for the memory test was twofold. First, it is thought to test robustness of the social and joint attention effects and second joint attentional effects on memory can be observed. The influence of the manipulation is expected to reduce spontaneous viewing behavior and foster a more systematic processing of the scene. It is important to note, that it is not expected that the influence of the presence of the person vanishes completely for social and joint attentional effects due to top-down modulation. It is supposed to be weaker, because it does not represent the optimal strategy for the participant. Therefore, the voluntary part of the scene processing might be overwritten. However, as social and joint attentional effects are expected to be (partially) independent of volition and occur spontaneously and reflexive the effects should still be visible regardless for the social attention part and for the joint attention part of this study. Specifically, prioritization effects for the head should be smaller but still visible in the *explicit encoding group*. For the joint attention measures a decreased prioritization for the referenced object is expected for the *explicit encoding group* is decreased compared with the uncued object. An optimal strategy for participant in the *explicit encoding group* would be to ignore completely the person in the scene, resulting in smaller differences in attentional prioritization between the two object roles.
Additionally, memory effects sensitive for the role of the object were examined, to answer the question whether more attentional resources on an object pays off with enhanced memorability. This is mainly interesting for the free viewing group with the unbiased viewing behavior, as it is expected that both objects bind comparable amount of attention due to the more systematic processing in the explicit encoding group.-->
<!-- participants from the free viewing group who did not receive any instruction in advance should provide an unaffected viewing behavior. Both groups had to recall as many as things as possible from the scenes in a free-recall memory test. With this manipulation it is thought to demonstrate top-down influence on social attention aspects, but also its influence on joint attention. As @Zwickel2010 showed, a person in a scene influences the viewing behavior of participants spontaneously and without further instruction or manipulation.-->
<!--*H4*. Additionally to the joint attentional effects stated in H1, the basic effects of social attention are expected. This means, that the head will be prioritized over the body. This prioritization can be measured, in multiple ways. Again, it is expected, that first fixation fall earlier on the head, that it is fixated longer and more often than the body. Additionally, fixations occur earlier on the head, than on the cued object, because the gaze cue needs to be processed in advance.--><!--gibt's gar keine Analyse für!!!-->
<!--*H5*. Due to the instruction of the memory test, and the assumed change in processing the scene, the viewing behavior should change for social stimuli. Due to explicit encoding, it is expected that natural viewing behavior that is known to prioritize the head is reduced and therefore the head loses some of its salience. As a consequence, the head should be prioritized stronger in the *free viewing group* compared to the *explicit encoding group*, resulting in longer fixations, faster fixations and more fixations.-->

<!--However, this replication included a top-down modulation, used photographic scenes and had considerably more power than the original study.-->

<!--Besides that, manipulation all participant did exactly the same experiment (with balanced and randomized stimuli). By instruction objects became relevant for succeeding in the memory task. Conclusively, attention towards the person would be inefficient.-->

<!--Unlike @Zwickel2010, naturalistic photographic are used, in contrast to computer rendered scenes.-->

<!--Note, however, that this was not a direct but a conceptual replication, showing that @Zwickel2010 findings can be generalized from computer rendered to naturalistic scenes.-->

<!--However, it is presumed that the effect showed by @Zwickel2010 is genuine for social stimuli. Therefore, no non-social condition comparable to the loudspeaker-condition was tested. -->

<!--#Schrott
Zwickel struktur: 1: general (social atteniton/head), 2: gaze cueing, 3(2a): diskutiert, 4(2b): diskutiert, 5: Leaving saccades, 6: head body, 7: surprising data, 8: vergleich mit unpassender studie, 9: conclusio
Meine struktur: 1: general (social attention/head) finding, 2: gaze cueing (object), 3: saccades (head -> object), 4: (kurzes oder detailliertes?!) fazit replication, 5: top down-modulation, 6: ?! DENK NACH! ?!, 7: conclusio
<!-- was will ich erzählen?! Replikation -> Kopf-prio, gazed-prio, sac-prio! --> 
<!-- was ist relevant?! --> 
<!-- was überrascht nicht?! topdown-modulatioon verändert blickverhalten--> 
<!-- was überrascht? Obwohl (!) top-down modulation, blickverhalten ändert, bleiben Effekte bestehen! --> 



<!-- alt: 

For genuine social stimuli like the person, and more specifically the head and the body, a similar pattern can be observed. The head had the lowest latency until first fixation, so it is even shorter than the latency till first fixation for the cued object. Under the light of joint attention this makes sense, since it is assumed, that the head in the first place leads the gaze of the participant to the cued object. Additionally, the head was also fixated longer and more often than the body.

The extension of this line of research, namely whether the gaze behavior can be influenced by top-down modulation. Consequently, this research is extended by the given study by an explicit task concerning the given scene. this modulation has an impact on gaze behavior.

-->

<!-- List bibtex errors:

-->

\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
