---
title             : "Gaze cueing in naturalistic scenes under top-down modulation - Effects on gaze behavior and memory performance"
shorttitle        : "Gaze cueing in naturalistic scenes"

author:
  - name          : "Jonas D. Großekathöfer"
    corresponding : yes    # Define only one corresponding author
    address       : "Marcusstraße 9-11, 97070 Würzburg"
    email         : "jonas.grossekathoefer@uni-wuerzburg.de"
  - name          : "Kristina Suchotzki"
  - name          : "Matthias Gamer"


affiliation:
  - id            : ""
    institution   : "Department of Psychology, University of Würzburg"

author_note: |
  Department of Psychology, Julius Maximilians University of Würzburg,
  Würzburg, Germany.

  Enter author note here.

abstract: |
  Humans as social beings rely on information provided by
  conspecifics. One important signal in social communication is eye
  gaze. The current study (*n* = 93) sought to replicate and extend
  previous findings of attentional guidance by eye gaze in complex
  everyday scenes. In line with previous studies, longer, more and
  earlier fixations for objects cued by gaze compared to objects that
  were not cued were observed in free viewing conditions. To
  investigate how robust this prioritization is against top-down
  modulation, half of the participants received a memory task that
  required scanning the whole scene instead of exclusively focusing on
  cued objects. Interestingly, similar gaze cueing effects occurred in
  this group. Moreover, the human beings depicted in the scene
  received a large amount of attention even though they were
  irrelevant to the current task. These results indicate that the mere
  presence of other human beings as well as their gaze orientation
  have a strong impact on attentional exploration.


keywords          : "Gaze Cueing, Joint Attention, Social Attention, Eye-Tracking, Top-Down Control, Memory Performace"
wordcount         : "5325"

bibliography      :
  - "../2017_repl-zwickel-vo.bib"

figsintext        : yes
figurelist        : no
tableintext       : no
tablelist         : no
footnotelist      : no
lineno            : no

documentclass     : "apa6"
classoption       : "man"
output            :
  papaja::apa6_word:
    #keep_tex: TRUE

header-includes: # found at https://crsh.github.io/papaja_man/limitations.html#customizing-the-document-preamble
  - \raggedbottom

---

```{r setup chunks, echo=FALSE}

knitr::opts_chunk$set(echo = FALSE, include = FALSE, warning = FALSE)


```


```{r load packages and data}

source("../02_scripts/01-libraries.R")
source("../02_scripts/02-paths.R")
source(path(path_scripts_data, "eyetracking.R"))
source(path(path_scripts_data, "memory.R"))
source(path(path_scripts_data, "questionnaire.R"))

source(path(path_scripts_publ, "plt_cosmetics.R")) # layout plots

```

# Introduction

Humans in their social environment rely on the information conspecifics provide.
This does not only hold for reading explicit signals, such as verbal
communication, but also for implicit signals, such as eye gaze or nonverbal
cues. Specifically, if an individual looks into a certain direction, this
information is often read spontaneously by an observer who redirects his or her
attention towards the referred object or location. Such guidance of someone
else’s attention is called gaze following. As a consequence, joint attention is
established.

 <!-- gaze cuing to joint attenion & social attention -->

The most frequently used paradigm to investigate such attentional shifts is the
so-called gaze cueing paradigm [@Friesen1998; @Driver1999; @Langton2000; for a
review see @Frischen2007a]. This paradigm has been inspired by classical studies
on spatial attention by @Posner1980 and consists of a centrally presented face
with varying gaze directions. This face is then followed by a subsequently
presented target at either the cued location (i.e., the location that the face
is looking at) or an uncued location (i.e., a location that is not being looked
at by the face). Studies using this gaze cueing paradigm have demonstrated that
gaze cues facilitate target processing as evident in smaller reaction times to
targets at cued as compared to uncued locations [@Frischen2007a]. **The paradigm was also used to show that gaze following is shaped by high-level social
cognitve processes like group identity [@Liuzza2011], theory-of-mind
[@Cole2015; @Teufel2009; @Wiese2012; @Wykowska2014] or physical self-similarity 
[@Hungr2012].**

<!-- critique -->

However, even though gaze cues are crucial for joint attention, this standard
gaze cueing paradigm can be criticized for lacking ecological validity. Whereas
in the real world, gaze signals occur within a rich context of competing visual
information, gaze cueing studies typically used isolated heads [@Friesen1998;
@Langton2000] or even cartoon faces [@Driver1999; @Ristic2005] as gaze cues [for
an overview see: @Risko2012]. **Although gaze cuing was also found with more
naturalistic stimuli [@Perez-Osorio2015]**, in a recent study in which
@Hayward2017 compared attentional measures of gaze following from laboratory
(classical gaze cueing) and real world (real social engagement) settings, they
did not find reliable links between those measures.

<!-- social attention -->

As a compromise between rich but also less controlled field conditions and
standardized but **impoverished laboratory studies complex naturalistic scenes
were used to investigate gaze behavior [e.g. @Fletcher-Watson2008;
@Zwickel2010; @Perez-Osorio2015]. To specifically investigate gaze cues,
@Zwickel2010 and @Perez-Osorio2015 used pictures of a person (instead of
isolated heads or faces) as a directional cue within a naturalistic scene.
@Zwickel2010 [in contrast to the gaze cueing task chosen by @Perez-Osorio2015]**
used a free viewing instruction, meaning that participants had no explicit task
to fulfill but should just freely explore the pictures. @Zwickel2010 argued that
the lack of a specific task puts gaze following to a stricter test since
previous studies frequently used target detection tasks [e.g. @Langton2017] or
comprised specific instructions such as asking participants to understand a
scene [@Castelhano2007]. Consequently, it remains unclear to what degree gaze
following occurred spontaneously or was caused by the specific task at hand. In
detail, @Zwickel2010 presented participants multiple 3D rendered outdoor and
indoor scenes for several seconds that always included two clearly visible
objects as well as either a person or a loudspeaker that was directed towards
one of these objects. The loudspeaker, that also represents an object with a
clear spatial orientation, served as a control condition to ensure that gaze
cueing effects are due to the social meaning (i.e., the direction of the
depicted person’s gaze) as compared to a mere following triggered by any
directional cue. The results of the study showed that participants fixated the
cued object remarkably earlier, more often and longer than the uncued object. By
showing that leaving saccades from the head most often landed onto the cued
object, the results gave further evidence for the direct influence of eye gaze
on attentional guidance. Crucially, similar effects were not obtained for the
loudspeaker as a non-social directional cue. The cued objects were not just
focused because they might have been salient by themselves (e.g. due to
positioning), or because they were cued by another object, but became more
salient by the person's reference. To sum up, @Zwickel2010 provide convincing
evidence that joint attention is a direct consequence of gaze cues and gaze
following, it happens spontaneously and has high relevance even in situations
that are more naturalistic (i.e., involve complex scenes and the absence of
explicit tasks) than classical gaze cueing studies based on variations of the
Posner paradigm.<!--softerer übergang pls-->

<!-- Hypothesis -->

In the current study, we were first interested in whether the previously
reported effects hold when using a different set of stimuli. **Replication in
itself is a core concept of scientific progress [@Schmidt2009], nevertheless our
motivation was also to improve certain aspects of the study and at the same time
extending the line of research.** Due to their low resolution and reduced
richness of details, the originally used 3D rendered scenes did not allow for an
assessment of the depicted person’s gaze direction. As a consequence, the
observed cueing effects could be rather due to directional information inferred
from both the body and head of the person. We therefore developed a new set of
photographic stimuli that had sufficient resolution to also allow for perceiving
gaze direction with clearly visible eyes of the depicted person. These photos
always included a human being who directed his/her gaze towards one of two
objects that were placed within reaching distance. In order to be consistent
with the study of @Zwickel2010, the depicted person’s head and body were
congruently aligned with his/her eye gaze. Second, in order to extend this line
of research, we manipulated top-down attentional processes by task instruction
to explore the susceptibility of gaze following effects in naturalistic scenes.
**Earlier research showed that social attention can be influenced by multiple
factors like social status of the observed persons [@Foulsham2010]**. Together
with @Zwickel2010, these studies have in common that they manipulate viewing
behavior of the participant by manipulating the stimuli. In contrast, in the
present study, we tried to modulate viewing behavior via task instructions.
Specifically, half of the participants received an instruction before the
viewing task, that they should try to remember as many objects from the scenes
as possible (explicit encoding group). For the other half of the participants
(free viewing group), the memory test that was accomplished after the experiment
was unannounced and therefore reflected spontaneous encoding of the respective
scene details. The motivation for this manipulation was twofold. First, it was
thought to test the robustness of gaze following against top-down processes by
discouraging observers to utilize the information provided by the gaze. Second,
it allowed for examining gaze following effects on memory.

We expected to replicate the findings of @Zwickel2010 in the free viewing group.
Specifically, we anticipated to observe an early fixation bias towards cued
objects, an enhanced exploration of these details (i.e., more fixations and
longer dwell times) and more saccades leaving the head towards the cued as
compared to the uncued object. The instruction in the explicit encoding group
was thought to induce a more systematic exploration of the presented scenes
resulting in higher prioritization of the uncued object. Furthermore, we
anticipated a generally enhanced recall performance in the explicit encoding
group. Due to the expected difference in attentional resources spent on the cued
and uncued object in the free viewing group, memory performance of the cued
object is expected to be better compared to memory performance of the uncued
object. Finally, as previous studies showed a strong preference of fixating the
head over body and background regions in static images [@End2017; @Freeth2013;
@Zwickel2010], we expected to see a similar bias in the current study regarding
dwell times, number of fixations and fixation latency. Additionally, we
hypothesized that the prioritization for the head decreases when participants
follow specific exploration goals such as in the explicit encoding group of the
current study [cf., @Flechsenhar2017].

# Methods

## Participants

The cueing effects in fixations and saccades that were obtained by @Zwickel2010
can be considered large (Cohen's *d~z~* > 0.70). However, since effects of the
top-down modulation implemented in the current study might be smaller, we used a
medium effect size for estimating the current sample size. When assuming an
effect size of Cohen's $f$ = 0.25 at an $\alpha$ level of .05 and a moderate
correlation of .40 between factor levels of the within-subjects manipulation
object role (cued vs. uncued), a sample size of 66 participants is needed to
reveal main effects of the object role or interaction effects between group and
object role at a power of .95. Under such conditions, the power for detecting
main effects of group is smaller (1-$\beta$ = .67). As a compromise, we aimed at
examining 90 participants (plus eventual drop outs) to achieve a power of .80
for the main effect of group and > .95 for main and interaction effects
involving the within-subjects manipulation object role.

```{r sample description}

complete_sample <- length(df_que$subject_id)
final_sample <- length(unique(df_et$subject_id))
n_female <- sum(as.numeric(df_que[df_que$subject_id!=23,]$subject_sex)-1)
n_male <- final_sample - n_female
min_age <- min(df_que[df_que$subject_id!=23,]$subject_age)
max_age <- max(df_que[df_que$subject_id!=23,]$subject_age)
m_age <- mean(df_que[df_que$subject_id!=23,]$subject_age) %>% round(2)
sd_age <- sd(df_que[df_que$subject_id!=23,]$subject_age) %>% round(2)
m_aqk <- mean(df_que[df_que$subject_id!=23,]$aqk_sumscore) %>% round(2)
sd_aqk <- sd(df_que[df_que$subject_id!=23,]$aqk_sumscore) %>% round(2)
max_aqk <- max(df_que[df_que$subject_id!=23,]$aqk_sumscore) %>% round(2)
min_aqk <- min(df_que[df_que$subject_id!=23,]$aqk_sumscore) %>% round(2)

```

Finally, `r complete_sample` subjects participated voluntarily. All participant 
had normal or corrected vision and were recruited via the University of Würzburg's
online subject pool or by blackboard. Participants received course credit or a
financial compensation of 5€. All participants gave written informed consent.
**One participant was excluded due to missing eyetracking data, resulting in a
final sample of $n = `r final_sample`$** for the analysis with $`r n_female`$
female and $`r n_male`$ male participants between $`r min_age`$ and $`r max_age`$
years ($M = `r m_age`$ years, $SD = `r sd_age`$ years). Overall, participants scored
very low for Autism traits in the Autism-Spectrum Quotient scale [AQ-k, German
version, @Freitag2007, Range 0 to 23, $M = `r m_aqk`$, $SD = `r sd_aqk`$].
**From the final sample, one participant actually exceeded the proposed cut-off
value of >= 17 for Autistic disorders ($Max = `r max_aqk`$, $Min =
`r min_aqk`$).**

## Stimuli and Apparatus

The experimental stimuli consisted of 26 different indoor and outdoor scenes. In
each scene a single individual was looking at one of two objects that were
placed within reaching distance. The direction of the gaze (left/right) and the
placement of the objects (object A and B left/right) were balanced by taking 4
photographs of each scene (see Figure\ \@ref(fig:FigExampleStimulus) for an
example). The position of the individual in the photograph, was comparable to
@Zwickel2010 stimuli not controlled for, as a consequence participants could not
expect a specific spatial structure of the scene and the gaze cue. This created
104 unique naturalistic pictures in total. For each participant, a set was
randomly taken from this pool containing one version of each scene, resulting in
26 trials. Eye movements were tracked with the corneal reflection method and
were recorded with an EyeLink1000plus tower system at, a sampling rate of 1000
Hz. The stimulation was controlled via Presentation® (Neurobehavioral Systems).
All stimuli had a resolution of 1280 x 960 pixels and were displayed on a 24" LG
24MB65PY-B screen (resolution: 1920 x 1200 pixels, display size: 516.9 x 323.1
mm) with a refresh rate of 60 Hz. The viewing distance amounted to 50 cm thus
resulting in a visual angle of 38.03° x 28.99° for the photographs.

```{r FigExampleStimulus, include = TRUE, fig.cap = "Example photographs of a single scene.Gaze direction and objects were balanced over participants. In total 104 photographs of 26 scenes were used. Since we did not obtain permission for publishing the original stimuli, this image shows an example that was not used in the experiment but taken post-hoc in order to illustrate the generation of the stimulus set.", dev.args = list(bg = 'white'), , fig.height=3, fig.width= 7.5}

# The individual depicted in this figure has given written informed consent to publish these pictures.
knitr::include_graphics("2017_repl-zwickel-vo_with-scripts_runing_files/example-stimuli/example.png")

```

## Design and Procedure

The experimental design was a 2 x 2 mixed design. First, as a two-level
between-subject factor each participant was either assigned to the free viewing
or the explicit encoding group (instruction group). Additionally, as a two-level
within subject factor object role was manipulated, with objects being cued or
uncued by the depicted individual in the scene.

After arriving at the laboratory individually, participants were asked to give
full informed consent. Then the eye-tracker was calibrated for each participant
using a 9-point grid. According to the manipulation, half of the participants
were told that there was a follow-up memory test for objects that were part of
the depicted scenes. All participants were then told to look at the following
scenes freely without specifying further exploration goals or mentioning the
content of the scenes. The presentation of the pictures was randomized. Each
trial started with the presentation of a fixation cross for one second, followed
by the scene for 10 seconds. This interval was chosen based on our previous
studies on social attention [@End2017; @Flechsenhar2017] and slightly longer
than the interval (7 s) that was used by @Zwickel2010. The inter-trial interval
varied randomly between 1 and 3 seconds. After the last trial, participants
filled in demographic questionnaires and completed the AQ-k. These
questionnaires were used for characterizing the current sample of participants,
but they were also introduced to reduce recency effects in the memory task that
was accomplished afterwards. Participants then were asked to recall as many
objects from the scene as possible and write them down on a blank sheet of
paper. No time limit was given but after 10 minutes, the experimenter asked
participants to come to an end. In fact, most participants stopped earlier and
indicated that they did not recall further objects. Finally, participants
received course credit or payment and were debriefed.

## Data analysis

```{r stim characterstics }

source(path(path_scripts_publ, "glmm_mem.R"))
source(path(path_scripts_publ, "dscr_roi.R"))

# means and sds for proportional roi size
m_stim_cued <- 
    dscr_roi[dscr_roi$fix_id == "object_cued", ]$m_size/(1280*960)*100 %>%
    round(1)
sd_stim_cued <-
    dscr_roi[dscr_roi$fix_id == "object_cued", ]$sd_size/(1280*960)*100 %>%
    round(1)

m_stim_uncued <- 
    dscr_roi[dscr_roi$fix_id == "object_uncued", ]$m_size*100 %>%
    round(1)
sd_stim_uncued <- 
    dscr_roi[dscr_roi$fix_id == "object_uncued", ]$sd_size/(1280*960)*100 %>%
    round(1)

m_stim_head <- 
    dscr_roi[dscr_roi$fix_id == "head", ]$m_size/(1280*960)*100 %>%
    round(1)
sd_stim_head <-
    dscr_roi[dscr_roi$fix_id == "head", ]$sd_size/(1280*960)*100 %>% 
    round(1)

m_stim_body <-
    dscr_roi[dscr_roi$fix_id == "body", ]$m_size/(1280*960)*100 %>% 
    round(1)
sd_stim_body <-
    dscr_roi[dscr_roi$fix_id == "body", ]$sd_size/(1280*960)*100 %>% 
    round(1)

```

For data processing and statistical analysis, the open-source statistical
programming language *R* [@R-base] was used with the packages *tidyverse*
[@R-tidyverse] and *knitr* [@R-knitr] and *papaja* [@R-papaja] for reproducible
reporting. For the analysis of the eye-tracking data, EyeLink's standard
configuration was used to parse eye movements into saccades and fixations.
Saccades were defined as eye movements exceeding a velocity threshold of 30°/sec
or an acceleration threshold of 8.000°/sec². Fixations were defined as time
periods between saccades.

We determined the following regions of interest (ROI) by color coding respective
images regions by hand using GIMP (GNU Image Manipulation Program): **the cued
object (average relative size on image: $M = `r m_stim_cued`\%$,
$SD = `r sd_stim_cued`\%$), the uncued object ($M = `r m_stim_uncued `\%$,
$SD = `r sd_stim_uncued `\%$), the head ($M = `r m_stim_head`\%$,
$SD = `r sd_stim_head`\%$) and the body ($M = `r m_stim_body`\%$,
$SD = `r sd_stim_body`\%$) of the depicted person**.  Gaze variables of interest
were calculated in a largely similar fashion as in @Zwickel2010. Specifically,
we determined the cumulative duration and number of fixations on each ROI per
trial. These values were divided by the total time or number of fixations,
respectively, to yield proportions.
As an additional measure of prioritization, particularly for early attentional
allocation, we determined the latency of the first fixation that was directed
towards each ROI. These measures allow for effective comparisons of
prioritization between the two relevant objects and between the head and the
body. To reveal direct relations between the head and the relevant objects, we
calculated the proportion of saccades that left the head region of the depicted
individual and landed on the cued and uncued objects, respectively. The memory
test was scored manually by comparing the list of recalled objects to the
objects that appeared in the scenes. We separately scored whether cued or uncued
objects were recalled and ignored any other reported details. Afterwards, we
calculated the sum of recalled objects separately for cued and uncued details.

In order to analyze the influence of the experimental manipulations on
eye-tracking and memory data, we carried out separate analyses of variance
(ANOVAs) with the package *afex* [@R-afex]. The ANOVAs had the between-subject
factor instruction group and the within-subject factor object role and were
conducted on the dependent variables fixation latency, relative fixation
duration, relative numbers of fixations, proportion of saccades from the head
towards the object and recalled items. To examine general effects of social
attention, separate ANOVAs using the between-subject factor instruction group
and the within-subject factor ROI (head vs. body region) were conducted on the
dependent variables fixation latency, fixation duration and number of fixations.
**To assess the temporal attentional allocation we included a within-subject
factor *time points* with 5 levels into the ANOVA for relative fixation duration as well
as relative number of fixations. Time points were bins for the sum of durations and numbers
of fixations for 2 seconds, resulting in 5 time points across 10 seconds of stimulus
presentation. To follow up the main analysis on gaze following, we calculated
contrasts using *emmeans* [@R-emmeans] for cued vs. uncued object for instruction group
and time points to assess the change of gaze following over time and for instruction
groups. For social prioritization, the ANOVA was followed by contrasts for free
viewing group vs explicit encoding group for head (and body) and time points. Here we
were mainly interested in whether prioritization between instruction groups
differed for the head over time.  
To complement the ANOVA on recall performance we used a generalized linear 
mixed model using *lme4* [@R-lme4] to examine the influence of relative 
fixation duration and relative numbers of fixation on stimulus recall
performance. In model 1 instruction group was entered as the main predictor of
subsequent recall performance. In the second step we added z-standardized 
relative fixation duration (i.e., range  
`r round(min(df_mem_glmm$prop_dur_z),2)` to 
`r round(max(df_mem_glmm$prop_dur_z),2)`) in model 2a. And analougusly, the
z-standardized relative number of fixations (i.e., range 
`r round(min(df_mem_glmm$prop_num_z),2)` to 
`r round(max(df_mem_glmm$prop_num_z),2)`) in model 2b. In the third step we added
object role to the previous models. We always tested the new model against the
previous model in an ANOVA to test if relative fixation duration and/or relative
fixation number had incremental value.**  
For all analysis the apriori significance level of $\alpha$ = 0.05 was used. As effect sizes generalized eta-square ($\eta^2_G$) for ANOVAs are reported, where guidelines suggest .26 as
a large, .13 as a medium and .02 as a small effect [@Bakeman2005].

# Results

```{r descriptives}

source(path(path_scripts_publ, "dscr_et.R"))   # all measures
source(path(path_scripts_publ, "dscr_ja.R"))   # joint attention
source(path(path_scripts_publ, "dscr_sa.R"))   # social attention
source(path(path_scripts_publ, "dscr_sac.R"))  # saccades
source(path(path_scripts_publ, "dscr_mem.R"))  # memory

```

```{r plots}

source(path(path_scripts_publ, "plt_ja.R"))    # plots ja
source(path(path_scripts_publ, "plt_sa.R"))    # plots sa
source(path(path_scripts_publ, "plt_mem.R"))   # plot memory
source(path(path_scripts_publ, "plt_et.R"))    # plot temporal 

```


## Gaze following
<!-- #### Fixation latency -->

```{r anova ja}

# perform anovas
source(path(path_scripts_publ, "aov_ja.R"))
source(path(path_scripts_publ, "aov_sac.R"))

# nice formatting of results
# latency
apa_aov_ja_lat <- apa_print(aov_ja_lat, correction="GG", mse = FALSE)$full_result
apa_emm_ja_lat <- apa_print(emm_ja_lat$contrasts)$statistic

# duration
apa_aov_ja_dur <- apa_print(aov_ja_dur, correction="GG", mse = FALSE)$full_result
apa_emm_ja_dur <- apa_print(emm_ja_dur$contrasts)$statistic

# number
apa_aov_ja_num <- apa_print(aov_ja_num, correction="GG", mse = FALSE)$full_result
apa_emm_ja_num <- apa_print(emm_ja_num$contrasts)$statistic

# saccades
apa_aov_sac_fgaze <- apa_print(aov_sac.fgaze, correction="GG", mse = FALSE)$full_result

```

```{r means main effects ja}

# object role
m_ja_lat_cued <-
    mean(dscr_ja_lat[dscr_ja_lat$fix_id == "object_cued",]$m_lat) %>% 
    round(0)
m_ja_lat_uncued <-
    mean(dscr_ja_lat[dscr_ja_lat$fix_id == "object_uncued",]$m_lat) %>% 
    round(0)

m_ja_dur_cued <-
    mean(dscr_ja_dur[dscr_ja_dur$fix_id == "object_cued",]$m_dur) %>% 
    round(4) * 100
m_ja_dur_uncued <-
    mean(dscr_ja_dur[dscr_ja_dur$fix_id == "object_uncued",]$m_dur) %>% 
    round(4) * 100

m_ja_num_cued <-
    mean(dscr_ja_num[dscr_ja_num$fix_id == "object_cued",]$m_num) %>% 
    round(4) * 100
m_ja_num_uncued <-
    mean(dscr_ja_num[dscr_ja_num$fix_id == "object_uncued",]$m_num) %>% 
    round(4) * 100

# group
m_ja_lat_free <-
    mean(dscr_ja_lat[dscr_ja_lat$group_id == "free",]$m_lat) %>% 
    round(0)
m_ja_lat_mem <-
    mean(dscr_ja_lat[dscr_ja_lat$group_id == "mem",]$m_lat) %>% 
    round(0)

m_ja_dur_free <-
    mean(dscr_ja_dur[dscr_ja_dur$group_id == "free",]$m_dur) %>% 
    round(4) * 100
m_ja_dur_mem <-
    mean(dscr_ja_dur[dscr_ja_dur$group_id == "mem",]$m_dur) %>% 
    round(4) * 100

m_ja_num_free <-
    mean(dscr_ja_num[dscr_ja_num$group_id == "free",]$m_num) %>% 
    round(4) * 100
m_ja_num_mem <-
    mean(dscr_ja_num[dscr_ja_num$group_id == "mem",]$m_num) %>% 
    round(4) * 100

# saccades
# object role
m_sac_cued <-
    mean(dscr_sac[dscr_sac$fix_id == "object_cued",]$m_sac) %>% 
    round(4) * 100
m_sac_uncued <-
    mean(dscr_sac[dscr_sac$fix_id == "object_uncued",]$m_sac) %>% 
    round(4) * 100

# group
m_sac_free <-
    mean(dscr_sac[dscr_sac$group_id == "free",]$m_sac) %>% 
    round(4) * 100
m_sac_mem <-
    mean(dscr_sac[dscr_sac$group_id == "mem",]$m_sac) %>% 
    round(4) * 100

```

A significant main effect of object role in the analysis of fixation latencies
indicates earlier fixations on cued compared to uncued **(
`r apa_aov_ja_lat$fix_id`; $M_{lat,cued} = `r m_ja_lat_cued` ms$, $M_{lat,uncued} 
= `r m_ja_lat_uncued` ms$)** objects. The main effect of instruction group was also
significant, with earlier fixations on both objects
in the explicit encoding **(`r apa_aov_ja_lat$group_id`; $M_{lat,mem} = 
`r m_ja_lat_mem` ms$)** compared to the free viewing group **($M_{lat,free} = 
`r m_ja_lat_free` ms$)**. The interaction effect failed to reach statistical
significance (`r apa_aov_ja_lat$group_id_fix_id`; see Figure
\ \@ref(fig:FigFixObjects) A).

```{r FigFixObjects, include =TRUE, fig.cap = "Bar plots of the different prioritization measures for the attentional orienting towards the cued and uncued objects as a function of instruction group. Error bars represent standard errors of the mean.", dev.args = list(bg = 'white'), fig.height=3, fig.width= 7.5}

plot_grid(
  plt_ja_lat, plt_ja_dur, plt_ja_num,
  pl.sac.obj, cLegendObjects,
  labels = c('A', 'B', 'C', 'D', ''),
  nrow = 1,
  rel_widths  = c(1,1,1,1,.5))

```

```{r FigFixTmp, include = TRUE, fig.cap = "Temporal course of fixation duration and number of fixation. Error bars represent standard errors of the mean.", dev.args = list(bg = 'white'), fig.height=3, fig.width= 7.5}

plot_grid(
    plt_et_dur_bin,
    plt_et_num_bin,
    cLegendTemp,
    labels = c('A', 'B'),
    nrow = 1,
    rel_widths  = c(1,1,.25))

```

<!--#### Fixation duration & number-->
<!-- (H1, H2) -->

Largely similar effects were obtained in the analyses of fixation duration and
numbers (see Figure\ \@ref(fig:FigFixObjects) B & C). Significant main effects
of object role indicated that participants fixated the cued object longer
**(`r apa_aov_ja_dur$fix_id`; $M_{dur,cued} = `r m_ja_dur_cued`\%$, 
$M_{dur,uncued} = `r m_ja_dur_uncued`\%$)**, and more often
**(`r apa_aov_ja_num$fix_id`; $M_{num,cued} = `r m_ja_num_cued`\%$, 
$M_{num,uncued} = `r m_ja_num_uncued`\%$)**, than the uncued object.
Explicit instructions also led to longer **(`r apa_aov_ja_dur$group_id`; 
$M_{dur,mem} = `r m_ja_dur_mem`\%$, $M_{dur,free} = `r m_ja_dur_free`\%$)**, 
and more fixations **(`r apa_aov_ja_dur$group_id`; $M_{num,cued} = 
`r m_ja_num_mem`\%$, $M_{num,free} = `r m_ja_num_free`\%$)**, on the objects as
compared to the free viewing condition. The interaction effects of instruction
group and object role were not statistically significant, neither for the
duration (`r apa_aov_ja_dur$group_id_fix_id`), nor for the number of fixations 
(`r apa_aov_ja_num$group_id_fix_id`).

<!-- #### Temporal dynamics duration & number -->

**To assess the time course of attentional resources on gaze following we
analyzed fixation duration and numbers across different bins on objects (see
Figure\ \@ref(fig:FigFixTmp)). For both we see an interaction between
object role and time points (duration: `r apa_aov_ja_dur$fix_id_bin_id`, number:
`r apa_aov_ja_num$fix_id_bin_id`). As well as an interaction between group and
time points (duration `r apa_aov_ja_dur$group_id_bin_id `, number:
`r apa_aov_ja_num$group_id_bin_id `). Pairwise t-test reveal that the interaction
between time points and object role was mainly driven by the first and last time
point in the free viewing group (first time point: duration:
`r apa_emm_ja_dur$object_cued_object_uncued_free_X1 `, number:
`r apa_emm_ja_num$object_cued_object_uncued_free_X1 `, last time point: duration:
`r apa_emm_ja_dur$object_cued_object_uncued_free_X5 `, number:
`r apa_emm_ja_num$object_cued_object_uncued_free_X5 `, with all other bins $p >
.2$), whereas for the explicit encoding group the first and the second time
point drive the interaction (first time point: duration:
`r apa_emm_ja_dur$object_cued_object_uncued_mem_X1 `, number:
`r apa_emm_ja_num$object_cued_object_uncued_mem_X1 `, second time point:
duration: `r apa_emm_ja_dur$object_cued_object_uncued_mem_X2 `, number:
`r apa_emm_ja_num$object_cued_object_uncued_mem_X2 `, with all other bins $p >
.2$).**

<!--### Saccades-->

<!-- (H1, H2) -->

Saccades leaving the head were more likely to land on the cued
compared to the uncued object as confirmed by a significant main
effect of object role, **(`r apa_aov_sac_fgaze$icond`; $M_{sac,cued} =
`r m_sac_cued`$, $M_{sac,uncued} = `r m_sac_uncued`$)**. The main effect for 
group showed that saccades of participants in the explicit encoding group were
more often directed towards any of the objects as compared to the free viewing
group **(`r apa_aov_sac_fgaze$df_w_et_group`; $M_{sac,free} = `r m_sac_free`$, 
$M_{sac,mem} = `r m_sac_mem`$)**. Again, the interaction effect of instruction 
group and object role failed to reach statistical significance 
(`r apa_aov_sac_fgaze$df_w_et_group_icond`; see Figure\ \@ref(fig:FigFixObjects) 
D).

## Memory for objects
<!-- (H3) -->

```{r anova mem }

source(path(path_scripts_publ, "aov_mem.R"))

apa_aov_mem <- apa_print(aov_mem, correction="GG", mse = FALSE)$full_result

```


```{r means main effects mem}

# group
m_mem_free <-
    round(mean(dscr_mem[dscr_mem$group_id == "free",]$m_recall), 2)
m_mem_mem <-
    round(mean(dscr_mem[dscr_mem$group_id == "mem",]$m_recall), 2)

```

An analysis of the recall data showed, that participants in the explicit
encoding group remembered more items than participants from the free viewing
group **(`r apa_aov_mem$group_id `; $M_{recall,free} =
`r m_mem_free`$, $M_{recall,mem} = `r m_mem_mem`$)**. Neither the main effect of
object role (`r apa_aov_mem$stim_cued `) nor the interaction effect were
statistically significant (`r apa_aov_mem$group_id_stim_cued `; see
Figure\ \@ref(fig:FigMem)).

```{r FigMem, include =TRUE, fig.cap = "Bar plot of the memory performance for the cued and uncued objects as a function fo instruction group. Error bars represent standard errors of the mean.", dev.args = list(bg = 'white'), fig.height=3, fig.width= 3}

plot_grid(
    plt_mem, cLegendObjects,
    nrow = 1,      
    rel_widths  = c(1,.5))

```

```{r GLMM memory}

source(path(path_scripts_publ, "glmm_mem.R"))
source(path(path_scripts_publ, "tbl_glmm.R"))

```

```{r tblGLMM, include = TRUE, results="asis"}

apa_table(
    tbl_glmm_results
    , caption = "Parameters and model selection criteria of general linear mixed models predicting object recall from group, number/duration of fixation and object role."
    , note = "Akaike information criterion; BIC = Bayesian information criterion; DIC = Deviance information criterion; df = Residual degrees of freedom."
    , landscape = F
    , align = c("l", rep("r", 8))
    , font_size = "small")

```

**For the follow up analysis of recall performance we started with the first
model where only group assignment was entered. Revealing the significant effect
for group as shown by the ANOVA previously (see Table\ \@ref(tab:tblGLMM) for model parameters and model selcetion criteria). Next, we added to separate models our variables of main
interest: Model 2a got extended by the main effect of (z-standardized) relative
fixation duration and the interaction with group. To Model 2b we added
(z-standardized) relative number of fixations and the interaction with group.
Surprisingly, the main effect of fixation duration and its interaction with
group in model 2a failed to reach statistical significance. As expected, the
main effect of number of fixations improved the prediction of recalled stimuli.
Again the interaction of group and number of fixations did reach significance.
For number of fixations we tested as a last step, whether object role further
improves the prediction of recall performance, that was not the case, with the
main effect an all other interactions remaining above our significance level.**

## Social prioritization

```{r anova fixations sa}

# perform anovas
source(path(path_scripts_publ, "aov_sa.R"))

# nice formatting of results
# latency
apa_aov_sa_lat <- apa_print(aov_sa_lat, correction="GG", mse = FALSE)$full_result
apa_emm_sa_lat <- apa_print(emm_sa_lat$contrasts)$statistic

# duration
apa_aov_sa_dur <- apa_print(aov_sa_dur, correction="GG", mse = FALSE)$full_result
apa_emm_sa_dur <- apa_print(emm_sa_dur$contrasts)$statistic

# number
apa_aov_sa_num <- apa_print(aov_sa_num, correction="GG", mse = FALSE)$full_result
apa_emm_sa_num <- apa_print(emm_sa_num$contrasts)$statistic

```

```{r means main effects sa}

# head vs body
m_sa_lat_head <-
    mean(dscr_sa_lat[dscr_sa_lat$fix_id == "head",]$m_lat) %>%
    round()
m_sa_lat_body <-
    mean(dscr_sa_lat[dscr_sa_lat$fix_id == "body",]$m_lat) %>% 
    round()

m_sa_dur_head <-
    mean(dscr_sa_dur[dscr_sa_dur$fix_id == "head",]$m_dur) %>% 
    round(4) * 100
m_sa_dur_body <-
    mean(dscr_sa_dur[dscr_sa_dur$fix_id == "body",]$m_dur) %>% 
    round(4) * 100

m_sa_num_head <-
    mean(dscr_sa_num[dscr_sa_num$fix_id == "head",]$m_num) %>% 
    round(4) * 100
m_sa_num_body <-
    mean(dscr_sa_num[dscr_sa_num$fix_id == "body",]$m_num) %>% 
    round(4) * 100

# group
m_sa_lat_free <-
    mean(dscr_sa_lat[dscr_sa_lat$group_id == "free",]$m_lat) %>% 
    round(4) * 100
m_sa_lat_mem <-
    mean(dscr_sa_lat[dscr_sa_lat$group_id == "mem",]$m_lat) %>% 
    round(4) * 100

m_sa_dur_free <-
    mean(dscr_sa_dur[dscr_sa_dur$group_id == "free",]$m_dur) %>% 
    round(4) * 100
m_sa_dur_mem <-
    mean(dscr_sa_dur[dscr_sa_dur$group_id == "mem",]$m_dur) %>% 
    round(4) * 100

m_sa_num_free <-
    mean(dscr_sa_num[dscr_sa_num$group_id == "free",]$m_num) %>% 
    round(4) * 100
m_sa_num_mem <-
    mean(dscr_sa_num[dscr_sa_num$group_id == "mem",]$m_num) %>% 
    round(4) * 100

```

<!--#### Fixation latency-->

Fixation latencies differed remarkably between the head and the body
(see Figure\ \@ref(fig:FigFixPerson) A). Consequently, the ANOVA yielded
a significant main effect of ROI, with earlier fixations of the
head compared to the body **(`r apa_aov_sa_lat$fix_id`; $M_{lat,head} =
`r m_sa_lat_head`$, $M_{lat,body} = `r m_sa_lat_body`$)**. There was neither a
statistically significant main effect of instruction group
(`r apa_aov_sa_lat$group_id`) nor an interaction of both
factors, (`r apa_aov_sa_lat$group_id_fix_id`).

```{r FigFixPerson, include = TRUE, fig.cap = "Bar plot of the different prioritization measures for attentional orienting towards and visual exploration of the depcited person's head and body as a function of instruction group. Error bars represent standard errors of the mean.", dev.args = list(bg = 'white'), , fig.height = 3, fig.width= 6}

plot_grid(
    plt_sa_lat, plt_sa_dur,
    plt_sa_num, cLegendHead,
    labels = c("A", "B", "C", ""),
    nrow = 1,      
    rel_widths  = c(1,1,1,.5))

```

<!--#### Fixation duration & number-->

Fixation duration and numbers showed a very similar pattern with longer,
**(`r apa_aov_sa_dur$fix_id`; $M_{dur,head} = `r m_sa_dur_head`$, $M_{dur,body}
= `r m_sa_dur_body`$)**, as well as more fixations **(`r apa_aov_sa_num$fix_id`;
$M_{num,head} = `r m_sa_num_head`$, $M_{num,body} = `r m_sa_num_body`$)**, on
the head than the body ROI. Remarkably, the instruction group did not exhibit a
statistically significant main effect, neither for the fixation duration
(`r apa_aov_sa_dur$group_id`), nor for the number of fixation, 
(`r apa_aov_sa_num$group_id`). Furthermore, the interaction effects of
instruction group and ROI failed to reach statistical significance
for fixation duration (`r apa_aov_sa_dur$group_id_fix_id`) and fixation
numbers, (`r apa_aov_sa_num$group_id_fix_id`; see Figure\ \@ref(fig:FigFixPerson)
B & C). These findings indicate that the prioritization of social ROIs was
unaffected by the explicit instruction to attend to objects depicted in the
scene.

<!-- #### Temporal dynamics duration & number -->

**To assess instruction effects over time for social prioritization we analysed
fixation duration and number across time points  (see Figure\ \@ref(fig:FigFixTmp)).
For both we see an significant interaction between group and time points (duration:
`r apa_aov_sa_dur$group_id_bin_id`, number: `r apa_aov_sa_num$group_id_bin_id `) as
well as between head/body and time points (duration: `r apa_aov_sa_dur$fix_id_bin_id `,
number: `r apa_aov_sa_num$fix_id_bin_id `) . Pairwise t-test reveal that the
interaction between group and time points was driven by the first and last time point (first
time point: `r apa_emm_sa_dur$free_mem_head_X1`, last time point:
`r apa_emm_sa_dur$free_mem_head_X5 `, with all other time points $p > .09$). For fixation
number on the head, surprisingly the first and second time point show significant
differences across group assignment (first: `r apa_emm_sa_num$free_mem_head_X1`,
second: `r apa_emm_sa_num$free_mem_head_X2`, except the last time point (
`r apa_emm_sa_num$free_mem_head_X5`), all other time points $p > .14$). None of the
contrasts for body reached significance across instruction group and time points (all $p >
.28$).**

# Discussion

<!-- Main motivation -->

By using naturalistic scenes with rich detail, this study aimed at conceptually
replicating previous findings of a general prioritization of social cues [i.e.
heads and bodies, @Birmingham2008; @End2017; @Flechsenhar2017] as well as
previously reported gaze cueing effects elicited by a person being directed
towards a specific object in the scene [@Zwickel2010]. **Both effects were
replicated**.

In detail, heads of persons in the scene were fixated earlier and explored more
extensively as compared to body regions but also cued and uncued objects.
Additionally, in line with @Zwickel2010, cued objects were preferred over uncued
objects. They were fixated remarkably earlier, longer and more often. Thus, gaze
following effects did not only occur with respect to a more thorough processing
during the whole time of scene presentation but were also evident in an early
allocation of attentional resources after stimulus onset. **Additional support
comes from the investigation of temporal allocation of attention. Differences 
between our manipulations were most present in
the first 2 seconds (see Figure\ \@ref(fig:FigFixTmp)). During that
time, fixations on the head differed clearly between instruction groups, with more
social prioritization during free viewing. Temporal analysis for gaze following
showed in the first 2 seconds strongest preference of the cued object in both groups.
Interestingly, viewing pattern differd between both groups after the intial 2
seconds. Object prioritization was reversed durint the second time point (seconds 2 - 4) in the explicit encoding group with more and longer fixations on the uncued object. In contrast, participants
in the free viewing group payed as much attention on the uncued object as on the
cued object during that time period. It seems that the instruction induced a
systematic exploration resulting in reducing attention for the cued object.**
All these findings together indicate a strong and automatic bias of using gaze
cues for attentional guidance **especially in the early phases of stimulus
exploration. From the temporal analysis of attentional allocation we can
conclude, that the unexpected missing interaction between group and object role
comes very likely from the extended stimulus presentation. Our results indicate
a rather strong group effects in the first 4 seconds, especially for object
role. However, comparing both groups the picture is less clear for the
preference of the head. The instruction manipulation seems to affect only very
early the attentional allocation which does not persist over time.**

<!-- saccacades -->

Moreover, the prioritization of the head and the preference for the cued object
indirectly suggest a link between these two regions. To investigate this
relationship in more detail, we examined saccades leaving the head towards the
cued and uncued object, respectively. Saccades leaving the head were
significantly more likely to end on the cued than on the uncued object directly
linking fixations of the head and the cued object. Thereby, current results
fully replicate the findings of @Zwickel2010 with a more naturalistic set of
stimuli. As often, by using more naturalistic material experimental control is
reduced. We tried to minimize unsystematic effects by producing the stimuli in
the same way as @Zwickel2010 but using real as compared to 3D rendered scenes.
In particular, each scene was photographed four times with gaze direction an
object placement being fully counterbalanced. Since four individual photographs
of each scene were taken in the current study, we could not fully control all
stimulus aspects. However, the full replication of the effects previously
obtained with a different set of virtual scenes indicates that these effects
generalize to naturalistic conditions and are stable against small variations in
scene layout and presentation.

<!-- top-down -->

Besides replicating previous findings, this study also aimed at extending the
line of research by testing the robustness of gaze following against top-down
modulations. This was achieved by instructing half of the participants to
memorize as many details of the presented scenes as possible. Since the depicted
human were not relevant to this task, we expected a generally reduced attention
towards head and body regions as well as a more systematic exploration pattern,
potentially reducing gaze cueing effects in fixations on and saccades towards
cued objects. Unsurprisingly the memory task that was accomplished after the eye
tracking experiment showed that participants, who knew about the free recall
task in advance performed better in recalling items. More interestingly,
however, social attention as well as cueing effects in viewing behavior were
largely unaffected by the explicit instruction to remember as many objects from
the scenes as possible. Specifically, participants in the explicit recall group
did not show reduced attention towards head and body regions as compared to the
free viewing group. Moreover, whereas they paid more attention towards depicted
objects in general, gaze cueing effects on fixation latencies and densities as
well as the direction for saccades leaving the head region did not differ
significantly between both experimental groups.

These findings indicate that the prioritization of social information in general
as well as of the cued objects in particular are largely unaffected by a
manipulation of goal-driven attention. The attentional guidance of gaze was
effective, even when participants investigated the scenes with an explicit
(non-social) task goal. This provides support for the automaticity and
reflexivity of social attentional processes and is in line with previous studies
on gaze cueing within highly controlled setups [e.g., @Ristic2005;
@Hayward2017], more naturalistic laboratory studies [e.g., @Castelhano2007;
@Zwickel2010] and real-life social situations [e.g., @Hayward2017;
@Richardson2007]. Moreover, the current results are consistent with recent
findings of an early attention bias towards social information [@End2017;
@Roesler2017] that seems to be relatively resistant against specific task
instructions [@Flechsenhar2017].

<!-- memory -->

As expected participants with specific recall instructions performed better in
the subsequent memory task. However, the contribution of the automatic
attentional processes to memory encoding remains unclear. In particular,
although cued objects were prioritized in the attentional exploration, **only
the number of fixations did improve the prediction of stimulus recall over group
assingment (see Table\ \@ref(tab:tblGLMM)). Fixation duration did not add incremental value**. This is
**partially in line with** studies on eye movements [e.g., @Hollingworth2002]
and (non-social) cueing [@Schmidt2002; @Belopolsky2008] which showed that
increased attention results in better memory performance. **We found this
relationship only for number of fixation**. **Originally we expected the cued
object to be recalled better than the uncued object [@Schmidt2002;
@Belopolsky2008]**. However, another study showed that if certain scene details
have a special meaning (e.g., by being central to the content of a picture
story), attention does no longer predict memory for these details [@Kim2013].
With respect to the current study, these findings may indicate that both objects
that were placed within reaching distance of the depicted person conveyed such
meaning and were therefore remembered with equal probability. Since we only
tested for early memory effects, it would be very interesting to delay the
memory test by at least 24h to examine whether memory consolidation differs
between cued and uncued objects [@Squire1993]. **Another explanation might be
that exploration time was sufficient to process both objects equally well. This
interpretation finds support in the fact that the group assignment primary
affected the first seconds of stimulus presentation.**

<!-- Limitations -->

Although the current study has several strengths including the systematic
generation of novel stimulus material and the large sample size, it also has
some limitations that need to be mentioned. First, although this study shows
that humans follow other persons gaze implicitly in unconstrained situations,
this was shown for situations without real interactions between humans. Research
shows, that fixation patterns differ remarkably when a real interaction between
persons is possible [e.g., @Hayward2017; @Laidlaw2011; for an overview see:
@Risko2016]. However, our findings add evidence to the classic highly controlled
laboratory approaches to social attention, yet at the same time approximates
more ecological research [@Risko2012]. An additional critique might be that we
did not control for directional information from the depicted person’s body in
contrast to the head. Earlier studies show, that body orientation is relevant
for cueing [@Hietanen1999; @Lawson2016] and the influence of body orientation on
the cueing effects (e.g., through peripheral vision) cannot be dissociated by
our study design. However, our results indicate a direct link between the head
and the cued object, as does @Zwickel2010. In fact, overall the first fixation
of the body occurs about 1 second after first fixation on the cued object.
<!--[3] A third limitation might be, that participants expected a specific
structure for each scene, increasing reflexive shifts towards the head. Although
we did not controll for head position in the scene, the head appeared most often
in the center and everytime in the upper half of the scene. To test this
expectancies in participants one might create scenes explicitly manipulating the
position of the social stimulus. --> 

<!-- Conclusio -->

Overall, the current results provide additional support for previous findings
that attention is shifted reflexively to locations where other persons are
looking at [e.g., @Ristic2005; @Hayward2017]. This evidence, which was
previously extended to free viewing of more complex static scenes by
@Zwickel2010, was shown to be valid in more naturalistic scenes and robust
against top-down modulation. Even when explicitly directing attention away from
depicted individuals by making objects task-relevant, social and joint attention
were still affected by the mere presence of a person, comparable to the unbiased
free viewing condition. These results indicate that the mere presence of other
human beings as well as their gaze orientation have a strong impact on
attentional exploration.

\newpage

# Acknowledgements

The authors thank Michael Strunz for his help with data collection. This work
was supported by the European Research Council (ERC-2013-StG #336305).


# References

```{r create_r-references }

r_refs(file = "r-references.bib")

```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
